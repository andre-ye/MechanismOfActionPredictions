{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036299,
     "end_time": "2020-11-07T22:11:18.532984",
     "exception": false,
     "start_time": "2020-11-07T22:11:18.496685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: white; background: #ADD8E6; text-align:center; font-size: 2.3em;\"> MoA + Feature Engeneering + Keras </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034414,
     "end_time": "2020-11-07T22:11:18.603410",
     "exception": false,
     "start_time": "2020-11-07T22:11:18.568996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f757229-7f7b-4ea6-b69a-270249d85727",
    "_uuid": "c014d6b0-3b76-46e4-a18f-de3d38f0ad9c",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:18.684476Z",
     "iopub.status.busy": "2020-11-07T22:11:18.683521Z",
     "iopub.status.idle": "2020-11-07T22:11:18.701672Z",
     "shell.execute_reply": "2020-11-07T22:11:18.700380Z"
    },
    "papermill": {
     "duration": 0.063885,
     "end_time": "2020-11-07T22:11:18.701791",
     "exception": false,
     "start_time": "2020-11-07T22:11:18.637906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.gitignore\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/LICENSE\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/README.md\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.cfg\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.travis.yml\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/test_ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/ml_stratifiers.py\n",
      "/kaggle/input/rank-gauss/rankGaussTrafo.py\n",
      "/kaggle/input/rank-gauss/gauss_rank_scaler.py\n",
      "/kaggle/input/rank-gauss/rgn.py\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/Rplot001.png\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/train_onehot.csv\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/pca_model.rdata\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/main_predictors.json\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/custom.css\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/__notebook__.ipynb\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/trainPCA.csv\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/__results__.html\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/__output__.json\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/errors_by_category.csv\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/testPCA.csv\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/submission.csv\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/test_onehot.csv\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/__results___files/__results___35_1.png\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/__results___files/__results___10_0.png\n",
      "/kaggle/input/t-test-pca-rfe-logistic-regression/__results___files/__results___11_0.png\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "99fa0530-d4cf-4803-9596-060a6980270b",
    "_uuid": "d4ba5f02-0fe6-4210-93a0-818ff7614bd6",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:18.787374Z",
     "iopub.status.busy": "2020-11-07T22:11:18.786533Z",
     "iopub.status.idle": "2020-11-07T22:11:25.527536Z",
     "shell.execute_reply": "2020-11-07T22:11:25.528211Z"
    },
    "papermill": {
     "duration": 6.788723,
     "end_time": "2020-11-07T22:11:25.528409",
     "exception": false,
     "start_time": "2020-11-07T22:11:18.739686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as C\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.optimizers as O\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "sys.path.append('../input/rank-gauss')\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035966,
     "end_time": "2020-11-07T22:11:25.601680",
     "exception": false,
     "start_time": "2020-11-07T22:11:25.565714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h3 style=\"color: \t#32CD32;\">Reading Files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "4523fe40-9241-427d-9054-391f9107ee80",
    "_uuid": "e5ebde3e-8733-4f43-9bd5-87942b293715",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:25.684549Z",
     "iopub.status.busy": "2020-11-07T22:11:25.683591Z",
     "iopub.status.idle": "2020-11-07T22:11:32.561125Z",
     "shell.execute_reply": "2020-11-07T22:11:32.560001Z"
    },
    "papermill": {
     "duration": 6.922635,
     "end_time": "2020-11-07T22:11:32.561276",
     "exception": false,
     "start_time": "2020-11-07T22:11:25.638641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "train_non = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "sub = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:32.647323Z",
     "iopub.status.busy": "2020-11-07T22:11:32.646203Z",
     "iopub.status.idle": "2020-11-07T22:11:37.841051Z",
     "shell.execute_reply": "2020-11-07T22:11:37.840313Z"
    },
    "papermill": {
     "duration": 5.24447,
     "end_time": "2020-11-07T22:11:37.841167",
     "exception": false,
     "start_time": "2020-11-07T22:11:32.596697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features2 = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "test_features2 = pd.read_csv('../input/lish-moa/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:37.921082Z",
     "iopub.status.busy": "2020-11-07T22:11:37.920274Z",
     "iopub.status.idle": "2020-11-07T22:11:37.924314Z",
     "shell.execute_reply": "2020-11-07T22:11:37.923713Z"
    },
    "papermill": {
     "duration": 0.044884,
     "end_time": "2020-11-07T22:11:37.924437",
     "exception": false,
     "start_time": "2020-11-07T22:11:37.879553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# json_file_path = '../input/t-test-pca-rfe-logistic-regression/main_predictors.json'\n",
    "\n",
    "# with open(json_file_path, 'r') as j:\n",
    "#     predictors = json.loads(j.read())\n",
    "#     predictors = predictors['start_predictors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.036765,
     "end_time": "2020-11-07T22:11:37.997127",
     "exception": false,
     "start_time": "2020-11-07T22:11:37.960362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "becd92fe-dbbe-4af0-bd33-3d5706f367a4",
    "_uuid": "2c5892c4-f2d9-430d-b986-4d9519e5e9f8",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.082120Z",
     "iopub.status.busy": "2020-11-07T22:11:38.081449Z",
     "iopub.status.idle": "2020-11-07T22:11:38.086114Z",
     "shell.execute_reply": "2020-11-07T22:11:38.085576Z"
    },
    "papermill": {
     "duration": 0.04715,
     "end_time": "2020-11-07T22:11:38.086227",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.039077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictors = []\n",
    "# for col in train_features.columns:\n",
    "#     if col in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']:\n",
    "#         continue\n",
    "#     if stats.ttest_ind(train_features[col], test_features[col]).pvalue > 0.8:\n",
    "#         predictors.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.171778Z",
     "iopub.status.busy": "2020-11-07T22:11:38.170840Z",
     "iopub.status.idle": "2020-11-07T22:11:38.174325Z",
     "shell.execute_reply": "2020-11-07T22:11:38.173804Z"
    },
    "papermill": {
     "duration": 0.050664,
     "end_time": "2020-11-07T22:11:38.174429",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.123765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictors = ['g-13','g-17','g-20','g-29','g-30','g-33','g-42','g-53','g-54','g-73','g-83','g-90','g-103','g-115','g-122','g-129','g-130','g-132','g-141','g-157','g-164','g-176','g-196','g-199','g-200','g-202','g-207','g-219','g-226','g-227','g-247','g-259','g-260','g-268','g-270','g-278','g-288','g-292','g-303','g-308','g-313','g-317','g-319','g-328','g-333','g-337','g-348','g-352','g-366','g-368','g-372','g-382','g-388','g-394','g-417','g-424','g-425','g-434','g-435','g-436','g-442','g-453','g-462','g-463','g-471','g-476','g-483','g-484','g-512','g-520','g-524','g-527','g-530','g-538','g-544','g-549','g-552','g-566','g-570','g-581','g-583','g-592','g-594','g-597','g-601','g-611','g-621','g-626','g-636','g-641','g-643','g-644','g-646','g-649','g-653','g-658','g-662','g-671','g-701','g-711','g-713','g-714','g-715','g-722','g-729','g-737','c-0','c-16','c-32','c-53','c-89']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "73558070-5142-4ddd-89cd-b86e4cd4a75a",
    "_uuid": "42ad87cf-23cc-469e-a95f-6aad1c77e47d",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.254022Z",
     "iopub.status.busy": "2020-11-07T22:11:38.253209Z",
     "iopub.status.idle": "2020-11-07T22:11:38.256622Z",
     "shell.execute_reply": "2020-11-07T22:11:38.256064Z"
    },
    "papermill": {
     "duration": 0.045711,
     "end_time": "2020-11-07T22:11:38.256729",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.211018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = sub.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.337094Z",
     "iopub.status.busy": "2020-11-07T22:11:38.336502Z",
     "iopub.status.idle": "2020-11-07T22:11:38.342297Z",
     "shell.execute_reply": "2020-11-07T22:11:38.342762Z"
    },
    "papermill": {
     "duration": 0.048512,
     "end_time": "2020-11-07T22:11:38.342882",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.294370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037814,
     "end_time": "2020-11-07T22:11:38.418819",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.381005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "9e4edee3-e66d-4d7f-8867-14c9711a1266",
    "_uuid": "7a82bf16-66c0-4ca9-9a3d-1ba945de3a15",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.499342Z",
     "iopub.status.busy": "2020-11-07T22:11:38.498662Z",
     "iopub.status.idle": "2020-11-07T22:11:38.503104Z",
     "shell.execute_reply": "2020-11-07T22:11:38.502552Z"
    },
    "papermill": {
     "duration": 0.046028,
     "end_time": "2020-11-07T22:11:38.503212",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.457184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed67160f-0448-4809-b3ee-540f0e1d16b6",
    "_uuid": "e7da134b-a5a3-4529-bcac-8fcf8294772c",
    "papermill": {
     "duration": 0.037132,
     "end_time": "2020-11-07T22:11:38.577618",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.540486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature Engineering/Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "a40fe33f-7d2f-4138-b8af-f6575d6104e4",
    "_uuid": "04b7cd59-14ee-4f65-bfd0-6e79428fbbd9",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.666203Z",
     "iopub.status.busy": "2020-11-07T22:11:38.665397Z",
     "iopub.status.idle": "2020-11-07T22:11:38.669372Z",
     "shell.execute_reply": "2020-11-07T22:11:38.670116Z"
    },
    "papermill": {
     "duration": 0.055925,
     "end_time": "2020-11-07T22:11:38.670303",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.614378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All the col that has genes and cells\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "25a30a96-a61b-488e-afc3-b673fc00c525",
    "_uuid": "e56a0dcf-057b-496a-88cf-dd405ba00af9",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.789059Z",
     "iopub.status.busy": "2020-11-07T22:11:38.788303Z",
     "iopub.status.idle": "2020-11-07T22:11:38.792334Z",
     "shell.execute_reply": "2020-11-07T22:11:38.791797Z"
    },
    "papermill": {
     "duration": 0.069672,
     "end_time": "2020-11-07T22:11:38.792433",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.722761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are actully adding the feature from the pca into the original data in here, it helps the score for some reason\n",
    "def pca(train, test, n_comp = 55, type_ = GENES, SEED = 123):\n",
    "    \n",
    "    kind = \"g\" if type_ == GENES else \"c\"\n",
    "    data = pd.concat([train[type_], test[type_]])\n",
    "    pca = PCA(n_components= n_comp, random_state = SEED)\n",
    "    data = pd.DataFrame(pca.fit_transform(data), columns = [f'pca_{kind}{i + 1}' for i in range(n_comp)])\n",
    "    train_ = data.iloc[:train.shape[0]]\n",
    "    test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n",
    "    train = pd.concat([train, train_], axis = 1)\n",
    "    test = pd.concat([test, test_], axis = 1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:38.879006Z",
     "iopub.status.busy": "2020-11-07T22:11:38.878040Z",
     "iopub.status.idle": "2020-11-07T22:11:38.979278Z",
     "shell.execute_reply": "2020-11-07T22:11:38.978595Z"
    },
    "papermill": {
     "duration": 0.149631,
     "end_time": "2020-11-07T22:11:38.979406",
     "exception": false,
     "start_time": "2020-11-07T22:11:38.829775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "test_features2 = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "train_targets = train_targets.iloc[train_features.index]\n",
    "train_features.reset_index(drop = True, inplace = True)\n",
    "test_features.reset_index(drop = True, inplace = True)\n",
    "test_features2.reset_index(drop = True, inplace = True)\n",
    "train_targets.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "167eb790-c44c-488a-a5d8-00a631765427",
    "_uuid": "3396e928-9df2-41af-8c35-b07880c3a179",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:39.115769Z",
     "iopub.status.busy": "2020-11-07T22:11:39.065153Z",
     "iopub.status.idle": "2020-11-07T22:11:41.333665Z",
     "shell.execute_reply": "2020-11-07T22:11:41.333069Z"
    },
    "papermill": {
     "duration": 2.315255,
     "end_time": "2020-11-07T22:11:41.333775",
     "exception": false,
     "start_time": "2020-11-07T22:11:39.018520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features, test_features = pca(train_features, test_features, n_comp = 80, type_ = GENES)\n",
    "train_features, test_features = pca(train_features, test_features, n_comp = 15, type_ = CELLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "877fb689-3259-43ce-87e1-f65ff61aee09",
    "_uuid": "7be02bf6-72ed-4853-987f-edb015922493",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:41.423178Z",
     "iopub.status.busy": "2020-11-07T22:11:41.421902Z",
     "iopub.status.idle": "2020-11-07T22:11:41.986487Z",
     "shell.execute_reply": "2020-11-07T22:11:41.985799Z"
    },
    "papermill": {
     "duration": 0.614559,
     "end_time": "2020-11-07T22:11:41.986617",
     "exception": false,
     "start_time": "2020-11-07T22:11:41.372058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Feature Selection with Variance Threshold\n",
    "#Had problems with submission on this\n",
    "c_n = [f for f in list(train_features.columns) if f not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]\n",
    "mask = (train_features[c_n].var() >= 0.65).values\n",
    "tmp = train_features[c_n].loc[:, mask]\n",
    "train_features = pd.concat([train_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n",
    "tmp = test_features[c_n].loc[:, mask]\n",
    "test_features = pd.concat([test_features[['sig_id', 'cp_type', 'cp_time', 'cp_dose']], tmp], axis=1)\n",
    "cols_numeric = [feat for feat in list(train_features.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:42.121002Z",
     "iopub.status.busy": "2020-11-07T22:11:42.120148Z",
     "iopub.status.idle": "2020-11-07T22:11:42.124594Z",
     "shell.execute_reply": "2020-11-07T22:11:42.125246Z"
    },
    "papermill": {
     "duration": 0.071348,
     "end_time": "2020-11-07T22:11:42.125452",
     "exception": false,
     "start_time": "2020-11-07T22:11:42.054104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All the col that has genes and cells\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:42.246486Z",
     "iopub.status.busy": "2020-11-07T22:11:42.245438Z",
     "iopub.status.idle": "2020-11-07T22:11:42.250092Z",
     "shell.execute_reply": "2020-11-07T22:11:42.251196Z"
    },
    "papermill": {
     "duration": 0.072213,
     "end_time": "2020-11-07T22:11:42.251382",
     "exception": false,
     "start_time": "2020-11-07T22:11:42.179169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_cluster(train, test, kind, n_cluster, str_kind):\n",
    "    \n",
    "    data = pd.concat([train[kind], test[kind]], ignore_index = True)\n",
    "    kmeans = KMeans(n_clusters = n_cluster, random_state = 0,).fit(data)\n",
    "    train[f'clusters_{str_kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "    test[f'clusters_{str_kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "    train = pd.get_dummies(train, columns = [f'clusters_{str_kind}'])\n",
    "    test = pd.get_dummies(test, columns = [f'clusters_{str_kind}'])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:42.381269Z",
     "iopub.status.busy": "2020-11-07T22:11:42.380124Z",
     "iopub.status.idle": "2020-11-07T22:11:42.382901Z",
     "shell.execute_reply": "2020-11-07T22:11:42.382103Z"
    },
    "papermill": {
     "duration": 0.075443,
     "end_time": "2020-11-07T22:11:42.383059",
     "exception": false,
     "start_time": "2020-11-07T22:11:42.307616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_features, test_features = create_cluster(train_features, test_features, GENES, 35, \"g\")\n",
    "# train_features, test_features = create_cluster(train_features, test_features, CELLS, 5, \"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:42.509356Z",
     "iopub.status.busy": "2020-11-07T22:11:42.508443Z",
     "iopub.status.idle": "2020-11-07T22:11:42.574627Z",
     "shell.execute_reply": "2020-11-07T22:11:42.575889Z"
    },
    "papermill": {
     "duration": 0.137156,
     "end_time": "2020-11-07T22:11:42.576065",
     "exception": false,
     "start_time": "2020-11-07T22:11:42.438909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train_features, test_features], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "0d1b2d54-fc20-4004-adf2-52d1bb8e0caa",
    "_uuid": "7f6bb532-a385-43d9-81de-73c63aac330f",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:11:42.705512Z",
     "iopub.status.busy": "2020-11-07T22:11:42.704486Z",
     "iopub.status.idle": "2020-11-07T22:12:02.173652Z",
     "shell.execute_reply": "2020-11-07T22:12:02.172794Z"
    },
    "papermill": {
     "duration": 19.540679,
     "end_time": "2020-11-07T22:12:02.173798",
     "exception": false,
     "start_time": "2020-11-07T22:11:42.633119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "scaler = GaussRankScaler()\n",
    "data[cols_numeric]  = scaler.fit_transform(data[cols_numeric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "de9aa3cb-01f0-4e29-b580-13286315aaa3",
    "_uuid": "2e587cf3-1478-4308-bbfc-5c5c1b4acade",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:02.261959Z",
     "iopub.status.busy": "2020-11-07T22:12:02.260633Z",
     "iopub.status.idle": "2020-11-07T22:12:02.323218Z",
     "shell.execute_reply": "2020-11-07T22:12:02.321942Z"
    },
    "papermill": {
     "duration": 0.10878,
     "end_time": "2020-11-07T22:12:02.323359",
     "exception": false,
     "start_time": "2020-11-07T22:12:02.214579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Spliting the data back into train and test\n",
    "train_features = data.iloc[:train_features.shape[0]].reset_index(drop = True)\n",
    "test_features = data.iloc[train_features.shape[0]:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "463ce4eb-b606-45c0-962e-2fb210eeb61e",
    "_uuid": "52ca14b2-5495-4fde-8ad8-71446c7ea3bf",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:02.417771Z",
     "iopub.status.busy": "2020-11-07T22:12:02.415502Z",
     "iopub.status.idle": "2020-11-07T22:12:02.420803Z",
     "shell.execute_reply": "2020-11-07T22:12:02.420186Z"
    },
    "papermill": {
     "duration": 0.058564,
     "end_time": "2020-11-07T22:12:02.420900",
     "exception": false,
     "start_time": "2020-11-07T22:12:02.362336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = list(train.columns[4:776])\n",
    "    features_c = list(train.columns[776:876])\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040609,
     "end_time": "2020-11-07T22:12:02.500829",
     "exception": false,
     "start_time": "2020-11-07T22:12:02.460220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "e81bae51-ad02-4047-87a5-cc786773b9c6",
    "_uuid": "ab77edfc-b403-46a2-9dcc-9857ab605f82",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:02.592185Z",
     "iopub.status.busy": "2020-11-07T22:12:02.591155Z",
     "iopub.status.idle": "2020-11-07T22:12:02.594376Z",
     "shell.execute_reply": "2020-11-07T22:12:02.593725Z"
    },
    "papermill": {
     "duration": 0.052974,
     "end_time": "2020-11-07T22:12:02.594476",
     "exception": false,
     "start_time": "2020-11-07T22:12:02.541502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data, target=False):\n",
    "    data.drop(\"sig_id\", axis=1, inplace=True)\n",
    "    if target:\n",
    "        return data\n",
    "    else:\n",
    "        data[\"cp_dose_time\"] = data[\"cp_dose\"] + \"_\" + data[\"cp_time\"].astype(\"str\")\n",
    "        data[\"cp_dose_time\"] = data[\"cp_dose_time\"].map({\"D1_24\":0 , \"D1_48\":1, \"D1_72\":2, \"D2_24\":3 , \"D2_48\":4, \"D2_72\":5})\n",
    "        data[\"cp_time\"] = data[\"cp_time\"].map({24:1, 48:2, 72:3})\n",
    "        #Doing this slightly improves the model\n",
    "        #https://www.kaggle.com/carlmcbrideellis/moa-setting-ctl-vehicle-0-improves-score\n",
    "        data.at[data['cp_type'].str.contains('ctl_vehicle'),data.filter(regex='-.*').columns] = 0.0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "27aa2178-9d1a-420f-a321-e95ddd73711f",
    "_uuid": "2d4ee019-392b-4a08-8432-c44b36c978a7",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:02.683630Z",
     "iopub.status.busy": "2020-11-07T22:12:02.682493Z",
     "iopub.status.idle": "2020-11-07T22:12:03.383383Z",
     "shell.execute_reply": "2020-11-07T22:12:03.382784Z"
    },
    "papermill": {
     "duration": 0.749389,
     "end_time": "2020-11-07T22:12:03.383520",
     "exception": false,
     "start_time": "2020-11-07T22:12:02.634131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = preprocess(train_features)\n",
    "train_targets = preprocess(train_targets, target=True)\n",
    "train_features = pd.get_dummies(train_features)\n",
    "test_features = preprocess(test_features)\n",
    "test_features = pd.get_dummies(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "d587cbf7-ee3a-4c92-aeae-6ae47a51f5d7",
    "_uuid": "fa9c6b2c-4e7f-4444-8dc3-8f476bc9a77e",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:03.471500Z",
     "iopub.status.busy": "2020-11-07T22:12:03.470339Z",
     "iopub.status.idle": "2020-11-07T22:12:08.516644Z",
     "shell.execute_reply": "2020-11-07T22:12:08.515787Z"
    },
    "papermill": {
     "duration": 5.094006,
     "end_time": "2020-11-07T22:12:08.516769",
     "exception": false,
     "start_time": "2020-11-07T22:12:03.422763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features, test_features = fe_stats(train_features, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "ea9e975d-8a2e-4083-8cb5-015961864045",
    "_uuid": "fb1152ad-f4b3-4647-abcb-6c37d6d1304f",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:08.617336Z",
     "iopub.status.busy": "2020-11-07T22:12:08.616120Z",
     "iopub.status.idle": "2020-11-07T22:12:08.658945Z",
     "shell.execute_reply": "2020-11-07T22:12:08.659579Z"
    },
    "papermill": {
     "duration": 0.096264,
     "end_time": "2020-11-07T22:12:08.659729",
     "exception": false,
     "start_time": "2020-11-07T22:12:08.563465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cp_time</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>g-6</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>...</th>\n",
       "      <th>c_sum</th>\n",
       "      <th>c_mean</th>\n",
       "      <th>c_std</th>\n",
       "      <th>c_kurt</th>\n",
       "      <th>c_skew</th>\n",
       "      <th>gc_sum</th>\n",
       "      <th>gc_mean</th>\n",
       "      <th>gc_std</th>\n",
       "      <th>gc_kurt</th>\n",
       "      <th>gc_skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.731433</td>\n",
       "      <td>-0.254104</td>\n",
       "      <td>-0.615315</td>\n",
       "      <td>-0.194236</td>\n",
       "      <td>-0.772257</td>\n",
       "      <td>-1.061067</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.513989</td>\n",
       "      <td>-0.130963</td>\n",
       "      <td>...</td>\n",
       "      <td>29.950827</td>\n",
       "      <td>0.299508</td>\n",
       "      <td>0.552298</td>\n",
       "      <td>0.115705</td>\n",
       "      <td>-0.192222</td>\n",
       "      <td>39.268721</td>\n",
       "      <td>0.045033</td>\n",
       "      <td>0.604015</td>\n",
       "      <td>-0.065454</td>\n",
       "      <td>-0.036394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>0.213820</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>0.849834</td>\n",
       "      <td>0.482972</td>\n",
       "      <td>0.217779</td>\n",
       "      <td>0.363904</td>\n",
       "      <td>-0.298420</td>\n",
       "      <td>0.730029</td>\n",
       "      <td>...</td>\n",
       "      <td>29.694569</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.477884</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>-0.400339</td>\n",
       "      <td>42.978577</td>\n",
       "      <td>0.049287</td>\n",
       "      <td>0.584949</td>\n",
       "      <td>-0.125831</td>\n",
       "      <td>0.005071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.462792</td>\n",
       "      <td>0.981712</td>\n",
       "      <td>-0.091777</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>1.055526</td>\n",
       "      <td>0.161005</td>\n",
       "      <td>0.242395</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.975415</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.183705</td>\n",
       "      <td>-0.041837</td>\n",
       "      <td>0.507697</td>\n",
       "      <td>-0.395305</td>\n",
       "      <td>0.193820</td>\n",
       "      <td>-19.827301</td>\n",
       "      <td>-0.022738</td>\n",
       "      <td>0.622199</td>\n",
       "      <td>-0.309186</td>\n",
       "      <td>-0.002089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.531916</td>\n",
       "      <td>-0.269127</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>1.595325</td>\n",
       "      <td>-0.621171</td>\n",
       "      <td>-1.509230</td>\n",
       "      <td>0.206249</td>\n",
       "      <td>-0.066407</td>\n",
       "      <td>-0.889590</td>\n",
       "      <td>...</td>\n",
       "      <td>-54.176846</td>\n",
       "      <td>-0.541768</td>\n",
       "      <td>0.585042</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>1.458391</td>\n",
       "      <td>-79.440660</td>\n",
       "      <td>-0.091102</td>\n",
       "      <td>0.733257</td>\n",
       "      <td>-0.931805</td>\n",
       "      <td>0.258173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.341668</td>\n",
       "      <td>0.808567</td>\n",
       "      <td>0.604615</td>\n",
       "      <td>0.951946</td>\n",
       "      <td>-0.630950</td>\n",
       "      <td>-0.243933</td>\n",
       "      <td>-0.103294</td>\n",
       "      <td>-0.684346</td>\n",
       "      <td>0.741863</td>\n",
       "      <td>...</td>\n",
       "      <td>22.473358</td>\n",
       "      <td>0.224734</td>\n",
       "      <td>0.565969</td>\n",
       "      <td>-0.116749</td>\n",
       "      <td>-0.319863</td>\n",
       "      <td>22.767152</td>\n",
       "      <td>0.026109</td>\n",
       "      <td>0.743760</td>\n",
       "      <td>-0.278707</td>\n",
       "      <td>-0.196499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>3</td>\n",
       "      <td>0.089366</td>\n",
       "      <td>0.176202</td>\n",
       "      <td>-0.226762</td>\n",
       "      <td>-0.244678</td>\n",
       "      <td>0.396468</td>\n",
       "      <td>-0.094758</td>\n",
       "      <td>-0.130161</td>\n",
       "      <td>-0.582338</td>\n",
       "      <td>0.056587</td>\n",
       "      <td>...</td>\n",
       "      <td>8.616747</td>\n",
       "      <td>0.086167</td>\n",
       "      <td>0.503629</td>\n",
       "      <td>-0.177283</td>\n",
       "      <td>0.462609</td>\n",
       "      <td>48.865715</td>\n",
       "      <td>0.056039</td>\n",
       "      <td>0.479665</td>\n",
       "      <td>-0.048608</td>\n",
       "      <td>-0.005369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>1</td>\n",
       "      <td>0.072056</td>\n",
       "      <td>-0.131998</td>\n",
       "      <td>-0.499092</td>\n",
       "      <td>-0.479222</td>\n",
       "      <td>0.667644</td>\n",
       "      <td>0.530397</td>\n",
       "      <td>0.333563</td>\n",
       "      <td>-0.356318</td>\n",
       "      <td>0.048109</td>\n",
       "      <td>...</td>\n",
       "      <td>23.792189</td>\n",
       "      <td>0.237922</td>\n",
       "      <td>0.509802</td>\n",
       "      <td>-0.278833</td>\n",
       "      <td>0.097052</td>\n",
       "      <td>37.947722</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>0.544859</td>\n",
       "      <td>0.228663</td>\n",
       "      <td>-0.167903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.346695</td>\n",
       "      <td>-0.369847</td>\n",
       "      <td>0.878667</td>\n",
       "      <td>-0.735139</td>\n",
       "      <td>0.613056</td>\n",
       "      <td>-0.194656</td>\n",
       "      <td>-0.446960</td>\n",
       "      <td>0.479298</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>...</td>\n",
       "      <td>3.316741</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.562406</td>\n",
       "      <td>-0.320131</td>\n",
       "      <td>0.404143</td>\n",
       "      <td>8.818301</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.554599</td>\n",
       "      <td>0.104428</td>\n",
       "      <td>0.089995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>1</td>\n",
       "      <td>0.490027</td>\n",
       "      <td>0.329593</td>\n",
       "      <td>0.157679</td>\n",
       "      <td>0.732402</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.034881</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.405150</td>\n",
       "      <td>-0.742506</td>\n",
       "      <td>...</td>\n",
       "      <td>31.153129</td>\n",
       "      <td>0.311531</td>\n",
       "      <td>0.738239</td>\n",
       "      <td>-0.326836</td>\n",
       "      <td>-0.320763</td>\n",
       "      <td>87.500526</td>\n",
       "      <td>0.100345</td>\n",
       "      <td>0.780387</td>\n",
       "      <td>-0.947005</td>\n",
       "      <td>-0.175951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.939748</td>\n",
       "      <td>-0.153659</td>\n",
       "      <td>0.695101</td>\n",
       "      <td>-0.365699</td>\n",
       "      <td>-1.352665</td>\n",
       "      <td>-1.150624</td>\n",
       "      <td>1.045868</td>\n",
       "      <td>1.472960</td>\n",
       "      <td>0.980444</td>\n",
       "      <td>...</td>\n",
       "      <td>-67.673459</td>\n",
       "      <td>-0.676735</td>\n",
       "      <td>0.677247</td>\n",
       "      <td>2.536177</td>\n",
       "      <td>1.765573</td>\n",
       "      <td>-108.516080</td>\n",
       "      <td>-0.124445</td>\n",
       "      <td>0.825195</td>\n",
       "      <td>-0.948120</td>\n",
       "      <td>0.206040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows × 965 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cp_time       g-0       g-2       g-3       g-4       g-5       g-6  \\\n",
       "0            1  0.731433 -0.254104 -0.615315 -0.194236 -0.772257 -1.061067   \n",
       "1            3  0.017020  0.213820  0.027107  0.849834  0.482972  0.217779   \n",
       "2            2  0.462792  0.981712 -0.091777 -0.039121  1.055526  0.161005   \n",
       "3            2 -0.531916 -0.269127  0.455184  1.595325 -0.621171 -1.509230   \n",
       "4            3 -0.341668  0.808567  0.604615  0.951946 -0.630950 -0.243933   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "21943        3  0.089366  0.176202 -0.226762 -0.244678  0.396468 -0.094758   \n",
       "21944        1  0.072056 -0.131998 -0.499092 -0.479222  0.667644  0.530397   \n",
       "21945        1 -1.346695 -0.369847  0.878667 -0.735139  0.613056 -0.194656   \n",
       "21946        1  0.490027  0.329593  0.157679  0.732402  0.001062  0.034881   \n",
       "21947        3 -0.939748 -0.153659  0.695101 -0.365699 -1.352665 -1.150624   \n",
       "\n",
       "            g-7       g-8       g-9  ...      c_sum    c_mean     c_std  \\\n",
       "0      0.005623  0.513989 -0.130963  ...  29.950827  0.299508  0.552298   \n",
       "1      0.363904 -0.298420  0.730029  ...  29.694569  0.296946  0.477884   \n",
       "2      0.242395  0.050070  0.975415  ...  -4.183705 -0.041837  0.507697   \n",
       "3      0.206249 -0.066407 -0.889590  ... -54.176846 -0.541768  0.585042   \n",
       "4     -0.103294 -0.684346  0.741863  ...  22.473358  0.224734  0.565969   \n",
       "...         ...       ...       ...  ...        ...       ...       ...   \n",
       "21943 -0.130161 -0.582338  0.056587  ...   8.616747  0.086167  0.503629   \n",
       "21944  0.333563 -0.356318  0.048109  ...  23.792189  0.237922  0.509802   \n",
       "21945 -0.446960  0.479298  0.549999  ...   3.316741  0.033167  0.562406   \n",
       "21946  0.079245  0.405150 -0.742506  ...  31.153129  0.311531  0.738239   \n",
       "21947  1.045868  1.472960  0.980444  ... -67.673459 -0.676735  0.677247   \n",
       "\n",
       "         c_kurt    c_skew      gc_sum   gc_mean    gc_std   gc_kurt   gc_skew  \n",
       "0      0.115705 -0.192222   39.268721  0.045033  0.604015 -0.065454 -0.036394  \n",
       "1      0.002827 -0.400339   42.978577  0.049287  0.584949 -0.125831  0.005071  \n",
       "2     -0.395305  0.193820  -19.827301 -0.022738  0.622199 -0.309186 -0.002089  \n",
       "3      1.558884  1.458391  -79.440660 -0.091102  0.733257 -0.931805  0.258173  \n",
       "4     -0.116749 -0.319863   22.767152  0.026109  0.743760 -0.278707 -0.196499  \n",
       "...         ...       ...         ...       ...       ...       ...       ...  \n",
       "21943 -0.177283  0.462609   48.865715  0.056039  0.479665 -0.048608 -0.005369  \n",
       "21944 -0.278833  0.097052   37.947722  0.043518  0.544859  0.228663 -0.167903  \n",
       "21945 -0.320131  0.404143    8.818301  0.010113  0.554599  0.104428  0.089995  \n",
       "21946 -0.326836 -0.320763   87.500526  0.100345  0.780387 -0.947005 -0.175951  \n",
       "21947  2.536177  1.765573 -108.516080 -0.124445  0.825195 -0.948120  0.206040  \n",
       "\n",
       "[21948 rows x 965 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b88ba9a1-8c8f-4017-8f66-11502b1a66c0",
    "_uuid": "ec5a0a57-9fe2-4010-b301-4eebe652da87",
    "papermill": {
     "duration": 0.040973,
     "end_time": "2020-11-07T22:12:08.748907",
     "exception": false,
     "start_time": "2020-11-07T22:12:08.707934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Error/Loss Metric and Prediction clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_cell_guid": "efaf4180-50af-4dcc-b3b0-1c4038301ce5",
    "_uuid": "16856a4e-8060-40d7-ac95-632d81b42b95",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:08.837962Z",
     "iopub.status.busy": "2020-11-07T22:12:08.837074Z",
     "iopub.status.idle": "2020-11-07T22:12:08.840314Z",
     "shell.execute_reply": "2020-11-07T22:12:08.839629Z"
    },
    "papermill": {
     "duration": 0.050745,
     "end_time": "2020-11-07T22:12:08.840422",
     "exception": false,
     "start_time": "2020-11-07T22:12:08.789677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_log_loss(y_true, y_pred):\n",
    "    losses = []\n",
    "    for col in y_true.columns:\n",
    "        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:08.930583Z",
     "iopub.status.busy": "2020-11-07T22:12:08.929696Z",
     "iopub.status.idle": "2020-11-07T22:12:08.933759Z",
     "shell.execute_reply": "2020-11-07T22:12:08.933089Z"
    },
    "papermill": {
     "duration": 0.052191,
     "end_time": "2020-11-07T22:12:08.933868",
     "exception": false,
     "start_time": "2020-11-07T22:12:08.881677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logloss(y_true, y_pred):\n",
    "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
    "    return -tf.keras.backend.mean(y_true*tf.keras.backend.log(y_pred) + (1-y_true)*tf.keras.backend.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "516b17c4-4697-4194-a9a1-e07020699394",
    "_uuid": "ad337b21-38a2-4984-b182-05c23e91307b",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:09.026131Z",
     "iopub.status.busy": "2020-11-07T22:12:09.025307Z",
     "iopub.status.idle": "2020-11-07T22:12:09.027840Z",
     "shell.execute_reply": "2020-11-07T22:12:09.028343Z"
    },
    "papermill": {
     "duration": 0.04863,
     "end_time": "2020-11-07T22:12:09.028470",
     "exception": false,
     "start_time": "2020-11-07T22:12:08.979840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip(y_pred, c_min, c_max):\n",
    "    return np.clip(y_pred, c_min, c_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "c1529cef-55df-4c59-a23d-2b21e6dd49ba",
    "_uuid": "6af491c3-aebc-4fd9-b2f8-1a78fe1c71a4",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:09.116696Z",
     "iopub.status.busy": "2020-11-07T22:12:09.116007Z",
     "iopub.status.idle": "2020-11-07T22:12:09.120297Z",
     "shell.execute_reply": "2020-11-07T22:12:09.119701Z"
    },
    "papermill": {
     "duration": 0.049769,
     "end_time": "2020-11-07T22:12:09.120405",
     "exception": false,
     "start_time": "2020-11-07T22:12:09.070636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_min = 0.001\n",
    "p_max = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1d569890-835d-4698-b4d3-52ae6ee6adea",
    "_uuid": "af8f3cde-4a98-4edb-a255-6d845e38a274",
    "papermill": {
     "duration": 0.041825,
     "end_time": "2020-11-07T22:12:09.204372",
     "exception": false,
     "start_time": "2020-11-07T22:12:09.162547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Model and Perform KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:09.309592Z",
     "iopub.status.busy": "2020-11-07T22:12:09.308535Z",
     "iopub.status.idle": "2020-11-07T22:12:09.311898Z",
     "shell.execute_reply": "2020-11-07T22:12:09.311193Z"
    },
    "papermill": {
     "duration": 0.064699,
     "end_time": "2020-11-07T22:12:09.312008",
     "exception": false,
     "start_time": "2020-11-07T22:12:09.247309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resnet(n_features, n_features_2, n_labels, label_smoothing = 0.0005):    \n",
    "    input_1 = L.Input(shape = (n_features,), name = 'Input1')\n",
    "    input_2 = L.Input(shape = (n_features_2,), name = 'Input2')\n",
    "\n",
    "    head_1 = M.Sequential([\n",
    "        L.BatchNormalization(),\n",
    "        L.Dropout(0.2),\n",
    "        L.Dense(512), \n",
    "        L.LeakyReLU(alpha = 0.10059420295821832),\n",
    "        L.BatchNormalization(),\n",
    "        L.Dense(256),\n",
    "        L.Dropout(0.2),\n",
    "        L.LeakyReLU(alpha = 0.10059420295821832),\n",
    "        L.Dense(256),\n",
    "        L.LeakyReLU(alpha = 0.10059420295821832),\n",
    "        ],name='Head1') \n",
    "\n",
    "    input_3 = head_1(input_1)\n",
    "    input_3_concat = L.Concatenate()([input_2, input_3])\n",
    "\n",
    "    head_2 = M.Sequential([\n",
    "        L.BatchNormalization(),\n",
    "        L.Dropout(0.3),\n",
    "        L.Dense(1024),\n",
    "        L.LeakyReLU(alpha = 0.10059420295821832),\n",
    "        L.BatchNormalization(),\n",
    "        L.Dropout(0.4),\n",
    "        L.Dense(512, \"elu\"),\n",
    "        L.BatchNormalization(),\n",
    "        L.Dropout(0.2),\n",
    "        L.Dense(256),\n",
    "        L.LeakyReLU(alpha = 0.10059420295821832),\n",
    "        L.BatchNormalization(),\n",
    "        L.Dense(256, \"elu\"),\n",
    "        ],name='Head2')\n",
    "\n",
    "    input_4 = head_2(input_3_concat)\n",
    "    input_4_avg = L.Average()([input_3, input_4]) \n",
    "\n",
    "    head_3 = M.Sequential([\n",
    "        L.BatchNormalization(),\n",
    "        L.Dense(512, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        L.BatchNormalization(),\n",
    "        L.Dropout(0.2),\n",
    "        L.Dense(n_labels, kernel_initializer='lecun_normal', activation='selu'),\n",
    "        L.BatchNormalization(),\n",
    "        L.Dense(n_labels, activation=\"sigmoid\")\n",
    "        ],name='Head3')\n",
    "\n",
    "    output = head_3(input_4_avg)\n",
    "\n",
    "\n",
    "    model = M.Model(inputs = [input_1, input_2], outputs = output)\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing), metrics=logloss)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "244e8f03-2225-496c-8215-c4487bfd8e61",
    "_uuid": "65dcb7f8-edf5-4544-a311-fa8202b8c793",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:09.406348Z",
     "iopub.status.busy": "2020-11-07T22:12:09.405424Z",
     "iopub.status.idle": "2020-11-07T22:12:09.419402Z",
     "shell.execute_reply": "2020-11-07T22:12:09.418853Z"
    },
    "papermill": {
     "duration": 0.065622,
     "end_time": "2020-11-07T22:12:09.419529",
     "exception": false,
     "start_time": "2020-11-07T22:12:09.353907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_folds(folds):\n",
    "    models = []\n",
    "    oof_preds_score = []\n",
    "    for i in range(5):\n",
    "        \n",
    "        seed = 34 + i\n",
    "        seed_everything(seed)\n",
    "        print(f\"REAPEAT NUMBER {i + 1} SEED {seed}\")\n",
    "        \n",
    "        model_list = list()\n",
    "        kfold = MultilabelStratifiedKFold(folds, shuffle = True)\n",
    "        oof_preds = train_targets.copy()\n",
    "        \n",
    "        \n",
    "        for fold, (train_ind, val_ind) in enumerate(kfold.split(train_targets, train_targets)):\n",
    "            \n",
    "            print(f\"fold {fold + 1}\")\n",
    "            print(\"\\n\")\n",
    "            print(\"ResNet\")\n",
    "            \n",
    "            train2 = train_features2.iloc[train_ind][predictors].values\n",
    "            train_val2 = train_features2.iloc[val_ind][predictors].values\n",
    "            \n",
    "            model = resnet(train_features.shape[1], train2.shape[1], 206)\n",
    "            \n",
    "            checkpoint_path = f'repeat:{seed}_Fold:{fold}.hdf5'\n",
    "            cb_checkpt = C.ModelCheckpoint(checkpoint_path, monitor = 'val_logloss', verbose = 1, save_best_only = True, save_weights_only = True, mode = 'auto')\n",
    "            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_logloss',\n",
    "                                                         mode = 'min',\n",
    "                                                         factor = 0.1,\n",
    "                                                         patience = 2,\n",
    "                                                         verbose = 1)\n",
    "            early = tf.keras.callbacks.EarlyStopping(monitor = 'val_logloss',\n",
    "                                                          mode = 'min',\n",
    "                                                          patience = 6,\n",
    "                                                          restore_best_weights = True,\n",
    "                                                          verbose = 1)\n",
    "            \n",
    "            model.fit([train_features.values[train_ind], train2],\n",
    "                    train_targets.values[train_ind], validation_data=([train_features.values[val_ind], train_val2], train_targets.values[val_ind]),\n",
    "                    epochs=100, batch_size=128, verbose=2, callbacks = [reduce_lr, early, cb_checkpt])\n",
    "            model_list.append(model)\n",
    "            model.load_weights(checkpoint_path)\n",
    "            oof_preds.loc[val_ind, :] = clip(model.predict([train_features.values[val_ind], train_val2]), p_min, p_max)\n",
    "            \n",
    "        m, oof = model_list, oof_preds\n",
    "        models = models + m\n",
    "        oof_preds_score.append(oof)\n",
    "    return models, oof_preds_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "29fa0f27-87b8-4fce-bc79-f669825381fa",
    "_uuid": "237db3f7-035b-4c21-87a1-1dbc90f6176a",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:12:09.509708Z",
     "iopub.status.busy": "2020-11-07T22:12:09.508915Z",
     "iopub.status.idle": "2020-11-07T22:37:27.699427Z",
     "shell.execute_reply": "2020-11-07T22:37:27.698615Z"
    },
    "papermill": {
     "duration": 1518.236779,
     "end_time": "2020-11-07T22:37:27.699570",
     "exception": false,
     "start_time": "2020-11-07T22:12:09.462791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAPEAT NUMBER 1 SEED 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04146, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.3439 - logloss: 0.3422 - val_loss: 0.0424 - val_logloss: 0.0415\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04146 to 0.02215, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0235 - val_logloss: 0.0222\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02215 to 0.01886, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0203 - val_loss: 0.0203 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01886 to 0.01786, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01786 to 0.01754, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01754 to 0.01748, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0185 - logloss: 0.0171 - val_loss: 0.0190 - val_logloss: 0.0175\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01748 to 0.01689, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01689 to 0.01687, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01687 to 0.01676, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01676\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01676 to 0.01673, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0185 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01673 to 0.01644, saving model to repeat:34_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0132 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01644\n",
      "155/155 - 2s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 00018: early stopping\n",
      "fold 2\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04271, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.3436 - logloss: 0.3419 - val_loss: 0.0436 - val_logloss: 0.0427\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04271 to 0.02169, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0230 - val_logloss: 0.0217\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02169 to 0.01911, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0205 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01911 to 0.01796, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0194 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01796 to 0.01741, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0188 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01741 to 0.01731, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0187 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01731 to 0.01722, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01722 to 0.01714, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01714 to 0.01702, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01702 to 0.01701, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01701 to 0.01694, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01694 to 0.01660, saving model to repeat:34_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01660\n",
      "155/155 - 2s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 00018: early stopping\n",
      "fold 3\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04526, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.3453 - logloss: 0.3436 - val_loss: 0.0462 - val_logloss: 0.0453\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04526 to 0.02216, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0287 - logloss: 0.0276 - val_loss: 0.0236 - val_logloss: 0.0222\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02216 to 0.01894, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0218 - logloss: 0.0204 - val_loss: 0.0205 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01894 to 0.01778, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0194 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01778 to 0.01732, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss did not improve from 0.01732\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01732 to 0.01688, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01688 to 0.01678, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01678\n",
      "155/155 - 1s - loss: 0.0171 - logloss: 0.0157 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01678 to 0.01651, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0162 - logloss: 0.0147 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01651 to 0.01651, saving model to repeat:34_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 00017: early stopping\n",
      "fold 4\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03868, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.3432 - logloss: 0.3415 - val_loss: 0.0399 - val_logloss: 0.0387\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03868 to 0.02145, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0230 - val_logloss: 0.0214\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02145 to 0.01848, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0202 - val_logloss: 0.0185\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01848 to 0.01735, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0200 - logloss: 0.0185 - val_loss: 0.0191 - val_logloss: 0.0174\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01735 to 0.01688, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0187 - val_logloss: 0.0169\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01688 to 0.01672, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0186 - val_logloss: 0.0167\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01672 to 0.01638, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01638 to 0.01614, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0181 - val_logloss: 0.0161\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01614 to 0.01600, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0179 - val_logloss: 0.0160\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01600\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0180 - val_logloss: 0.0161\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01600\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0149 - val_loss: 0.0180 - val_logloss: 0.0160\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01600 to 0.01567, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01567 to 0.01563, saving model to repeat:34_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0176 - val_logloss: 0.0156\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01563\n",
      "155/155 - 2s - loss: 0.0146 - logloss: 0.0132 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01563\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01563\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01563\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01563\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 19/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01563\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0176 - val_logloss: 0.0157\n",
      "Epoch 00019: early stopping\n",
      "fold 5\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04009, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.3435 - logloss: 0.3418 - val_loss: 0.0411 - val_logloss: 0.0401\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04009 to 0.02191, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0232 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02191 to 0.01896, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0204 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01896 to 0.01804, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01804 to 0.01756, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01756 to 0.01736, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0189 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss did not improve from 0.01736\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0190 - val_logloss: 0.0174\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01736 to 0.01692, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0162 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0187 - val_logloss: 0.0172\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01692 to 0.01662, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0182 - val_logloss: 0.0166\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01662 to 0.01660, saving model to repeat:34_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0166\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0166\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0182 - val_logloss: 0.0166\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01660\n",
      "155/155 - 2s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 00018: early stopping\n",
      "fold 6\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04068, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.3423 - logloss: 0.3407 - val_loss: 0.0417 - val_logloss: 0.0407\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04068 to 0.02162, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0229 - val_logloss: 0.0216\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02162 to 0.01916, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0206 - val_logloss: 0.0192\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01916 to 0.01813, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0201 - logloss: 0.0186 - val_loss: 0.0196 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01813 to 0.01764, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0192 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01764 to 0.01727, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01727 to 0.01700, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01700\n",
      "155/155 - 2s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01700 to 0.01689, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01689\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0152 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01689\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01689 to 0.01676, saving model to repeat:34_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01676\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01676\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01676\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01676\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01676\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01676\n",
      "155/155 - 2s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 00018: early stopping\n",
      "fold 7\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03917, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.3436 - logloss: 0.3419 - val_loss: 0.0402 - val_logloss: 0.0392\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03917 to 0.02185, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0287 - logloss: 0.0276 - val_loss: 0.0233 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02185 to 0.01888, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0204 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01888 to 0.01794, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0196 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01794 to 0.01742, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01742 to 0.01698, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01698 to 0.01678, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01678 to 0.01671, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0185 - val_logloss: 0.0167\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01671 to 0.01655, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01655\n",
      "155/155 - 2s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01655 to 0.01653, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01653 to 0.01624, saving model to repeat:34_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0180 - val_logloss: 0.0162\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01624\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01624\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01624\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01624\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01624\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0163\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01624\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0163\n",
      "Epoch 00018: early stopping\n",
      "fold 8\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04181, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.3439 - logloss: 0.3422 - val_loss: 0.0427 - val_logloss: 0.0418\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04181 to 0.02195, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0232 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02195 to 0.01915, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0205 - val_logloss: 0.0192\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01915 to 0.01795, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01795 to 0.01753, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01753 to 0.01728, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0187 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss did not improve from 0.01728\n",
      "155/155 - 2s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0187 - val_logloss: 0.0174\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01728 to 0.01704, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0163 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01704 to 0.01699, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01699\n",
      "155/155 - 1s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01699 to 0.01670, saving model to repeat:34_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0159 - logloss: 0.0145 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01670\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01670\n",
      "155/155 - 2s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01670\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01670\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01670\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01670\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 00017: early stopping\n",
      "fold 9\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04134, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.3428 - logloss: 0.3411 - val_loss: 0.0423 - val_logloss: 0.0413\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04134 to 0.02152, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0229 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02152 to 0.01898, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0205 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01898 to 0.01797, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0196 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01797 to 0.01732, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0193 - logloss: 0.0178 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01732 to 0.01717, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0189 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01717 to 0.01673, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01673 to 0.01660, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01660 to 0.01660, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01660 to 0.01651, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01651 to 0.01627, saving model to repeat:34_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01627\n",
      "155/155 - 2s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 00017: early stopping\n",
      "fold 10\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04074, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.3427 - logloss: 0.3410 - val_loss: 0.0417 - val_logloss: 0.0407\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04074 to 0.02185, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0232 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02185 to 0.01911, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0218 - logloss: 0.0205 - val_loss: 0.0205 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01911 to 0.01801, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0195 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01801 to 0.01741, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0189 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01741 to 0.01717, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01717 to 0.01702, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01702\n",
      "155/155 - 1s - loss: 0.0178 - logloss: 0.0163 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01702 to 0.01689, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01689 to 0.01683, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01683\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01683 to 0.01656, saving model to repeat:34_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01656\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01656\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01656\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0131 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01656\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01656\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01656\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 00018: early stopping\n",
      "REAPEAT NUMBER 2 SEED 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04557, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.3463 - logloss: 0.3446 - val_loss: 0.0466 - val_logloss: 0.0456\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04557 to 0.02129, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0289 - logloss: 0.0278 - val_loss: 0.0228 - val_logloss: 0.0213\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02129 to 0.01881, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0219 - logloss: 0.0206 - val_loss: 0.0205 - val_logloss: 0.0188\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01881 to 0.01788, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0196 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01788 to 0.01760, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0193 - logloss: 0.0179 - val_loss: 0.0193 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01760 to 0.01706, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0188 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss did not improve from 0.01706\n",
      "155/155 - 2s - loss: 0.0183 - logloss: 0.0168 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01706 to 0.01697, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0178 - logloss: 0.0163 - val_loss: 0.0188 - val_logloss: 0.0170\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01697 to 0.01644, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0169 - logloss: 0.0155 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01644 to 0.01641, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0166 - logloss: 0.0152 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01641 to 0.01639, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0165 - logloss: 0.0150 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01639 to 0.01639, saving model to repeat:35_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01639\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01639\n",
      "155/155 - 2s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01639\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0147 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01639\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0147 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01639\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01639\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0147 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 00018: early stopping\n",
      "fold 2\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04077, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.3424 - logloss: 0.3407 - val_loss: 0.0417 - val_logloss: 0.0408\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04077 to 0.02214, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0235 - val_logloss: 0.0221\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02214 to 0.01908, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0206 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01908 to 0.01813, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0198 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01813 to 0.01764, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0193 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01764 to 0.01742, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0191 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01742 to 0.01714, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0188 - val_logloss: 0.0171\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01714 to 0.01710, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0188 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01710 to 0.01697, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01697 to 0.01691, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0186 - val_logloss: 0.0169\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01691\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01691 to 0.01669, saving model to repeat:35_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0132 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01669\n",
      "155/155 - 2s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 00018: early stopping\n",
      "fold 3\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04404, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.3437 - logloss: 0.3420 - val_loss: 0.0449 - val_logloss: 0.0440\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04404 to 0.02146, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0273 - val_loss: 0.0228 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02146 to 0.01900, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0217 - logloss: 0.0203 - val_loss: 0.0204 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01900 to 0.01806, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0196 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01806 to 0.01772, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0193 - val_logloss: 0.0177\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01772 to 0.01747, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0191 - val_logloss: 0.0175\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01747 to 0.01725, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01725 to 0.01713, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0178 - logloss: 0.0163 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01713 to 0.01704, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01704\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0154 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01704 to 0.01674, saving model to repeat:35_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0159 - logloss: 0.0144 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 00017: early stopping\n",
      "fold 4\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03833, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.3432 - logloss: 0.3416 - val_loss: 0.0395 - val_logloss: 0.0383\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03833 to 0.02149, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0230 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02149 to 0.01855, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0202 - val_logloss: 0.0186\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01855 to 0.01773, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0177\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01773 to 0.01726, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01726 to 0.01708, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0189 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01708 to 0.01682, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01682 to 0.01679, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0178 - logloss: 0.0163 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01679 to 0.01662, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0159 - val_loss: 0.0185 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01662 to 0.01660, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01660 to 0.01655, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0150 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01655 to 0.01627, saving model to repeat:35_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01627\n",
      "155/155 - 2s - loss: 0.0150 - logloss: 0.0136 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0132 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 00018: early stopping\n",
      "fold 5\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04298, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.3437 - logloss: 0.3420 - val_loss: 0.0439 - val_logloss: 0.0430\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04298 to 0.02149, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0228 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02149 to 0.01875, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0203 - val_logloss: 0.0187\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01875 to 0.01785, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0194 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01785 to 0.01731, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01731 to 0.01709, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01709 to 0.01675, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01675\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01675 to 0.01655, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01655\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01655\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01655 to 0.01627, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01627 to 0.01627, saving model to repeat:35_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01627\n",
      "155/155 - 2s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 19/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 00019: early stopping\n",
      "fold 6\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04089, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.3425 - logloss: 0.3408 - val_loss: 0.0419 - val_logloss: 0.0409\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04089 to 0.02152, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0230 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02152 to 0.01894, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0219 - logloss: 0.0206 - val_loss: 0.0205 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01894 to 0.01790, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0202 - logloss: 0.0187 - val_loss: 0.0194 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01790 to 0.01718, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0193 - logloss: 0.0178 - val_loss: 0.0187 - val_logloss: 0.0172\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01718 to 0.01697, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01697 to 0.01673, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01673 to 0.01652, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01652 to 0.01652, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01652 to 0.01640, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0180 - val_logloss: 0.0164\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0159 - logloss: 0.0145 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01640 to 0.01609, saving model to repeat:35_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0178 - val_logloss: 0.0161\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01609\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0178 - val_logloss: 0.0161\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01609\n",
      "155/155 - 1s - loss: 0.0141 - logloss: 0.0127 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01609\n",
      "155/155 - 1s - loss: 0.0139 - logloss: 0.0124 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01609\n",
      "155/155 - 1s - loss: 0.0139 - logloss: 0.0124 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01609\n",
      "155/155 - 1s - loss: 0.0137 - logloss: 0.0123 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01609\n",
      "155/155 - 1s - loss: 0.0138 - logloss: 0.0124 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 00019: early stopping\n",
      "fold 7\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04137, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.3434 - logloss: 0.3417 - val_loss: 0.0423 - val_logloss: 0.0414\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04137 to 0.02231, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0287 - logloss: 0.0275 - val_loss: 0.0236 - val_logloss: 0.0223\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02231 to 0.01932, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0219 - logloss: 0.0205 - val_loss: 0.0206 - val_logloss: 0.0193\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01932 to 0.01796, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0194 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01796 to 0.01742, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0189 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01742 to 0.01731, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01731 to 0.01718, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0183 - logloss: 0.0168 - val_loss: 0.0187 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01718 to 0.01685, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0178 - logloss: 0.0163 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01685 to 0.01676, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01676 to 0.01668, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01668 to 0.01662, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0165 - logloss: 0.0150 - val_loss: 0.0182 - val_logloss: 0.0166\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01662\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01662 to 0.01644, saving model to repeat:35_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0180 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0140 - logloss: 0.0126 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01644\n",
      "155/155 - 2s - loss: 0.0140 - logloss: 0.0126 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0140 - logloss: 0.0125 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0140 - logloss: 0.0126 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 00019: early stopping\n",
      "fold 8\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03917, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.3420 - logloss: 0.3403 - val_loss: 0.0401 - val_logloss: 0.0392\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03917 to 0.02188, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0231 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02188 to 0.01905, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0204 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01905 to 0.01797, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0193 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01797 to 0.01755, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01755 to 0.01718, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01718 to 0.01702, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01702 to 0.01684, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01684\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01684 to 0.01676, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01676 to 0.01649, saving model to repeat:35_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0179 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01649\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01649\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01649\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01649\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01649\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0136 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01649\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 00017: early stopping\n",
      "fold 9\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04472, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.3443 - logloss: 0.3426 - val_loss: 0.0456 - val_logloss: 0.0447\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04472 to 0.02181, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0288 - logloss: 0.0276 - val_loss: 0.0230 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02181 to 0.01946, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0219 - logloss: 0.0205 - val_loss: 0.0207 - val_logloss: 0.0195\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01946 to 0.01822, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0195 - val_logloss: 0.0182\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01822 to 0.01773, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0190 - val_logloss: 0.0177\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01773 to 0.01755, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0189 - val_logloss: 0.0175\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01755 to 0.01732, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0186 - val_logloss: 0.0173\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01732\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0188 - val_logloss: 0.0175\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01732\n",
      "155/155 - 2s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0187 - val_logloss: 0.0174\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01732 to 0.01692, saving model to repeat:35_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0163 - logloss: 0.0149 - val_loss: 0.0182 - val_logloss: 0.0169\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0183 - val_logloss: 0.0170\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0142 - val_loss: 0.0183 - val_logloss: 0.0170\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01692\n",
      "155/155 - 2s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0183 - val_logloss: 0.0170\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0183 - val_logloss: 0.0170\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01692\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0142 - val_loss: 0.0183 - val_logloss: 0.0170\n",
      "Epoch 00016: early stopping\n",
      "fold 10\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04280, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.3432 - logloss: 0.3416 - val_loss: 0.0436 - val_logloss: 0.0428\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04280 to 0.02205, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0233 - val_logloss: 0.0221\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02205 to 0.01889, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0203 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01889 to 0.01806, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01806 to 0.01757, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01757 to 0.01730, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01730 to 0.01703, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01703\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01703\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01703 to 0.01684, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01684 to 0.01679, saving model to repeat:35_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0159 - logloss: 0.0145 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01679\n",
      "155/155 - 1s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01679\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01679\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01679\n",
      "155/155 - 2s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01679\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01679\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 00017: early stopping\n",
      "REAPEAT NUMBER 3 SEED 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04218, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.3442 - logloss: 0.3425 - val_loss: 0.0432 - val_logloss: 0.0422\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04218 to 0.02176, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0274 - val_loss: 0.0231 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02176 to 0.01907, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0205 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01907 to 0.01803, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0196 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01803 to 0.01763, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0192 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01763 to 0.01751, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0190 - val_logloss: 0.0175\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01751 to 0.01718, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01718 to 0.01713, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01713 to 0.01696, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0171 - logloss: 0.0157 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01696 to 0.01691, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0152 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01691\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01691 to 0.01664, saving model to repeat:36_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01664\n",
      "155/155 - 2s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0127 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0127 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 00018: early stopping\n",
      "fold 2\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04055, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.3425 - logloss: 0.3409 - val_loss: 0.0415 - val_logloss: 0.0406\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04055 to 0.02200, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0233 - val_logloss: 0.0220\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02200 to 0.01905, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0205 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01905 to 0.01809, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01809 to 0.01771, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0192 - val_logloss: 0.0177\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01771 to 0.01739, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01739 to 0.01735, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01735 to 0.01713, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01713 to 0.01702, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01702\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01702 to 0.01698, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01698 to 0.01675, saving model to repeat:36_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01675\n",
      "155/155 - 2s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01675\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01675\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01675\n",
      "155/155 - 2s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01675\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01675\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 00018: early stopping\n",
      "fold 3\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04040, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.3427 - logloss: 0.3410 - val_loss: 0.0414 - val_logloss: 0.0404\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04040 to 0.02165, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0230 - val_logloss: 0.0216\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02165 to 0.01894, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0202 - val_loss: 0.0204 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01894 to 0.01769, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0193 - val_logloss: 0.0177\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01769 to 0.01725, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0189 - val_logloss: 0.0172\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss did not improve from 0.01725\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0189 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01725 to 0.01683, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01683 to 0.01668, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01668 to 0.01653, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01653 to 0.01648, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0154 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01648\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01648 to 0.01614, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0138 - val_loss: 0.0178 - val_logloss: 0.0161\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01614 to 0.01612, saving model to repeat:36_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0179 - val_logloss: 0.0161\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01612\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0132 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01612\n",
      "155/155 - 2s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01612\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01612\n",
      "155/155 - 2s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01612\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 19/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01612\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 00019: early stopping\n",
      "fold 4\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04134, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.3430 - logloss: 0.3414 - val_loss: 0.0423 - val_logloss: 0.0413\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04134 to 0.02126, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0226 - val_logloss: 0.0213\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02126 to 0.01869, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0202 - val_logloss: 0.0187\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01869 to 0.01784, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0193 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01784 to 0.01718, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0188 - val_logloss: 0.0172\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss did not improve from 0.01718\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01718 to 0.01707, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01707 to 0.01681, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01681 to 0.01665, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01665 to 0.01664, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0152 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01664 to 0.01632, saving model to repeat:36_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0179 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01632\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0180 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01632\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0131 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01632\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01632\n",
      "155/155 - 2s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01632\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01632\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 00018: early stopping\n",
      "fold 5\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04096, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.3424 - logloss: 0.3407 - val_loss: 0.0419 - val_logloss: 0.0410\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04096 to 0.02186, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0283 - logloss: 0.0272 - val_loss: 0.0232 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02186 to 0.01900, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0205 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01900 to 0.01784, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01784 to 0.01743, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01743 to 0.01729, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0189 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01729 to 0.01689, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01689 to 0.01675, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01675\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01675\n",
      "155/155 - 2s - loss: 0.0169 - logloss: 0.0155 - val_loss: 0.0189 - val_logloss: 0.0171\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01675 to 0.01647, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0145 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01647 to 0.01645, saving model to repeat:36_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01645\n",
      "155/155 - 2s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 00018: early stopping\n",
      "fold 6\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04261, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.3443 - logloss: 0.3426 - val_loss: 0.0434 - val_logloss: 0.0426\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04261 to 0.02184, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0230 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02184 to 0.01904, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0204 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01904 to 0.01803, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01803 to 0.01748, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0189 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss did not improve from 0.01748\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0190 - val_logloss: 0.0176\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01748 to 0.01707, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0185 - val_logloss: 0.0171\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01707 to 0.01691, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01691\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01691\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01691 to 0.01654, saving model to repeat:36_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0143 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0136 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0180 - val_logloss: 0.0166\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 00017: early stopping\n",
      "fold 7\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04232, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.3432 - logloss: 0.3415 - val_loss: 0.0434 - val_logloss: 0.0423\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04232 to 0.02093, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0225 - val_logloss: 0.0209\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02093 to 0.01863, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0203 - val_logloss: 0.0186\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01863 to 0.01780, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01780 to 0.01721, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0193 - logloss: 0.0178 - val_loss: 0.0189 - val_logloss: 0.0172\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss did not improve from 0.01721\n",
      "155/155 - 1s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0191 - val_logloss: 0.0175\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01721 to 0.01680, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0182 - logloss: 0.0168 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01680\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0163 - val_loss: 0.0186 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01680\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0159 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01680 to 0.01628, saving model to repeat:36_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01628\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0145 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01628\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01628\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01628\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01628\n",
      "155/155 - 2s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01628\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 00016: early stopping\n",
      "fold 8\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04127, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.3420 - logloss: 0.3403 - val_loss: 0.0423 - val_logloss: 0.0413\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04127 to 0.02202, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0234 - val_logloss: 0.0220\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02202 to 0.01909, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0206 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01909 to 0.01805, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01805 to 0.01741, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01741 to 0.01697, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss did not improve from 0.01697\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0188 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01697 to 0.01684, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01684 to 0.01661, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01661 to 0.01652, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0154 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01652\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01652 to 0.01627, saving model to repeat:36_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0153 - logloss: 0.0138 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0132 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 00018: early stopping\n",
      "fold 9\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03858, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.3438 - logloss: 0.3422 - val_loss: 0.0396 - val_logloss: 0.0386\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03858 to 0.02185, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0231 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02185 to 0.01898, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0203 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01898 to 0.01806, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01806 to 0.01751, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01751 to 0.01733, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01733 to 0.01705, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01705 to 0.01672, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0176 - logloss: 0.0162 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01672\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01672\n",
      "155/155 - 2s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01672 to 0.01643, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0180 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01643 to 0.01643, saving model to repeat:36_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0180 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01643\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01643\n",
      "155/155 - 2s - loss: 0.0150 - logloss: 0.0136 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01643\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01643\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01643\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01643\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0180 - val_logloss: 0.0165\n",
      "Epoch 00018: early stopping\n",
      "fold 10\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04197, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.3435 - logloss: 0.3418 - val_loss: 0.0429 - val_logloss: 0.0420\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04197 to 0.02156, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0228 - val_logloss: 0.0216\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02156 to 0.01886, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0203 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01886 to 0.01787, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01787 to 0.01738, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01738 to 0.01718, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01718 to 0.01708, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01708 to 0.01682, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01682\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01682\n",
      "155/155 - 2s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0186 - val_logloss: 0.0169\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01682 to 0.01646, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01646 to 0.01645, saving model to repeat:36_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01645\n",
      "155/155 - 2s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 00018: early stopping\n",
      "REAPEAT NUMBER 4 SEED 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04159, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.3438 - logloss: 0.3421 - val_loss: 0.0426 - val_logloss: 0.0416\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04159 to 0.02142, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0228 - val_logloss: 0.0214\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02142 to 0.01892, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0215 - logloss: 0.0202 - val_loss: 0.0204 - val_logloss: 0.0189\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01892 to 0.01781, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01781 to 0.01742, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0190 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss did not improve from 0.01742\n",
      "155/155 - 1s - loss: 0.0185 - logloss: 0.0171 - val_loss: 0.0190 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01742 to 0.01701, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01701 to 0.01680, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01680 to 0.01667, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01667 to 0.01662, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01662\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01662 to 0.01641, saving model to repeat:37_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 00018: early stopping\n",
      "fold 2\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03891, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.3442 - logloss: 0.3425 - val_loss: 0.0400 - val_logloss: 0.0389\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03891 to 0.02225, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0237 - val_logloss: 0.0223\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02225 to 0.01908, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0207 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01908 to 0.01783, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01783 to 0.01735, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01735 to 0.01699, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0185 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01699 to 0.01680, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01680\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01680 to 0.01662, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01662 to 0.01661, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01661 to 0.01651, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01651\n",
      "155/155 - 2s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0185 - val_logloss: 0.0166\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0187 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss improved from 0.01651 to 0.01644, saving model to repeat:37_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0127 - val_loss: 0.0184 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0137 - logloss: 0.0123 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0135 - logloss: 0.0121 - val_loss: 0.0185 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0134 - logloss: 0.0119 - val_loss: 0.0185 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01644\n",
      "155/155 - 2s - loss: 0.0134 - logloss: 0.0119 - val_loss: 0.0185 - val_logloss: 0.0165\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0134 - logloss: 0.0119 - val_loss: 0.0185 - val_logloss: 0.0165\n",
      "Epoch 20/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00020: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0134 - logloss: 0.0120 - val_loss: 0.0185 - val_logloss: 0.0165\n",
      "Epoch 00020: early stopping\n",
      "fold 3\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04109, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.3441 - logloss: 0.3424 - val_loss: 0.0420 - val_logloss: 0.0411\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04109 to 0.02148, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0228 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02148 to 0.01856, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0201 - val_logloss: 0.0186\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01856 to 0.01776, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0193 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01776 to 0.01709, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0188 - val_logloss: 0.0171\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01709 to 0.01681, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01681 to 0.01664, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01664\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01664 to 0.01614, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0179 - val_logloss: 0.0161\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01614 to 0.01610, saving model to repeat:37_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0178 - val_logloss: 0.0161\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01610\n",
      "155/155 - 1s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0179 - val_logloss: 0.0161\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01610\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0161\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01610\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0161\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01610\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01610\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01610\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0162\n",
      "Epoch 00017: early stopping\n",
      "fold 4\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03831, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.3432 - logloss: 0.3415 - val_loss: 0.0392 - val_logloss: 0.0383\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03831 to 0.02250, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0237 - val_logloss: 0.0225\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02250 to 0.01936, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0207 - val_logloss: 0.0194\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01936 to 0.01823, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0196 - val_logloss: 0.0182\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01823 to 0.01771, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0191 - val_logloss: 0.0177\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01771 to 0.01729, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0185 - logloss: 0.0170 - val_loss: 0.0187 - val_logloss: 0.0173\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01729 to 0.01717, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0187 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01717 to 0.01702, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0162 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01702 to 0.01693, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01693\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0153 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01693 to 0.01671, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0157 - logloss: 0.0143 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01671 to 0.01669, saving model to repeat:37_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01669\n",
      "155/155 - 2s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01669\n",
      "155/155 - 2s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0167\n",
      "Epoch 00018: early stopping\n",
      "fold 5\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04074, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.3442 - logloss: 0.3425 - val_loss: 0.0418 - val_logloss: 0.0407\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04074 to 0.02200, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0234 - val_logloss: 0.0220\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02200 to 0.01866, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0203 - val_logloss: 0.0187\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01866 to 0.01781, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01781 to 0.01737, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0191 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01737 to 0.01719, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0189 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01719 to 0.01691, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0169\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01691 to 0.01682, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01682 to 0.01660, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0152 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01660\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01660 to 0.01641, saving model to repeat:37_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0132 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01641\n",
      "155/155 - 2s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 00018: early stopping\n",
      "fold 6\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04266, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.3433 - logloss: 0.3416 - val_loss: 0.0437 - val_logloss: 0.0427\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04266 to 0.02150, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0275 - val_loss: 0.0229 - val_logloss: 0.0215\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02150 to 0.01881, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0204 - val_logloss: 0.0188\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01881 to 0.01779, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0194 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01779 to 0.01733, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01733 to 0.01710, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0187 - logloss: 0.0173 - val_loss: 0.0188 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01710 to 0.01679, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0168 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01679 to 0.01675, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0163 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01675 to 0.01659, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0173 - logloss: 0.0159 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01659\n",
      "155/155 - 1s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01659\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0185 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01659 to 0.01629, saving model to repeat:37_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01629\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0135 - val_loss: 0.0181 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01629\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01629\n",
      "155/155 - 1s - loss: 0.0146 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01629\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01629\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01629\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0131 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 00018: early stopping\n",
      "fold 7\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03977, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.3439 - logloss: 0.3423 - val_loss: 0.0407 - val_logloss: 0.0398\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03977 to 0.02179, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0230 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02179 to 0.01929, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0215 - logloss: 0.0202 - val_loss: 0.0206 - val_logloss: 0.0193\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01929 to 0.01807, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01807 to 0.01759, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0190 - val_logloss: 0.0176\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01759 to 0.01736, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0185 - logloss: 0.0170 - val_loss: 0.0188 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01736 to 0.01718, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0187 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01718\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01718 to 0.01693, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01693 to 0.01691, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0152 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01691 to 0.01688, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01688 to 0.01666, saving model to repeat:37_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 00018: early stopping\n",
      "fold 8\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04059, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.3428 - logloss: 0.3412 - val_loss: 0.0417 - val_logloss: 0.0406\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04059 to 0.02227, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0237 - val_logloss: 0.0223\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02227 to 0.01912, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0218 - logloss: 0.0205 - val_loss: 0.0207 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01912 to 0.01797, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0196 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01797 to 0.01735, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0191 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01735 to 0.01694, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0187 - val_logloss: 0.0169\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01694 to 0.01684, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01684 to 0.01676, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01676 to 0.01654, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01654\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01654 to 0.01642, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0183 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0143 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0138 - val_loss: 0.0187 - val_logloss: 0.0167\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss improved from 0.01642 to 0.01640, saving model to repeat:37_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0141 - logloss: 0.0126 - val_loss: 0.0183 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0136 - logloss: 0.0122 - val_loss: 0.0183 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0133 - logloss: 0.0119 - val_loss: 0.0184 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0133 - logloss: 0.0118 - val_loss: 0.0184 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0133 - logloss: 0.0118 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0133 - logloss: 0.0118 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 20/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00020: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0133 - logloss: 0.0118 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 00020: early stopping\n",
      "fold 9\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04009, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.3431 - logloss: 0.3414 - val_loss: 0.0410 - val_logloss: 0.0401\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04009 to 0.02188, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0231 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02188 to 0.01878, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0202 - val_logloss: 0.0188\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01878 to 0.01789, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0200 - logloss: 0.0185 - val_loss: 0.0193 - val_logloss: 0.0179\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01789 to 0.01746, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0189 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01746 to 0.01716, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0187 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01716 to 0.01699, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01699 to 0.01691, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01691\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01691 to 0.01644, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01644 to 0.01642, saving model to repeat:37_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0159 - logloss: 0.0145 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0143 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0141 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0179 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0179 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01642\n",
      "155/155 - 1s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0179 - val_logloss: 0.0165\n",
      "Epoch 00017: early stopping\n",
      "fold 10\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03977, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.3435 - logloss: 0.3419 - val_loss: 0.0409 - val_logloss: 0.0398\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03977 to 0.02159, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0231 - val_logloss: 0.0216\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02159 to 0.01911, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0207 - val_logloss: 0.0191\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01911 to 0.01781, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0195 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01781 to 0.01728, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01728 to 0.01701, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01701 to 0.01696, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01696 to 0.01668, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0185 - val_logloss: 0.0167\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01668 to 0.01668, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0185 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01668\n",
      "155/155 - 2s - loss: 0.0166 - logloss: 0.0152 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01668 to 0.01635, saving model to repeat:37_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0141 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01635\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0137 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01635\n",
      "155/155 - 1s - loss: 0.0150 - logloss: 0.0136 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01635\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0133 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01635\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01635\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01635\n",
      "155/155 - 2s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 00017: early stopping\n",
      "REAPEAT NUMBER 5 SEED 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04161, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.3424 - logloss: 0.3407 - val_loss: 0.0426 - val_logloss: 0.0416\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04161 to 0.02237, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0287 - logloss: 0.0276 - val_loss: 0.0238 - val_logloss: 0.0224\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02237 to 0.01927, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0218 - logloss: 0.0205 - val_loss: 0.0209 - val_logloss: 0.0193\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01927 to 0.01799, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0198 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01799 to 0.01733, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0192 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01733 to 0.01719, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0190 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01719 to 0.01701, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0167 - val_loss: 0.0188 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01701 to 0.01689, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0187 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01689 to 0.01666, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0185 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01666\n",
      "155/155 - 2s - loss: 0.0168 - logloss: 0.0154 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01666\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0186 - val_logloss: 0.0167\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01666 to 0.01646, saving model to repeat:38_Fold:0.hdf5\n",
      "155/155 - 1s - loss: 0.0152 - logloss: 0.0138 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01646\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0183 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01646\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0132 - val_loss: 0.0184 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01646\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01646\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01646\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01646\n",
      "155/155 - 2s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0184 - val_logloss: 0.0166\n",
      "Epoch 00018: early stopping\n",
      "fold 2\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04011, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.3425 - logloss: 0.3408 - val_loss: 0.0411 - val_logloss: 0.0401\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04011 to 0.02212, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0233 - val_logloss: 0.0221\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02212 to 0.01934, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0218 - logloss: 0.0204 - val_loss: 0.0206 - val_logloss: 0.0193\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01934 to 0.01812, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01812 to 0.01771, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0177\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01771 to 0.01740, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0188 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01740 to 0.01720, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0180 - logloss: 0.0166 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01720 to 0.01712, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01712 to 0.01698, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0171 - logloss: 0.0157 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01698\n",
      "155/155 - 1s - loss: 0.0166 - logloss: 0.0152 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01698 to 0.01696, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 2s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01696 to 0.01674, saving model to repeat:38_Fold:1.hdf5\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0136 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0182 - val_logloss: 0.0168\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0145 - logloss: 0.0130 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0128 - val_loss: 0.0183 - val_logloss: 0.0168\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01674\n",
      "155/155 - 1s - loss: 0.0142 - logloss: 0.0128 - val_loss: 0.0183 - val_logloss: 0.0169\n",
      "Epoch 00018: early stopping\n",
      "fold 3\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04302, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.3430 - logloss: 0.3413 - val_loss: 0.0439 - val_logloss: 0.0430\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04302 to 0.02174, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0231 - val_logloss: 0.0217\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02174 to 0.01884, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0203 - val_logloss: 0.0188\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01884 to 0.01800, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0195 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01800 to 0.01748, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0190 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01748 to 0.01708, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0186 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01708 to 0.01684, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01684\n",
      "155/155 - 2s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01684 to 0.01650, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01650 to 0.01644, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0154 - val_loss: 0.0180 - val_logloss: 0.0164\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01644\n",
      "155/155 - 1s - loss: 0.0163 - logloss: 0.0148 - val_loss: 0.0182 - val_logloss: 0.0166\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01644 to 0.01631, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0179 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01631 to 0.01631, saving model to repeat:38_Fold:2.hdf5\n",
      "155/155 - 2s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0179 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01631\n",
      "155/155 - 1s - loss: 0.0147 - logloss: 0.0132 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01631\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01631\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01631\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01631\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 19/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01631\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0130 - val_loss: 0.0179 - val_logloss: 0.0164\n",
      "Epoch 00019: early stopping\n",
      "fold 4\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04078, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 3s - loss: 0.3444 - logloss: 0.3427 - val_loss: 0.0418 - val_logloss: 0.0408\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04078 to 0.02182, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0232 - val_logloss: 0.0218\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02182 to 0.01864, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0202 - val_logloss: 0.0186\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01864 to 0.01778, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0184 - val_loss: 0.0194 - val_logloss: 0.0178\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01778 to 0.01728, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01728 to 0.01705, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0185 - logloss: 0.0170 - val_loss: 0.0187 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01705 to 0.01675, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0180 - logloss: 0.0166 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01675\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01675 to 0.01668, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01668 to 0.01644, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0161 - logloss: 0.0147 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01644 to 0.01641, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01641 to 0.01641, saving model to repeat:38_Fold:3.hdf5\n",
      "155/155 - 2s - loss: 0.0156 - logloss: 0.0142 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01641\n",
      "155/155 - 2s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01641\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 00018: early stopping\n",
      "fold 5\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04020, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.3420 - logloss: 0.3403 - val_loss: 0.0412 - val_logloss: 0.0402\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04020 to 0.02190, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0232 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02190 to 0.01897, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0218 - logloss: 0.0205 - val_loss: 0.0203 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01897 to 0.01796, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0194 - val_logloss: 0.0180\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01796 to 0.01739, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0189 - val_logloss: 0.0174\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01739 to 0.01704, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0186 - val_logloss: 0.0170\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01704 to 0.01697, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01697\n",
      "155/155 - 2s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01697 to 0.01658, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0153 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01658 to 0.01656, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0165 - logloss: 0.0150 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01656 to 0.01655, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0162 - logloss: 0.0148 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01655 to 0.01653, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 1s - loss: 0.0161 - logloss: 0.0146 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01653 to 0.01653, saving model to repeat:38_Fold:4.hdf5\n",
      "155/155 - 2s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01653\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01653\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0145 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01653\n",
      "155/155 - 2s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01653\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01653\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0146 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01653\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0145 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 00019: early stopping\n",
      "fold 6\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04184, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.3437 - logloss: 0.3420 - val_loss: 0.0427 - val_logloss: 0.0418\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04184 to 0.02160, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0285 - logloss: 0.0274 - val_loss: 0.0230 - val_logloss: 0.0216\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02160 to 0.01875, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0215 - logloss: 0.0202 - val_loss: 0.0203 - val_logloss: 0.0187\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01875 to 0.01774, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0199 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0177\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01774 to 0.01746, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01746 to 0.01693, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0186 - val_logloss: 0.0169\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01693 to 0.01669, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0180 - logloss: 0.0166 - val_loss: 0.0184 - val_logloss: 0.0167\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01669\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01669 to 0.01659, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0157 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01659 to 0.01628, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 2s - loss: 0.0161 - logloss: 0.0147 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01628 to 0.01627, saving model to repeat:38_Fold:5.hdf5\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0144 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01627\n",
      "155/155 - 2s - loss: 0.0156 - logloss: 0.0142 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01627\n",
      "155/155 - 2s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01627\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0180 - val_logloss: 0.0163\n",
      "Epoch 00017: early stopping\n",
      "fold 7\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03992, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.3424 - logloss: 0.3407 - val_loss: 0.0409 - val_logloss: 0.0399\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03992 to 0.02172, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0284 - logloss: 0.0273 - val_loss: 0.0230 - val_logloss: 0.0217\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02172 to 0.01901, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0205 - val_logloss: 0.0190\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01901 to 0.01807, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0196 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01807 to 0.01753, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0177 - val_loss: 0.0191 - val_logloss: 0.0175\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01753 to 0.01711, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0186 - logloss: 0.0171 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01711 to 0.01690, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0185 - val_logloss: 0.0169\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01690 to 0.01677, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01677\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01677 to 0.01667, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0169 - logloss: 0.0154 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01667 to 0.01645, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0150 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01645\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0145 - val_loss: 0.0187 - val_logloss: 0.0168\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01645\n",
      "155/155 - 2s - loss: 0.0155 - logloss: 0.0140 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss improved from 0.01645 to 0.01637, saving model to repeat:38_Fold:6.hdf5\n",
      "155/155 - 1s - loss: 0.0143 - logloss: 0.0129 - val_loss: 0.0181 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01637\n",
      "155/155 - 2s - loss: 0.0139 - logloss: 0.0124 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01637\n",
      "155/155 - 2s - loss: 0.0136 - logloss: 0.0122 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01637\n",
      "155/155 - 1s - loss: 0.0136 - logloss: 0.0122 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01637\n",
      "155/155 - 1s - loss: 0.0136 - logloss: 0.0121 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01637\n",
      "155/155 - 1s - loss: 0.0136 - logloss: 0.0121 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 20/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00020: val_logloss did not improve from 0.01637\n",
      "155/155 - 1s - loss: 0.0136 - logloss: 0.0121 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 00020: early stopping\n",
      "fold 8\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04100, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.3420 - logloss: 0.3403 - val_loss: 0.0418 - val_logloss: 0.0410\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04100 to 0.02192, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0285 - logloss: 0.0273 - val_loss: 0.0231 - val_logloss: 0.0219\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02192 to 0.01937, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0207 - val_logloss: 0.0194\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01937 to 0.01815, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0201 - logloss: 0.0187 - val_loss: 0.0195 - val_logloss: 0.0181\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01815 to 0.01767, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0192 - logloss: 0.0178 - val_loss: 0.0190 - val_logloss: 0.0177\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01767 to 0.01740, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0187 - logloss: 0.0172 - val_loss: 0.0188 - val_logloss: 0.0174\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01740 to 0.01722, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0182 - logloss: 0.0167 - val_loss: 0.0186 - val_logloss: 0.0172\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01722 to 0.01703, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0163 - val_loss: 0.0184 - val_logloss: 0.0170\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss did not improve from 0.01703\n",
      "155/155 - 1s - loss: 0.0173 - logloss: 0.0158 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01703 to 0.01683, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 1s - loss: 0.0168 - logloss: 0.0154 - val_loss: 0.0184 - val_logloss: 0.0168\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss did not improve from 0.01683\n",
      "155/155 - 1s - loss: 0.0164 - logloss: 0.0149 - val_loss: 0.0185 - val_logloss: 0.0170\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01683\n",
      "155/155 - 1s - loss: 0.0160 - logloss: 0.0145 - val_loss: 0.0184 - val_logloss: 0.0169\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss improved from 0.01683 to 0.01657, saving model to repeat:38_Fold:7.hdf5\n",
      "155/155 - 2s - loss: 0.0147 - logloss: 0.0133 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01657\n",
      "155/155 - 1s - loss: 0.0144 - logloss: 0.0129 - val_loss: 0.0181 - val_logloss: 0.0166\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01657\n",
      "155/155 - 1s - loss: 0.0141 - logloss: 0.0126 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01657\n",
      "155/155 - 2s - loss: 0.0139 - logloss: 0.0124 - val_loss: 0.0182 - val_logloss: 0.0167\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01657\n",
      "155/155 - 2s - loss: 0.0139 - logloss: 0.0124 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01657\n",
      "155/155 - 1s - loss: 0.0138 - logloss: 0.0123 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00019: val_logloss did not improve from 0.01657\n",
      "155/155 - 1s - loss: 0.0138 - logloss: 0.0124 - val_loss: 0.0183 - val_logloss: 0.0167\n",
      "Epoch 00019: early stopping\n",
      "fold 9\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.04018, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.3438 - logloss: 0.3421 - val_loss: 0.0412 - val_logloss: 0.0402\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.04018 to 0.02256, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0287 - logloss: 0.0276 - val_loss: 0.0240 - val_logloss: 0.0226\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02256 to 0.01878, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0217 - logloss: 0.0204 - val_loss: 0.0204 - val_logloss: 0.0188\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01878 to 0.01773, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0186 - val_loss: 0.0194 - val_logloss: 0.0177\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01773 to 0.01720, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0193 - logloss: 0.0178 - val_loss: 0.0188 - val_logloss: 0.0172\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01720 to 0.01705, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 2s - loss: 0.0186 - logloss: 0.0172 - val_loss: 0.0187 - val_logloss: 0.0171\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01705 to 0.01680, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0181 - logloss: 0.0166 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss improved from 0.01680 to 0.01659, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0177 - logloss: 0.0162 - val_loss: 0.0183 - val_logloss: 0.0166\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01659 to 0.01651, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0172 - logloss: 0.0158 - val_loss: 0.0181 - val_logloss: 0.0165\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00010: val_logloss did not improve from 0.01651\n",
      "155/155 - 1s - loss: 0.0167 - logloss: 0.0152 - val_loss: 0.0182 - val_logloss: 0.0165\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01651 to 0.01620, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0142 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: val_logloss improved from 0.01620 to 0.01616, saving model to repeat:38_Fold:8.hdf5\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01616\n",
      "155/155 - 1s - loss: 0.0151 - logloss: 0.0137 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01616\n",
      "155/155 - 2s - loss: 0.0149 - logloss: 0.0135 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01616\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01616\n",
      "155/155 - 2s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01616\n",
      "155/155 - 1s - loss: 0.0149 - logloss: 0.0134 - val_loss: 0.0178 - val_logloss: 0.0162\n",
      "Epoch 18/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00018: val_logloss did not improve from 0.01616\n",
      "155/155 - 1s - loss: 0.0148 - logloss: 0.0134 - val_loss: 0.0178 - val_logloss: 0.0163\n",
      "Epoch 00018: early stopping\n",
      "fold 10\n",
      "\n",
      "\n",
      "ResNet\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: val_logloss improved from inf to 0.03929, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.3426 - logloss: 0.3410 - val_loss: 0.0404 - val_logloss: 0.0393\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: val_logloss improved from 0.03929 to 0.02202, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0286 - logloss: 0.0274 - val_loss: 0.0235 - val_logloss: 0.0220\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: val_logloss improved from 0.02202 to 0.01872, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0216 - logloss: 0.0203 - val_loss: 0.0204 - val_logloss: 0.0187\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: val_logloss improved from 0.01872 to 0.01772, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0200 - logloss: 0.0185 - val_loss: 0.0194 - val_logloss: 0.0177\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: val_logloss improved from 0.01772 to 0.01726, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0191 - logloss: 0.0176 - val_loss: 0.0190 - val_logloss: 0.0173\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: val_logloss improved from 0.01726 to 0.01716, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0185 - logloss: 0.0171 - val_loss: 0.0189 - val_logloss: 0.0172\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: val_logloss improved from 0.01716 to 0.01679, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0180 - logloss: 0.0165 - val_loss: 0.0185 - val_logloss: 0.0168\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: val_logloss did not improve from 0.01679\n",
      "155/155 - 1s - loss: 0.0176 - logloss: 0.0161 - val_loss: 0.0186 - val_logloss: 0.0168\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00009: val_logloss improved from 0.01679 to 0.01674, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 2s - loss: 0.0171 - logloss: 0.0156 - val_loss: 0.0186 - val_logloss: 0.0167\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: val_logloss improved from 0.01674 to 0.01641, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0161 - logloss: 0.0147 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: val_logloss improved from 0.01641 to 0.01640, saving model to repeat:38_Fold:9.hdf5\n",
      "155/155 - 1s - loss: 0.0158 - logloss: 0.0143 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00012: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0156 - logloss: 0.0142 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00014: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0140 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0153 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00016: val_logloss did not improve from 0.01640\n",
      "155/155 - 1s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 17/100\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00017: val_logloss did not improve from 0.01640\n",
      "155/155 - 2s - loss: 0.0154 - logloss: 0.0139 - val_loss: 0.0182 - val_logloss: 0.0164\n",
      "Epoch 00017: early stopping\n"
     ]
    }
   ],
   "source": [
    "models, oof_preds = make_folds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.857094,
     "end_time": "2020-11-07T22:37:29.732890",
     "exception": false,
     "start_time": "2020-11-07T22:37:28.875796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "b7d35a42-c682-49e2-a0c6-b33a528442df",
    "_uuid": "9a8b91ee-8644-4072-b810-1c4d51913302",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:37:31.398399Z",
     "iopub.status.busy": "2020-11-07T22:37:31.396935Z",
     "iopub.status.idle": "2020-11-07T22:37:38.756703Z",
     "shell.execute_reply": "2020-11-07T22:37:38.757560Z"
    },
    "papermill": {
     "duration": 8.246627,
     "end_time": "2020-11-07T22:37:38.757754",
     "exception": false,
     "start_time": "2020-11-07T22:37:30.511127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAKY RELU SCORE\n",
      "Repeat 1 OOF Log Loss: 0.016543303267814467\n",
      "LEAKY RELU SCORE\n",
      "Repeat 2 OOF Log Loss: 0.016576982988045965\n",
      "LEAKY RELU SCORE\n",
      "Repeat 3 OOF Log Loss: 0.016511309522550553\n",
      "LEAKY RELU SCORE\n",
      "Repeat 4 OOF Log Loss: 0.01655908453466226\n",
      "LEAKY RELU SCORE\n",
      "Repeat 5 OOF Log Loss: 0.016534854065383058\n",
      "Mean OOF Log Loss: 0.0160795758831711\n"
     ]
    }
   ],
   "source": [
    "mean_oof_preds = train_targets.copy()\n",
    "mean_oof_preds.loc[:, target_cols] = 0\n",
    "scores = []\n",
    "for i, p in enumerate(oof_preds):\n",
    "    loss = multi_log_loss(train_targets, p)\n",
    "    print(\"LEAKY RELU SCORE\")\n",
    "    print(f\"Repeat {i + 1} OOF Log Loss: {loss}\")\n",
    "    scores.append(loss)\n",
    "    mean_oof_preds.loc[:,target_cols] += p\n",
    "mean_oof_preds.loc[:, target_cols] /= len(oof_preds)\n",
    "print(f\"Mean OOF Log Loss: {multi_log_loss(train_targets, mean_oof_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.768949,
     "end_time": "2020-11-07T22:37:40.284551",
     "exception": false,
     "start_time": "2020-11-07T22:37:39.515602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.743031,
     "end_time": "2020-11-07T22:37:41.797347",
     "exception": false,
     "start_time": "2020-11-07T22:37:41.054316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:37:43.341091Z",
     "iopub.status.busy": "2020-11-07T22:37:43.340165Z",
     "iopub.status.idle": "2020-11-07T22:37:43.975992Z",
     "shell.execute_reply": "2020-11-07T22:37:43.975356Z"
    },
    "papermill": {
     "duration": 1.428935,
     "end_time": "2020-11-07T22:37:43.976121",
     "exception": false,
     "start_time": "2020-11-07T22:37:42.547186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ = pd.read_csv('../input/lish-moa/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:37:45.471663Z",
     "iopub.status.busy": "2020-11-07T22:37:45.470323Z",
     "iopub.status.idle": "2020-11-07T22:37:45.474213Z",
     "shell.execute_reply": "2020-11-07T22:37:45.473677Z"
    },
    "papermill": {
     "duration": 0.763072,
     "end_time": "2020-11-07T22:37:45.474329",
     "exception": false,
     "start_time": "2020-11-07T22:37:44.711257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sig_id = test_[test_[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-07T22:37:47.008860Z",
     "iopub.status.busy": "2020-11-07T22:37:47.007838Z",
     "iopub.status.idle": "2020-11-07T22:38:05.245478Z",
     "shell.execute_reply": "2020-11-07T22:38:05.243822Z"
    },
    "papermill": {
     "duration": 19.032113,
     "end_time": "2020-11-07T22:38:05.245625",
     "exception": false,
     "start_time": "2020-11-07T22:37:46.213512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds = sub.copy()\n",
    "test_preds[target_cols] = 0\n",
    "test_preds = test_preds.drop(\"sig_id\", axis = 1)\n",
    "test_preds[\"sig_id\"] = sig_id\n",
    "test_preds.dropna(inplace=True)\n",
    "test_preds.drop(\"sig_id\", axis = 1, inplace=True)\n",
    "for model in models:\n",
    "    preds = model.predict([test_features.values, test_features2[predictors].values])\n",
    "    test_preds.loc[:,target_cols] += clip(preds, p_min, p_max)\n",
    "test_preds.loc[:,target_cols] /= len(models)\n",
    "test_preds[\"sig_id\"] = sig_id\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "submission = pd.merge(test_features[[\"sig_id\"]], test_preds, on = \"sig_id\", how = \"left\")\n",
    "submission.fillna(0, inplace = True)\n",
    "sub = submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "18a1cc50-0be4-4f77-8342-162b5ca2ec0f",
    "_uuid": "b91ecf02-c13e-4005-a8db-9f2b0cd86e5e",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:38:06.756562Z",
     "iopub.status.busy": "2020-11-07T22:38:06.755544Z",
     "iopub.status.idle": "2020-11-07T22:38:06.798306Z",
     "shell.execute_reply": "2020-11-07T22:38:06.798896Z"
    },
    "papermill": {
     "duration": 0.780762,
     "end_time": "2020-11-07T22:38:06.799044",
     "exception": false,
     "start_time": "2020-11-07T22:38:06.018282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5458</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>-0.5135</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>1.5500</td>\n",
       "      <td>-0.1644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.2067</td>\n",
       "      <td>-0.2303</td>\n",
       "      <td>-0.1193</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-0.7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.1829</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>1.2080</td>\n",
       "      <td>-0.4522</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>-0.3319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>-0.1852</td>\n",
       "      <td>-1.0310</td>\n",
       "      <td>-1.3670</td>\n",
       "      <td>-0.3690</td>\n",
       "      <td>-0.5382</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>-0.4764</td>\n",
       "      <td>-1.3810</td>\n",
       "      <td>-0.7300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>-0.1404</td>\n",
       "      <td>-0.3911</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>-1.4380</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2261</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>-1.3840</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>-1.9530</td>\n",
       "      <td>-1.0140</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>0.4924</td>\n",
       "      <td>-0.1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1955</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>0.4244</td>\n",
       "      <td>-0.5855</td>\n",
       "      <td>-1.2020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>-0.1784</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.4325</td>\n",
       "      <td>-0.9005</td>\n",
       "      <td>0.8131</td>\n",
       "      <td>-0.1305</td>\n",
       "      <td>0.5645</td>\n",
       "      <td>-0.5809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.3979</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>1.9130</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>-0.5864</td>\n",
       "      <td>-0.0166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4965</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>-0.1580</td>\n",
       "      <td>1.0510</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>1.0900</td>\n",
       "      <td>-0.2962</td>\n",
       "      <td>-0.5313</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>1.8380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>id_ff7004b87</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>-0.5743</td>\n",
       "      <td>3.3930</td>\n",
       "      <td>-0.6202</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>1.6240</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1790</td>\n",
       "      <td>-0.6422</td>\n",
       "      <td>-0.4367</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>-0.6539</td>\n",
       "      <td>-0.4791</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>-1.1280</td>\n",
       "      <td>-0.4167</td>\n",
       "      <td>-0.6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>id_ff925dd0d</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5885</td>\n",
       "      <td>-0.2548</td>\n",
       "      <td>2.5850</td>\n",
       "      <td>0.3456</td>\n",
       "      <td>0.4401</td>\n",
       "      <td>0.3107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.5780</td>\n",
       "      <td>-0.5888</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.9312</td>\n",
       "      <td>1.2730</td>\n",
       "      <td>0.2614</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>id_ffb710450</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.3985</td>\n",
       "      <td>-0.1554</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>-0.6813</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.4791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4418</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>-0.1862</td>\n",
       "      <td>0.4049</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.4666</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>-0.4205</td>\n",
       "      <td>-0.1504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>id_ffbb869f2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.0960</td>\n",
       "      <td>-1.7750</td>\n",
       "      <td>-0.3977</td>\n",
       "      <td>1.0160</td>\n",
       "      <td>-1.3350</td>\n",
       "      <td>-0.2207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3079</td>\n",
       "      <td>-0.4473</td>\n",
       "      <td>-0.8192</td>\n",
       "      <td>0.7785</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>-0.2618</td>\n",
       "      <td>0.5074</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>-0.0484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>id_ffd5800b6</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5174</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>0.3286</td>\n",
       "      <td>-0.0428</td>\n",
       "      <td>-0.0800</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.5939</td>\n",
       "      <td>-0.0507</td>\n",
       "      <td>0.2811</td>\n",
       "      <td>-0.4041</td>\n",
       "      <td>-0.4948</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>-0.1356</td>\n",
       "      <td>0.5280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sig_id      cp_type  cp_time cp_dose     g-0     g-1     g-2  \\\n",
       "0     id_0004d9e33       trt_cp       24      D1 -0.5458  0.1306 -0.5135   \n",
       "1     id_001897cda       trt_cp       72      D1 -0.1829  0.2320  1.2080   \n",
       "2     id_002429b5b  ctl_vehicle       24      D1  0.1852 -0.1404 -0.3911   \n",
       "3     id_00276f245       trt_cp       24      D2  0.4828  0.1955  0.3825   \n",
       "4     id_0027f1083       trt_cp       48      D1 -0.3979 -1.2680  1.9130   \n",
       "...            ...          ...      ...     ...     ...     ...     ...   \n",
       "3977  id_ff7004b87       trt_cp       24      D1  0.4571 -0.5743  3.3930   \n",
       "3978  id_ff925dd0d       trt_cp       24      D1 -0.5885 -0.2548  2.5850   \n",
       "3979  id_ffb710450       trt_cp       72      D1 -0.3985 -0.1554  0.2677   \n",
       "3980  id_ffbb869f2       trt_cp       48      D2 -1.0960 -1.7750 -0.3977   \n",
       "3981  id_ffd5800b6       trt_cp       72      D1 -0.5174  0.2953  0.3286   \n",
       "\n",
       "         g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     0.4408  1.5500 -0.1644  ...  0.0981  0.7978 -0.1430 -0.2067 -0.2303   \n",
       "1    -0.4522 -0.3652 -0.3319  ... -0.1190 -0.1852 -1.0310 -1.3670 -0.3690   \n",
       "2     0.1310 -1.4380  0.2455  ... -0.2261  0.3370 -1.3840  0.8604 -1.9530   \n",
       "3     0.4244 -0.5855 -1.2020  ...  0.1260  0.1570 -0.1784 -1.1200 -0.4325   \n",
       "4     0.2057 -0.5864 -0.0166  ...  0.4965  0.7578 -0.1580  1.0510  0.5742   \n",
       "...      ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "3977 -0.6202  0.8557  1.6240  ... -1.1790 -0.6422 -0.4367  0.0159 -0.6539   \n",
       "3978  0.3456  0.4401  0.3107  ...  0.0210  0.5780 -0.5888  0.8057  0.9312   \n",
       "3979 -0.6813  0.0152  0.4791  ...  0.4418  0.9153 -0.1862  0.4049  0.9568   \n",
       "3980  1.0160 -1.3350 -0.2207  ...  0.3079 -0.4473 -0.8192  0.7785  0.3133   \n",
       "3981 -0.0428 -0.0800  0.8702  ...  0.0363  0.1708  0.5939 -0.0507  0.2811   \n",
       "\n",
       "        c-95    c-96    c-97    c-98    c-99  \n",
       "0    -0.1193  0.0210 -0.0502  0.1510 -0.7750  \n",
       "1    -0.5382  0.0359 -0.4764 -1.3810 -0.7300  \n",
       "2    -1.0140  0.8662  1.0160  0.4924 -0.1942  \n",
       "3    -0.9005  0.8131 -0.1305  0.5645 -0.5809  \n",
       "4     1.0900 -0.2962 -0.5313  0.9931  1.8380  \n",
       "...      ...     ...     ...     ...     ...  \n",
       "3977 -0.4791 -1.2680 -1.1280 -0.4167 -0.6600  \n",
       "3978  1.2730  0.2614 -0.2790 -0.0131 -0.0934  \n",
       "3979  0.4666  0.0461  0.5888 -0.4205 -0.1504  \n",
       "3980  0.1286 -0.2618  0.5074  0.7430 -0.0484  \n",
       "3981 -0.4041 -0.4948  0.0757 -0.1356  0.5280  \n",
       "\n",
       "[3982 rows x 876 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "c62c652f-4c90-4a01-a1d3-931c8b6edf9d",
    "_uuid": "93d6474d-7ee0-482b-9094-c47068c4e216",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:38:08.319279Z",
     "iopub.status.busy": "2020-11-07T22:38:08.318328Z",
     "iopub.status.idle": "2020-11-07T22:38:08.424138Z",
     "shell.execute_reply": "2020-11-07T22:38:08.424683Z"
    },
    "papermill": {
     "duration": 0.857121,
     "end_time": "2020-11-07T22:38:08.424838",
     "exception": false,
     "start_time": "2020-11-07T22:38:07.567717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Diff(list1, list2): \n",
    "    return (list(list(set(list1)-set(list2)) + list(set(list2)-set(list1)))) \n",
    "\n",
    "Diff (test_preds.columns, pd.read_csv('../input/lish-moa/sample_submission.csv').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "de9c971a-7604-4a7e-bec7-90e39a82fdc6",
    "_uuid": "e603f1e3-aa40-49dd-9863-19511b31a206",
    "execution": {
     "iopub.execute_input": "2020-11-07T22:38:10.217353Z",
     "iopub.status.busy": "2020-11-07T22:38:10.215578Z",
     "iopub.status.idle": "2020-11-07T22:38:10.219988Z",
     "shell.execute_reply": "2020-11-07T22:38:10.219447Z"
    },
    "papermill": {
     "duration": 0.820186,
     "end_time": "2020-11-07T22:38:10.220097",
     "exception": false,
     "start_time": "2020-11-07T22:38:09.399911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Diff(train_targets, train_non))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "683dfabd-24ea-4fcf-9569-9f0593e24aa7",
    "_uuid": "0bef53c9-1ea1-46c1-8a67-f29fa4179cf4",
    "papermill": {
     "duration": 0.737836,
     "end_time": "2020-11-07T22:38:11.717589",
     "exception": false,
     "start_time": "2020-11-07T22:38:10.979753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1619.764464,
   "end_time": "2020-11-07T22:38:13.989586",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-07T22:11:14.225122",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
