{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066306,
     "end_time": "2020-11-21T04:36:13.804353",
     "exception": false,
     "start_time": "2020-11-21T04:36:13.738047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color: white; background: #ADD8E6; text-align:center; font-size: 2.3em;\"> MoA + Feature Engeneering + Keras </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.068909,
     "end_time": "2020-11-21T04:36:13.945491",
     "exception": false,
     "start_time": "2020-11-21T04:36:13.876582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Version Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062173,
     "end_time": "2020-11-21T04:36:14.071784",
     "exception": false,
     "start_time": "2020-11-21T04:36:14.009611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Version 4: Fine Tuned Params on TabNet. Changes: n_independent, n_shared, and momentum\n",
    "### Version 5: Outputed CSV file for OOF preds, used to optimize blending weights\n",
    "### Version 6: Added weighted blending, decreased the learning rate and changed the optimizer to AdaBelief\n",
    "### Version 14: Added Information Gain for Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066246,
     "end_time": "2020-11-21T04:36:14.202061",
     "exception": false,
     "start_time": "2020-11-21T04:36:14.135815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2f757229-7f7b-4ea6-b69a-270249d85727",
    "_uuid": "c014d6b0-3b76-46e4-a18f-de3d38f0ad9c",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:36:14.342383Z",
     "iopub.status.busy": "2020-11-21T04:36:14.341340Z",
     "iopub.status.idle": "2020-11-21T04:36:14.357667Z",
     "shell.execute_reply": "2020-11-21T04:36:14.356751Z"
    },
    "papermill": {
     "duration": 0.089326,
     "end_time": "2020-11-21T04:36:14.357813",
     "exception": false,
     "start_time": "2020-11-21T04:36:14.268487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lish-moa/train_targets_scored.csv\n",
      "/kaggle/input/lish-moa/train_drug.csv\n",
      "/kaggle/input/lish-moa/train_targets_nonscored.csv\n",
      "/kaggle/input/lish-moa/train_features.csv\n",
      "/kaggle/input/lish-moa/sample_submission.csv\n",
      "/kaggle/input/lish-moa/test_features.csv\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.travis.yml\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/setup.cfg\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/.gitignore\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/LICENSE\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/README.md\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/iterstrat/ml_stratifiers.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/__init__.py\n",
      "/kaggle/input/iterative-stratification/iterative-stratification-master/tests/test_ml_stratifiers.py\n",
      "/kaggle/input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
      "/kaggle/input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1/pytorch_tabnet-2.0.1.tar\n",
      "/kaggle/input/rank-gauss/rankGaussTrafo.py\n",
      "/kaggle/input/rank-gauss/gauss_rank_scaler.py\n",
      "/kaggle/input/rank-gauss/rgn.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.065623,
     "end_time": "2020-11-21T04:36:14.489705",
     "exception": false,
     "start_time": "2020-11-21T04:36:14.424082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:36:14.632864Z",
     "iopub.status.busy": "2020-11-21T04:36:14.631891Z",
     "iopub.status.idle": "2020-11-21T04:37:30.360655Z",
     "shell.execute_reply": "2020-11-21T04:37:30.359927Z"
    },
    "papermill": {
     "duration": 75.804787,
     "end_time": "2020-11-21T04:37:30.360789",
     "exception": false,
     "start_time": "2020-11-21T04:36:14.556002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: typing 3.7.4.3\r\n",
      "Uninstalling typing-3.7.4.3:\r\n",
      "  Successfully uninstalled typing-3.7.4.3\r\n",
      "Processing /kaggle/input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.6.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.18.5)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (4.45.0)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (1.4.1)\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==2.0.1) (0.23.2)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==2.0.1) (0.18.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==2.0.1) (2.1.0)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-2.0.1\r\n",
      "Processing /kaggle/input/iterative-stratification/iterative-stratification-master\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.18.5)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (1.4.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from iterative-stratification==0.1.6) (0.23.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->iterative-stratification==0.1.6) (2.1.0)\r\n",
      "Building wheels for collected packages: iterative-stratification\r\n",
      "  Building wheel for iterative-stratification (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for iterative-stratification: filename=iterative_stratification-0.1.6-py3-none-any.whl size=8401 sha256=44b5a6a8a87f67724de067565038068842946b36c995f08d592e9e9499c2afd5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/47/3f/eb4af42d124f37d23d6f13a4c8bbc32c1d70140e6e1cecb4aa\r\n",
      "Successfully built iterative-stratification\r\n",
      "Installing collected packages: iterative-stratification\r\n",
      "Successfully installed iterative-stratification-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y typing\n",
    "!pip install ../input/pytorchtabnetpretraining/pytorch_tabnet-2.0.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/iterative-stratification/iterative-stratification-master/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.072312,
     "end_time": "2020-11-21T04:37:30.505458",
     "exception": false,
     "start_time": "2020-11-21T04:37:30.433146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "99fa0530-d4cf-4803-9596-060a6980270b",
    "_uuid": "d4ba5f02-0fe6-4210-93a0-818ff7614bd6",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:30.666624Z",
     "iopub.status.busy": "2020-11-21T04:37:30.663649Z",
     "iopub.status.idle": "2020-11-21T04:37:37.912234Z",
     "shell.execute_reply": "2020-11-21T04:37:37.910890Z"
    },
    "papermill": {
     "duration": 7.334968,
     "end_time": "2020-11-21T04:37:37.912394",
     "exception": false,
     "start_time": "2020-11-21T04:37:30.577426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Keras and Tf ##\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.callbacks as C\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.optimizers as O\n",
    "import tensorflow.keras.utils as U\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "#Utils\n",
    "\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "\n",
    "## Preprocessing and sklearn ##\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD, IncrementalPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "sys.path.append('../input/rank-gauss')\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "\n",
    "## Pytorch ##\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "## TabNet\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071762,
     "end_time": "2020-11-21T04:37:38.054945",
     "exception": false,
     "start_time": "2020-11-21T04:37:37.983183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "4523fe40-9241-427d-9054-391f9107ee80",
    "_uuid": "e5ebde3e-8733-4f43-9bd5-87942b293715",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:38.208732Z",
     "iopub.status.busy": "2020-11-21T04:37:38.207660Z",
     "iopub.status.idle": "2020-11-21T04:37:44.338836Z",
     "shell.execute_reply": "2020-11-21T04:37:44.337691Z"
    },
    "papermill": {
     "duration": 6.211623,
     "end_time": "2020-11-21T04:37:44.339012",
     "exception": false,
     "start_time": "2020-11-21T04:37:38.127389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sub = pd.read_csv('../input/lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "becd92fe-dbbe-4af0-bd33-3d5706f367a4",
    "_uuid": "2c5892c4-f2d9-430d-b986-4d9519e5e9f8",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:44.495758Z",
     "iopub.status.busy": "2020-11-21T04:37:44.494763Z",
     "iopub.status.idle": "2020-11-21T04:37:44.550724Z",
     "shell.execute_reply": "2020-11-21T04:37:44.551383Z"
    },
    "papermill": {
     "duration": 0.140029,
     "end_time": "2020-11-21T04:37:44.551547",
     "exception": false,
     "start_time": "2020-11-21T04:37:44.411518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>ctl_vehicle</td>\n",
       "      <td>48</td>\n",
       "      <td>D2</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows Ã— 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id      cp_type  cp_time cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2       trt_cp       24      D1  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc       trt_cp       72      D1  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a       trt_cp       48      D1  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391       trt_cp       48      D1 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3       trt_cp       72      D2 -0.3254 -0.4009  0.9700   \n",
       "...             ...          ...      ...     ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed       trt_cp       24      D2  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c       trt_cp       24      D2 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4  ctl_vehicle       48      D2  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c       trt_cp       24      D1  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b       trt_cp       72      D1 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "73558070-5142-4ddd-89cd-b86e4cd4a75a",
    "_uuid": "42ad87cf-23cc-469e-a95f-6aad1c77e47d",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:44.723321Z",
     "iopub.status.busy": "2020-11-21T04:37:44.721465Z",
     "iopub.status.idle": "2020-11-21T04:37:44.724162Z",
     "shell.execute_reply": "2020-11-21T04:37:44.724772Z"
    },
    "papermill": {
     "duration": 0.091316,
     "end_time": "2020-11-21T04:37:44.724928",
     "exception": false,
     "start_time": "2020-11-21T04:37:44.633612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = sub.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.083865,
     "end_time": "2020-11-21T04:37:44.891520",
     "exception": false,
     "start_time": "2020-11-21T04:37:44.807655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Set Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "9e4edee3-e66d-4d7f-8867-14c9711a1266",
    "_uuid": "7a82bf16-66c0-4ca9-9a3d-1ba945de3a15",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:45.051535Z",
     "iopub.status.busy": "2020-11-21T04:37:45.050586Z",
     "iopub.status.idle": "2020-11-21T04:37:45.055180Z",
     "shell.execute_reply": "2020-11-21T04:37:45.054617Z"
    },
    "papermill": {
     "duration": 0.083659,
     "end_time": "2020-11-21T04:37:45.055291",
     "exception": false,
     "start_time": "2020-11-21T04:37:44.971632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed67160f-0448-4809-b3ee-540f0e1d16b6",
    "_uuid": "e7da134b-a5a3-4529-bcac-8fcf8294772c",
    "papermill": {
     "duration": 0.075739,
     "end_time": "2020-11-21T04:37:45.204264",
     "exception": false,
     "start_time": "2020-11-21T04:37:45.128525",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Feature Engineering/Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.071681,
     "end_time": "2020-11-21T04:37:45.349412",
     "exception": false,
     "start_time": "2020-11-21T04:37:45.277731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Remove Control Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:45.509278Z",
     "iopub.status.busy": "2020-11-21T04:37:45.508526Z",
     "iopub.status.idle": "2020-11-21T04:37:45.592776Z",
     "shell.execute_reply": "2020-11-21T04:37:45.592168Z"
    },
    "papermill": {
     "duration": 0.169527,
     "end_time": "2020-11-21T04:37:45.592908",
     "exception": false,
     "start_time": "2020-11-21T04:37:45.423381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove control group\n",
    "train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "train_targets = train_targets.iloc[train_features.index]\n",
    "train_features.reset_index(drop = True, inplace = True)\n",
    "test_features.reset_index(drop = True, inplace = True)\n",
    "train_targets.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.074096,
     "end_time": "2020-11-21T04:37:45.740801",
     "exception": false,
     "start_time": "2020-11-21T04:37:45.666705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "877fb689-3259-43ce-87e1-f65ff61aee09",
    "_uuid": "7be02bf6-72ed-4853-987f-edb015922493",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:45.945421Z",
     "iopub.status.busy": "2020-11-21T04:37:45.944380Z",
     "iopub.status.idle": "2020-11-21T04:37:45.954260Z",
     "shell.execute_reply": "2020-11-21T04:37:45.954991Z"
    },
    "papermill": {
     "duration": 0.140375,
     "end_time": "2020-11-21T04:37:45.955202",
     "exception": false,
     "start_time": "2020-11-21T04:37:45.814827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_numeric = [feat for feat in list(train_features.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:46.186846Z",
     "iopub.status.busy": "2020-11-21T04:37:46.184970Z",
     "iopub.status.idle": "2020-11-21T04:37:46.191934Z",
     "shell.execute_reply": "2020-11-21T04:37:46.191199Z"
    },
    "papermill": {
     "duration": 0.118966,
     "end_time": "2020-11-21T04:37:46.192138",
     "exception": false,
     "start_time": "2020-11-21T04:37:46.073172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lp = LabelPowerset()\n",
    "# train_tar_lp = lp.transform(train_targets[target_cols])\n",
    "# features = mutual_info_classif(train_features[cols_numeric].values, train_tar_lp)\n",
    "# info_gain = dict(zip(cols_numeric, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:46.412485Z",
     "iopub.status.busy": "2020-11-21T04:37:46.411233Z",
     "iopub.status.idle": "2020-11-21T04:37:46.642776Z",
     "shell.execute_reply": "2020-11-21T04:37:46.644220Z"
    },
    "papermill": {
     "duration": 0.349962,
     "end_time": "2020-11-21T04:37:46.644442",
     "exception": false,
     "start_time": "2020-11-21T04:37:46.294480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_gain = {'g-0': 0.16536566375573258, 'g-1': 0.03238642542722747, 'g-2': 0.02038068618937583, 'g-3': 0.09527671795569503, 'g-4': 0.07908530616603926, 'g-5': 0.025348972481901377, 'g-6': 0.05355601095899942, 'g-7': 0.14184118591618233, 'g-8': 0.2138847416035512, 'g-9': 0.10435325296164777, 'g-10': 0.15602108443308182, 'g-11': 0.07289685258891243, 'g-12': 0.11427909947471626, 'g-13': 0.1272941810593835, 'g-14': 0.09944541695424114, 'g-15': 0.04565967743223798, 'g-16': 0.1423456298264707, 'g-17': 0.12501647940795113, 'g-18': 0.07056301705786971, 'g-19': 0.04457810683971175, 'g-20': 0.18965887722988128, 'g-21': 0.06803247770675114, 'g-22': 0.1423287358038383, 'g-23': 0.060946006671929176, 'g-24': 0.107982656140031, 'g-25': 0.057650690164224194, 'g-26': 0.1238766487769638, 'g-27': 0.07199177266490153, 'g-28': 0.18687461407960626, 'g-29': 0.18120797110720677, 'g-30': 0.1819113316806975, 'g-31': 0.12417511142018345, 'g-32': 0.06884918035581222, 'g-33': 0.12378293326472001, 'g-34': 0.1327596761741976, 'g-35': 0.09950184615992708, 'g-36': 0.14125436564072924, 'g-37': 0.22127673820556293, 'g-38': 0.18872903625916138, 'g-39': 0.15238733874743193, 'g-40': 0.09793923435719787, 'g-41': 0.15685747585461485, 'g-42': 0.0660507388649707, 'g-43': 0.06050097693879941, 'g-44': 0.04270060533217279, 'g-45': 0.04177206803184408, 'g-46': 0.1253773187563123, 'g-47': 0.1502539563307037, 'g-48': 0.1576052645679109, 'g-49': 0.09081220100940346, 'g-50': 0.2304596074774965, 'g-51': 0.1275915531311833, 'g-52': 0.1520840633257441, 'g-53': 0.10065606722198073, 'g-54': 0.0679819420681822, 'g-55': 0.12531786342364892, 'g-56': 0.011558717321303646, 'g-57': 0.13309120127911633, 'g-58': 0.18425530697781678, 'g-59': 0.14765066302603813, 'g-60': 0.12175286400516683, 'g-61': 0.09930138628993834, 'g-62': 0.22142979647525518, 'g-63': 0.237288235961981, 'g-64': 0.07776849312408274, 'g-65': 0.1147765176905704, 'g-66': 0.13709758060749877, 'g-67': 0.14529817107071974, 'g-68': 0.17243748600696662, 'g-69': 0.06848208725586336, 'g-70': 0.13510554334285363, 'g-71': 0.053446647803481184, 'g-72': 0.19955924356595567, 'g-73': 0.07571375289977311, 'g-74': 0.11572783237363726, 'g-75': 0.3080205427081655, 'g-76': 0.05112910015115979, 'g-77': 0.04265682396133208, 'g-78': 0.0890439760639179, 'g-79': 0.1251680804942179, 'g-80': 0.09755163779536868, 'g-81': 0.10558849665722292, 'g-82': 0.028854833896020615, 'g-83': 0.1552905730272034, 'g-84': 0.1042158283409762, 'g-85': 0.1637057480340074, 'g-86': 0.13139133174824025, 'g-87': 0.09481537139826646, 'g-88': 0.08238744736747527, 'g-89': 0.09203536208879903, 'g-90': 0.13789428345059918, 'g-91': 0.23857292815517894, 'g-92': 0.1394837841813965, 'g-93': 0.08361404533625372, 'g-94': 0.10781069803060728, 'g-95': 0.1520414429166097, 'g-96': 0.16454480849009379, 'g-97': 0.1494572836772381, 'g-98': 0.16363620826016056, 'g-99': 0.11797143594701875, 'g-100': 0.2571539741904383, 'g-101': 0.03569610096287601, 'g-102': 0.20663306612790233, 'g-103': 0.09201033175821571, 'g-104': 0.03943936199092324, 'g-105': 0.1091660636965397, 'g-106': 0.16030300546952514, 'g-107': 0.1561113639018048, 'g-108': 0.07973785141838885, 'g-109': 0.1042395724785905, 'g-110': 0.08732221728318201, 'g-111': 0.08621140710657649, 'g-112': 0.12567929952601453, 'g-113': 0.1643520565412926, 'g-114': 0.13011837306610907, 'g-115': 0.05550823686001927, 'g-116': 0.10933324665341715, 'g-117': 0.16375191395924027, 'g-118': 0.10133622474899706, 'g-119': 0.0979350676391535, 'g-120': 0.05706711033305911, 'g-121': 0.20020434177631952, 'g-122': 0.11146247778987739, 'g-123': 0.17310287760109588, 'g-124': 0.07795218506830182, 'g-125': 0.08021665585772375, 'g-126': 0.13837978121167271, 'g-127': 0.054401691396393836, 'g-128': 0.19139386995875718, 'g-129': 0.07973388139019466, 'g-130': 0.08245600074046422, 'g-131': 0.19572993225039514, 'g-132': 0.1386884128217396, 'g-133': 0.16942071958720284, 'g-134': 0.17115045650048977, 'g-135': 0.1473298451760474, 'g-136': 0.13262521107136038, 'g-137': 0.1440466743263471, 'g-138': 0.13021515511668103, 'g-139': 0.1659799266692339, 'g-140': 0.1919116164222885, 'g-141': 0.06546812642130995, 'g-142': 0.175233703493074, 'g-143': 0.08328885695295707, 'g-144': 0.17058855094274072, 'g-145': 0.10103414632419572, 'g-146': 0.1684088496538454, 'g-147': 0.18708743494735192, 'g-148': 0.16565140139458556, 'g-149': 0.07504781840726782, 'g-150': 0.0680140421870794, 'g-151': 0.03343460572320289, 'g-152': 0.18329595445065916, 'g-153': 0.0555603503067692, 'g-154': 0.05614498394639256, 'g-155': 0.15838633604114705, 'g-156': 0.1395705539052461, 'g-157': 0.14958582685807542, 'g-158': 0.19566250883497105, 'g-159': 0.036545991906004716, 'g-160': 0.1518685974472671, 'g-161': 0.014069788492908408, 'g-162': 0.10251692777304733, 'g-163': 0.18359422784542367, 'g-164': 0.15929871720608269, 'g-165': 0.09760471807395277, 'g-166': 0.14074835729796842, 'g-167': 0.14103988270997547, 'g-168': 0.14150336173057276, 'g-169': 0.12545010407586332, 'g-170': 0.15280099596582275, 'g-171': 0.05795768115607203, 'g-172': 0.077509780759768, 'g-173': 0.16048599457395074, 'g-174': 0.08530790855275772, 'g-175': 0.24391131664825316, 'g-176': 0.08864313908073473, 'g-177': 0.11407331359821704, 'g-178': 0.19582368893908697, 'g-179': 0.06927149322784132, 'g-180': 0.08792397205796298, 'g-181': 0.2187322627000814, 'g-182': 0.06395574587205477, 'g-183': 0.19469586451910104, 'g-184': 0.05350363911045619, 'g-185': 0.18571739781990892, 'g-186': 0.1949491290949652, 'g-187': 0.1059197979748685, 'g-188': 0.06110808341351248, 'g-189': 0.1978492455341252, 'g-190': 0.08782281846713236, 'g-191': 0.0728362779735523, 'g-192': 0.12692928278274174, 'g-193': 0.06135472862879521, 'g-194': 0.13259764151225273, 'g-195': 0.22708187366528243, 'g-196': 0.1103036071445418, 'g-197': 0.11311139374185775, 'g-198': 0.0864860609028355, 'g-199': 0.1382055005998568, 'g-200': 0.09485033125301534, 'g-201': 0.20687054714330078, 'g-202': 0.18493505906831853, 'g-203': 0.23054039682447236, 'g-204': 0.07289169149519825, 'g-205': 0.03106952567131227, 'g-206': 0.20664576521315858, 'g-207': 0.1770633504545458, 'g-208': 0.2108786683919881, 'g-209': 0.06486479481988106, 'g-210': 0.163918637609898, 'g-211': 0.07695955973083901, 'g-212': 0.07256243540321972, 'g-213': 0.13755274094610215, 'g-214': 0.10948423410266361, 'g-215': 0.19403398425950158, 'g-216': 0.06211230679298918, 'g-217': 0.09603730034449676, 'g-218': 0.14648409140306828, 'g-219': 0.02725293531087125, 'g-220': 0.12939973368236668, 'g-221': 0.1632630068243177, 'g-222': 0.11425592749312496, 'g-223': 0.09361175635949603, 'g-224': 0.0659552041427478, 'g-225': 0.0847219177210441, 'g-226': 0.15872417022928786, 'g-227': 0.058925249013953795, 'g-228': 0.2215931715877817, 'g-229': 0.1621550760135797, 'g-230': 0.08436324860358013, 'g-231': 0.2441432107158228, 'g-232': 0.09452167820518032, 'g-233': 0.10326535230906586, 'g-234': 0.06740914163089329, 'g-235': 0.16697381832975644, 'g-236': 0.08882905758956827, 'g-237': 0.09142220037790771, 'g-238': 0.11125064908650639, 'g-239': 0.03496816942869874, 'g-240': 0.0890883843402932, 'g-241': 0.1913713775323327, 'g-242': 0.12864078637083765, 'g-243': 0.17314180284166447, 'g-244': 0.10508112096771693, 'g-245': 0.13626163522510826, 'g-246': 0.0771452555257568, 'g-247': 0.01393513004108815, 'g-248': 0.1982748515802566, 'g-249': 0.09656700125872764, 'g-250': 0.12947277902741394, 'g-251': 0.13889829095462503, 'g-252': 0.11578922328376251, 'g-253': 0.12588885899715674, 'g-254': 0.1617289502127699, 'g-255': 0.11212269176736811, 'g-256': 0.07932384339664189, 'g-257': 0.20480449675300338, 'g-258': 0.08720703201703106, 'g-259': 0.12690150073772877, 'g-260': 0.10500051354505668, 'g-261': 0.17650648443701744, 'g-262': 0.06543662500195335, 'g-263': 0.0816121956057918, 'g-264': 0.09119920093656297, 'g-265': 0.10013641173941323, 'g-266': 0.14524890726063244, 'g-267': 0.03765148559059561, 'g-268': 0.07283243451916022, 'g-269': 0.11749663586305292, 'g-270': 0.17341298609150524, 'g-271': 0.14943815381128545, 'g-272': 0.18570410959097483, 'g-273': 0.07829173272490131, 'g-274': 0.06578543359290467, 'g-275': 0.11665698529098822, 'g-276': 0.027787631917153277, 'g-277': 0.08574989340883299, 'g-278': 0.10843728476999104, 'g-279': 0.10628297231968276, 'g-280': 0.14399388148604153, 'g-281': 0.061903009152420196, 'g-282': 0.15183646215293578, 'g-283': 0.09476177356410886, 'g-284': 0.08657326150131706, 'g-285': 0.09032470150799199, 'g-286': 0.02324496819406985, 'g-287': 0.13170984778790285, 'g-288': 0.14262580779434852, 'g-289': 0.15561000072216657, 'g-290': 0.10082141689049351, 'g-291': 0.1783406777393921, 'g-292': 0.0855391481986878, 'g-293': 0.14437076739548704, 'g-294': 0.17107547385279354, 'g-295': 0.06272958349379554, 'g-296': 0.13038758598450606, 'g-297': 0.1883130958070245, 'g-298': 0.14612321904950232, 'g-299': 0.08374233014465471, 'g-300': 0.24214451407347326, 'g-301': 0.10784711333563646, 'g-302': 0.03709780832121101, 'g-303': 0.06363805040376747, 'g-304': 0.15170127241229991, 'g-305': 0.07393329623970857, 'g-306': 0.11166320811029529, 'g-307': 0.07063842875556503, 'g-308': 0.1159735586759334, 'g-309': 0.16011735726524723, 'g-310': 0.08556160793783096, 'g-311': 0.1561486373199914, 'g-312': 0.08313372867655477, 'g-313': 0.07667378472715569, 'g-314': 0.14213388642839853, 'g-315': 0.11338165268354494, 'g-316': 0.16693484797475922, 'g-317': 0.12663045598070166, 'g-318': 0.0657802895647901, 'g-319': 0.10356565549011165, 'g-320': 0.1697202136677083, 'g-321': 0.1036031993028601, 'g-322': 0.17777414766998945, 'g-323': 0.05532896845371216, 'g-324': 0.07783253441644256, 'g-325': 0.07505688379151998, 'g-326': 0.0336625197468754, 'g-327': 0.14147047702493598, 'g-328': 0.17779264059472677, 'g-329': 0.18105903436328052, 'g-330': 0.1100287595495697, 'g-331': 0.08470328978657093, 'g-332': 0.1838010035605322, 'g-333': 0.09699442244273371, 'g-334': 0.16586487336961042, 'g-335': 0.16094329263717277, 'g-336': 0.09455197095588108, 'g-337': 0.12122465849335828, 'g-338': 0.1539152297697184, 'g-339': 0.12975940547198217, 'g-340': 0.04252667391279186, 'g-341': 0.05089773821011612, 'g-342': 0.11301490211244136, 'g-343': 0.055019621266022334, 'g-344': 0.11652599111175643, 'g-345': 0.07756527275212388, 'g-346': 0.059819775238661776, 'g-347': 0.09666167111163393, 'g-348': 0.09409943975215906, 'g-349': 0.20259603300824125, 'g-350': 0.15214898358676976, 'g-351': 0.17764102970080575, 'g-352': 0.06091417327468207, 'g-353': 0.1793638678301841, 'g-354': 0.12764386795055493, 'g-355': 0.13880986116700544, 'g-356': 0.12155959610650946, 'g-357': 0.17062377869126255, 'g-358': 0.03370574375941793, 'g-359': 0.14495313388206732, 'g-360': 0.14818545755118429, 'g-361': 0.11124990606961305, 'g-362': 0.03099228850638802, 'g-363': 0.09107112078418567, 'g-364': 0.15726629128906122, 'g-365': 0.1820362830235549, 'g-366': 0.15735110428514165, 'g-367': 0.15894312257640753, 'g-368': 0.17694205417721776, 'g-369': 0.23159691584248598, 'g-370': 0.029948590967862287, 'g-371': 0.08356817192975363, 'g-372': 0.1260640385295888, 'g-373': 0.09329350222080546, 'g-374': 0.17752531284909878, 'g-375': 0.07580588294357327, 'g-376': 0.008993371368714698, 'g-377': 0.11605744434152943, 'g-378': 0.05263161512947345, 'g-379': 0.22026240219950743, 'g-380': 0.04819413915986992, 'g-381': 0.07827286363260022, 'g-382': 0.05584397425320997, 'g-383': 0.0430748362152622, 'g-384': 0.07699746612133751, 'g-385': 0.24027527812692373, 'g-386': 0.2206232055728634, 'g-387': 0.12962878801392375, 'g-388': 0.10069917190348576, 'g-389': 0.09274248816353747, 'g-390': 0.12755844954743, 'g-391': 0.061891261234122474, 'g-392': 0.23409519975683946, 'g-393': 0.14755369119144035, 'g-394': 0.14365813899002955, 'g-395': 0.1562148825627787, 'g-396': 0.05229814112499298, 'g-397': 0.13104196650845523, 'g-398': 0.13204720827108574, 'g-399': 0.018563839679180738, 'g-400': 0.13395682504360362, 'g-401': 0.03629671534652701, 'g-402': 0.1314539029208719, 'g-403': 0.07243734422459713, 'g-404': 0.05210462927506754, 'g-405': 0.12338281814512442, 'g-406': 0.2032431381255826, 'g-407': 0.17903476513936312, 'g-408': 0.11887123997738147, 'g-409': 0.16192829950219245, 'g-410': 0.20019045120813317, 'g-411': 0.1920023695854738, 'g-412': 0.057411089453180075, 'g-413': 0.012679609570461103, 'g-414': 0.2009991756974303, 'g-415': 0.06489922692300354, 'g-416': 0.044269964294591624, 'g-417': 0.1847174135736509, 'g-418': 0.18275041780076418, 'g-419': 0.05453403587779615, 'g-420': 0.051855714047088775, 'g-421': 0.10887724130494192, 'g-422': 0.12803700548685626, 'g-423': 0.15099978931379887, 'g-424': 0.15024966224815817, 'g-425': 0.04661054758353167, 'g-426': 0.07559710898931726, 'g-427': 0.1570306454682422, 'g-428': 0.06266051893650904, 'g-429': 0.11346016078103283, 'g-430': 0.015876622619090064, 'g-431': 0.13703506253201603, 'g-432': 0.14754237526219782, 'g-433': 0.15763429010435992, 'g-434': 0.15254122647112567, 'g-435': 0.05438188380840181, 'g-436': 0.07761491544056032, 'g-437': 0.10708781487797214, 'g-438': 0.05349438005669693, 'g-439': 0.1859293878013606, 'g-440': 0.12252777743395793, 'g-441': 0.14398198072599389, 'g-442': 0.11245816912941109, 'g-443': 0.21672117506934097, 'g-444': 0.08110879712936114, 'g-445': 0.08670534626183812, 'g-446': 0.08620939416467888, 'g-447': 0.10104840365137324, 'g-448': 0.10311813806201364, 'g-449': 0.09461032175466677, 'g-450': 0.12153423363843974, 'g-451': 0.09087593470634214, 'g-452': 0.015073317906795225, 'g-453': 0.08160225966664125, 'g-454': 0.07597196292339703, 'g-455': 0.10427244358599985, 'g-456': 0.14303230318155258, 'g-457': 0.06248174865202749, 'g-458': 0.12012416702596695, 'g-459': 0.15387196819217763, 'g-460': 0.18809530444651834, 'g-461': 0.08274334717980558, 'g-462': 0.05092400432005917, 'g-463': 0.0703103068878379, 'g-464': 0.1669964484620392, 'g-465': 0.03143864282943376, 'g-466': 0.06758772573962091, 'g-467': 0.16501757160194952, 'g-468': 0.1308598159239569, 'g-469': 0.05739682206900465, 'g-470': 0.08335002328149699, 'g-471': 0.06988721930826891, 'g-472': 0.055020653871244996, 'g-473': 0.08083880678301814, 'g-474': 0.10863398930731716, 'g-475': 0.10295024090047811, 'g-476': 0.0701217072243514, 'g-477': 0.10426234313724692, 'g-478': 0.1621476756958442, 'g-479': 0.1114306515204806, 'g-480': 0.1421750311719645, 'g-481': 0.04306192216100424, 'g-482': 0.07628941450511029, 'g-483': 0.03616207563129681, 'g-484': 0.08788169364477838, 'g-485': 0.08426327605913642, 'g-486': 0.2047534164054179, 'g-487': 0.12975076973616861, 'g-488': 0.15968529247809826, 'g-489': 0.20341120876060526, 'g-490': 0.03653772756527385, 'g-491': 0.12326434568103917, 'g-492': 0.0940516643626026, 'g-493': 0.10163075975734426, 'g-494': 0.15322020308395068, 'g-495': 0.0839673955316913, 'g-496': 0.09998962980109738, 'g-497': 0.12110316831932355, 'g-498': 0.12663381028052445, 'g-499': 0.1286014217842828, 'g-500': 0.1803637620373717, 'g-501': 0.06892776852744387, 'g-502': 0.10030444040913267, 'g-503': 0.1680051768943871, 'g-504': 0.11587881992675975, 'g-505': 0.04880918241340826, 'g-506': 0.1980527455372849, 'g-507': 0.06804442732789795, 'g-508': 0.14475821184680449, 'g-509': 0.12098541461824741, 'g-510': 0.04773207590343542, 'g-511': 0.08540333006153844, 'g-512': 0.23397984428924534, 'g-513': 0.02989826565903364, 'g-514': 0.04579741461862685, 'g-515': 0.1023351437040434, 'g-516': 0.037135148638883564, 'g-517': 0.13309157702545882, 'g-518': 0.053777161885862235, 'g-519': 0.06516871719903872, 'g-520': 0.06326431080860129, 'g-521': 0.059292825683400885, 'g-522': 0.2297250548796237, 'g-523': 0.08725456167204992, 'g-524': 0.06747897621290555, 'g-525': 0.12963481358632656, 'g-526': 0.07192641185196003, 'g-527': 0.03568825570622458, 'g-528': 0.06875255271310543, 'g-529': 0.19106859928091868, 'g-530': 0.05661709249040037, 'g-531': 0.1327944268356651, 'g-532': 0.05623992659521715, 'g-533': 0.16086880242368196, 'g-534': 0.20839357936126568, 'g-535': 0.08744962715836113, 'g-536': 0.025815824175616342, 'g-537': 0.13170027321408195, 'g-538': 0.059523014418769726, 'g-539': 0.10357705228381597, 'g-540': 0.10517646162812966, 'g-541': 0.11241079120680553, 'g-542': 0.06332053680446759, 'g-543': 0.10839964709655447, 'g-544': 0.05979684401420826, 'g-545': 0.09286962967935519, 'g-546': 0.16507691012506864, 'g-547': 0.0994802540680606, 'g-548': 0.09561599324147974, 'g-549': 0.047462617154252484, 'g-550': 0.02597650448136868, 'g-551': 0.1029145805377425, 'g-552': 0.043915200805758836, 'g-553': 0.17191346691138953, 'g-554': 0.10355941698799587, 'g-555': 0.03983255950142084, 'g-556': 0.06710893358661174, 'g-557': 0.07932223918078929, 'g-558': 0.09280873089307828, 'g-559': 0.15023784535187712, 'g-560': 0.06117857211298183, 'g-561': 0.1744063778092988, 'g-562': 0.12716849101655558, 'g-563': 0.1517070621553671, 'g-564': 0.1106093524623839, 'g-565': 0.15465910907212965, 'g-566': 0.11002736457449469, 'g-567': 0.03851419193338135, 'g-568': 0.16400305068381105, 'g-569': 0.20882523165362343, 'g-570': 0.1493614465153268, 'g-571': 0.027498173767191325, 'g-572': 0.031821741938257375, 'g-573': 0.05410167883461092, 'g-574': 0.1283108265708517, 'g-575': 0.11440170869260236, 'g-576': 0.08156451070947845, 'g-577': 0.12144942109193213, 'g-578': 0.16922218847159787, 'g-579': 0.07594161530741239, 'g-580': 0.10262190510905089, 'g-581': 0.04390003475563997, 'g-582': 0.06740787919542335, 'g-583': 0.06238665089533324, 'g-584': 0.030614897374029226, 'g-585': 0.021797850386856332, 'g-586': 0.09377900563626973, 'g-587': 0.05814854082952259, 'g-588': 0.1105640690982641, 'g-589': 0.08252363968293253, 'g-590': 0.1730259991685319, 'g-591': 0.08326606723626906, 'g-592': 0.05625757273077259, 'g-593': 0.11123007136324858, 'g-594': 0.10720088068136224, 'g-595': 0.06818606491434931, 'g-596': 0.08497667010228138, 'g-597': 0.16350755568632813, 'g-598': 0.07795364964950746, 'g-599': 0.08127615681081402, 'g-600': 0.10902938692847375, 'g-601': 0.07694275574328291, 'g-602': 0.0982441757653092, 'g-603': 0.14375766832706738, 'g-604': 0.08936560007388472, 'g-605': 0.0322852130613267, 'g-606': 0.06218918131835505, 'g-607': 0.0909350924313852, 'g-608': 0.07343292929748291, 'g-609': 0.062479662020916926, 'g-610': 0.06363588349192373, 'g-611': 0.047178181533859664, 'g-612': 0.023267168625569212, 'g-613': 0.10313319622205519, 'g-614': 0.12795810200293278, 'g-615': 0.16455045895239362, 'g-616': 0.07416517398763744, 'g-617': 0.10901023878485283, 'g-618': 0.11666669422120801, 'g-619': 0.13633404262865945, 'g-620': 0.19087902796224832, 'g-621': 0.10094466266795266, 'g-622': 0.0765320189743699, 'g-623': 0.09271096752473174, 'g-624': 0.1181164000437196, 'g-625': 0.1769157356037283, 'g-626': 0.12862231332864127, 'g-627': 0.06900232959237229, 'g-628': 0.15972275633567357, 'g-629': 0.17896013863432714, 'g-630': 0.06771765883421477, 'g-631': 0.1076247494494611, 'g-632': 0.1725378561745723, 'g-633': 0.08517238438271058, 'g-634': 0.13497910993205942, 'g-635': 0.204635713205497, 'g-636': 0.13167101236288126, 'g-637': 0.035527525865052034, 'g-638': 0.12615721092177967, 'g-639': 0.147100452610748, 'g-640': 0.09390450458037503, 'g-641': 0.12935894818054283, 'g-642': 0.10511659088065706, 'g-643': 0.0658706795930426, 'g-644': 0.09267494222353267, 'g-645': 0.12217278848771684, 'g-646': 0.10535772263441068, 'g-647': 0.13630918445266893, 'g-648': 0.09425135891154213, 'g-649': 0.03146785914971417, 'g-650': 0.04236469809357413, 'g-651': 0.0544986455666967, 'g-652': 0.10761583560559274, 'g-653': 0.05510763130183616, 'g-654': 0.04979706702844222, 'g-655': 0.05217255106493468, 'g-656': 0.07769146467920418, 'g-657': 0.08341373454794976, 'g-658': 0.08310628771372297, 'g-659': 0.07743455909561625, 'g-660': 0.033539161000653905, 'g-661': 0.13629948584147034, 'g-662': 0.15581354594167962, 'g-663': 0.06789575093749889, 'g-664': 0.17884079825670973, 'g-665': 0.11189711314679673, 'g-666': 0.040976579078032316, 'g-667': 0.07830040349233158, 'g-668': 0.10142466648370085, 'g-669': 0.17780906386673578, 'g-670': 0.13655730761405227, 'g-671': 0.15824992382406577, 'g-672': 0.21517050892343637, 'g-673': 0.17397690837068946, 'g-674': 0.17046577436238586, 'g-675': 0.08592953679728765, 'g-676': 0.05342368869905556, 'g-677': 0.1368484611124785, 'g-678': 0.10058728242480708, 'g-679': 0.09655466841718763, 'g-680': 0.0871590694164448, 'g-681': 0.10880451676915293, 'g-682': 0.054912756640666416, 'g-683': 0.19680691416086393, 'g-684': 0.09433344047711145, 'g-685': 0.09681401238215503, 'g-686': 0.10288496255983137, 'g-687': 0.051627522758704636, 'g-688': 0.06404237960079939, 'g-689': 0.18254835691552262, 'g-690': 0.09987164565157602, 'g-691': 0.15241069382782868, 'g-692': 0.03666107445823119, 'g-693': 0.12110961475207382, 'g-694': 0.029479572336891913, 'g-695': 0.11474064549336038, 'g-696': 0.12760206318113632, 'g-697': 0.12392438286006513, 'g-698': 0.14536225758806687, 'g-699': 0.12059241047794522, 'g-700': 0.12256441762609604, 'g-701': 0.12844348511967407, 'g-702': 0.13182668199270697, 'g-703': 0.11636788281543531, 'g-704': 0.13627450281943343, 'g-705': 0.16042582230670543, 'g-706': 0.12355877506112556, 'g-707': 0.005250828712061839, 'g-708': 0.1476962169555769, 'g-709': 0.1315193685394922, 'g-710': 0.07873113265734144, 'g-711': 0.12531957350339518, 'g-712': 0.18164239474974764, 'g-713': 0.020945829252316805, 'g-714': 0.09105313107503044, 'g-715': 0.017050497015484112, 'g-716': 0.0858185178680797, 'g-717': 0.11091177515971484, 'g-718': 0.07670210348503481, 'g-719': 0.026892365100590787, 'g-720': 0.14806507163978555, 'g-721': 0.1176756027385526, 'g-722': 0.07622857050218013, 'g-723': 0.19233141492045114, 'g-724': 0.15858745842314015, 'g-725': 0.06180312174959557, 'g-726': 0.16289709156253984, 'g-727': 0.09747095326855515, 'g-728': 0.18058800390515106, 'g-729': 0.08015208590777867, 'g-730': 0.051235214453147826, 'g-731': 0.23347977934589892, 'g-732': 0.06622681144224174, 'g-733': 0.15597170791580517, 'g-734': 0.05527667568577943, 'g-735': 0.06662901859059023, 'g-736': 0.09967409924797277, 'g-737': 0.037295012018612184, 'g-738': 0.12693591149394745, 'g-739': 0.06392333201297085, 'g-740': 0.04511931985468198, 'g-741': 0.08791935702571685, 'g-742': 0.1289871148205206, 'g-743': 0.1701396084234048, 'g-744': 0.22952666970406987, 'g-745': 0.13788474509850346, 'g-746': 0.0875724389520256, 'g-747': 0.06098705126289783, 'g-748': 0.06070485532117864, 'g-749': 0.1061399869135986, 'g-750': 0.17684030894615876, 'g-751': 0.07053560046310103, 'g-752': 0.12093275685227312, 'g-753': 0.09610285560997234, 'g-754': 0.08696763444613431, 'g-755': 0.035676756832764944, 'g-756': 0.03826892918201885, 'g-757': 0.09974336659258132, 'g-758': 0.09305288330497685, 'g-759': 0.15746025261411667, 'g-760': 0.19253852943861371, 'g-761': 0.21226630948217018, 'g-762': 0.04151650675811247, 'g-763': 0.14158358270253846, 'g-764': 0.16858907371683696, 'g-765': 0.09731433103624099, 'g-766': 0.09401819632985653, 'g-767': 0.12508003572667192, 'g-768': 0.08858144397629264, 'g-769': 0.11883397742590862, 'g-770': 0.14902629641635023, 'g-771': 0.14596299230710752, 'c-0': 0.18027605556534265, 'c-1': 0.2356018806479323, 'c-2': 0.21680667868386028, 'c-3': 0.18256580192158012, 'c-4': 0.20546663049673253, 'c-5': 0.19462546449157347, 'c-6': 0.2395951437961683, 'c-7': 0.16279933943354763, 'c-8': 0.2017084760151615, 'c-9': 0.22136561156080958, 'c-10': 0.22023437696402048, 'c-11': 0.20242490043306827, 'c-12': 0.1951371134135398, 'c-13': 0.2135842936282195, 'c-14': 0.19567791564295423, 'c-15': 0.1936687427444994, 'c-16': 0.15079121830738007, 'c-17': 0.22198629540710524, 'c-18': 0.23061787335278439, 'c-19': 0.18394498882724175, 'c-20': 0.18256563144532034, 'c-21': 0.20573697529538215, 'c-22': 0.15911312130151956, 'c-23': 0.1969616252651747, 'c-24': 0.18862554469651283, 'c-25': 0.19176698635460188, 'c-26': 0.23219858063352472, 'c-27': 0.1917156531367299, 'c-28': 0.1913870736952239, 'c-29': 0.16996280220109483, 'c-30': 0.21567893166363294, 'c-31': 0.20665161877598326, 'c-32': 0.1748172106821677, 'c-33': 0.22639549613335586, 'c-34': 0.21934396725894434, 'c-35': 0.18943160769484813, 'c-36': 0.20592565875402258, 'c-37': 0.17436662726382846, 'c-38': 0.22454680323538412, 'c-39': 0.17961783646593954, 'c-40': 0.2086796727015301, 'c-41': 0.20191469458260425, 'c-42': 0.2128310209454174, 'c-43': 0.1872478030396545, 'c-44': 0.20543649087846916, 'c-45': 0.195111903273804, 'c-46': 0.17900712406022468, 'c-47': 0.1971538920508653, 'c-48': 0.2042483146491305, 'c-49': 0.19619747124953957, 'c-50': 0.1975665095600938, 'c-51': 0.19541774911824028, 'c-52': 0.2144165341141102, 'c-53': 0.1810688324124241, 'c-54': 0.20037174091619558, 'c-55': 0.19314233550351734, 'c-56': 0.1510265314004835, 'c-57': 0.19252155976062113, 'c-58': 0.16679476108353075, 'c-59': 0.20121804538778054, 'c-60': 0.21931490197246006, 'c-61': 0.16989877131280195, 'c-62': 0.21625712738249891, 'c-63': 0.20426474992140697, 'c-64': 0.19283014549030053, 'c-65': 0.26850593939912937, 'c-66': 0.22365319574079745, 'c-67': 0.21264618563131332, 'c-68': 0.18572882386857614, 'c-69': 0.1693502832497238, 'c-70': 0.24428114188586125, 'c-71': 0.1893334349313074, 'c-72': 0.18573086261002203, 'c-73': 0.20533317957050912, 'c-74': 0.16291454288408502, 'c-75': 0.19303823218427407, 'c-76': 0.2104696899002665, 'c-77': 0.1996032580043403, 'c-78': 0.19775175593178052, 'c-79': 0.20602899149870968, 'c-80': 0.18390715540246028, 'c-81': 0.2009569254673167, 'c-82': 0.20861584726838878, 'c-83': 0.21820864678344076, 'c-84': 0.1962352165769916, 'c-85': 0.19536057517695227, 'c-86': 0.18619946881396832, 'c-87': 0.16713788433835663, 'c-88': 0.1848939320430283, 'c-89': 0.16322023363102556, 'c-90': 0.20388892857888852, 'c-91': 0.18710772133713593, 'c-92': 0.20920530222044675, 'c-93': 0.20824215409484115, 'c-94': 0.20864842749842172, 'c-95': 0.17779565677586007, 'c-96': 0.20697027625998032, 'c-97': 0.18190898421144297, 'c-98': 0.2587361233185357, 'c-99': 0.15206802991683777}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:47.016231Z",
     "iopub.status.busy": "2020-11-21T04:37:47.015288Z",
     "iopub.status.idle": "2020-11-21T04:37:47.752668Z",
     "shell.execute_reply": "2020-11-21T04:37:47.753854Z"
    },
    "papermill": {
     "duration": 0.980792,
     "end_time": "2020-11-21T04:37:47.754074",
     "exception": false,
     "start_time": "2020-11-21T04:37:46.773282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe37b104590>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "info_gain = pd.DataFrame(info_gain, index = [0]).T\n",
    "info_gain.columns = [\"info_gain\"]\n",
    "sns.distplot(info_gain[\"info_gain\"], bins = 60, color = \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:47.978937Z",
     "iopub.status.busy": "2020-11-21T04:37:47.977458Z",
     "iopub.status.idle": "2020-11-21T04:37:47.979654Z",
     "shell.execute_reply": "2020-11-21T04:37:47.978150Z"
    },
    "papermill": {
     "duration": 0.111259,
     "end_time": "2020-11-21T04:37:47.979776",
     "exception": false,
     "start_time": "2020-11-21T04:37:47.868517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_gain = info_gain.reset_index()\n",
    "info_gain.columns = [\"feat_names\", \"info_gain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:48.197773Z",
     "iopub.status.busy": "2020-11-21T04:37:48.196929Z",
     "iopub.status.idle": "2020-11-21T04:37:48.216269Z",
     "shell.execute_reply": "2020-11-21T04:37:48.217479Z"
    },
    "papermill": {
     "duration": 0.13544,
     "end_time": "2020-11-21T04:37:48.217735",
     "exception": false,
     "start_time": "2020-11-21T04:37:48.082295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_feat = info_gain[info_gain[\"info_gain\"] > 0.06][\"feat_names\"].to_list()\n",
    "selected_feat.extend(['sig_id', 'cp_type', 'cp_time', 'cp_dose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:48.406863Z",
     "iopub.status.busy": "2020-11-21T04:37:48.405509Z",
     "iopub.status.idle": "2020-11-21T04:37:48.460579Z",
     "shell.execute_reply": "2020-11-21T04:37:48.459970Z"
    },
    "papermill": {
     "duration": 0.136167,
     "end_time": "2020-11-21T04:37:48.460719",
     "exception": false,
     "start_time": "2020-11-21T04:37:48.324552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = train_features[selected_feat]\n",
    "test_features = test_features[selected_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:48.618998Z",
     "iopub.status.busy": "2020-11-21T04:37:48.618257Z",
     "iopub.status.idle": "2020-11-21T04:37:48.623244Z",
     "shell.execute_reply": "2020-11-21T04:37:48.622633Z"
    },
    "papermill": {
     "duration": 0.085513,
     "end_time": "2020-11-21T04:37:48.623360",
     "exception": false,
     "start_time": "2020-11-21T04:37:48.537847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_numeric = [feat for feat in list(train_features.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:48.780341Z",
     "iopub.status.busy": "2020-11-21T04:37:48.779445Z",
     "iopub.status.idle": "2020-11-21T04:37:48.827802Z",
     "shell.execute_reply": "2020-11-21T04:37:48.828425Z"
    },
    "papermill": {
     "duration": 0.130347,
     "end_time": "2020-11-21T04:37:48.828585",
     "exception": false,
     "start_time": "2020-11-21T04:37:48.698238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>g-10</th>\n",
       "      <th>g-11</th>\n",
       "      <th>g-12</th>\n",
       "      <th>g-13</th>\n",
       "      <th>...</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0620</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>0.5548</td>\n",
       "      <td>-0.0921</td>\n",
       "      <td>1.1830</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>-0.4015</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.3372</td>\n",
       "      <td>-0.4047</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>-1.1520</td>\n",
       "      <td>-0.4201</td>\n",
       "      <td>-0.0958</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6280</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>1.2300</td>\n",
       "      <td>-0.4797</td>\n",
       "      <td>-0.5631</td>\n",
       "      <td>-0.0366</td>\n",
       "      <td>-1.8300</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.5138</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>0.1792</td>\n",
       "      <td>-0.1321</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>-0.8269</td>\n",
       "      <td>-0.3584</td>\n",
       "      <td>-0.8511</td>\n",
       "      <td>-0.5844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>48</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.3254</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.1498</td>\n",
       "      <td>-0.8789</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>-0.2219</td>\n",
       "      <td>-0.5121</td>\n",
       "      <td>-0.9577</td>\n",
       "      <td>1.1750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>0.1608</td>\n",
       "      <td>-0.2239</td>\n",
       "      <td>-0.2431</td>\n",
       "      <td>-0.1777</td>\n",
       "      <td>-0.7480</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>0.5493</td>\n",
       "      <td>-0.5083</td>\n",
       "      <td>0.4112</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4753</td>\n",
       "      <td>-0.2504</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>0.8413</td>\n",
       "      <td>-0.4259</td>\n",
       "      <td>0.2434</td>\n",
       "      <td>id_fff8c2444</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.3055</td>\n",
       "      <td>-0.4726</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>-0.4532</td>\n",
       "      <td>-1.0790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>-0.5565</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.6727</td>\n",
       "      <td>-0.1851</td>\n",
       "      <td>2.8650</td>\n",
       "      <td>-0.2140</td>\n",
       "      <td>-0.6153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>-0.7985</td>\n",
       "      <td>0.5742</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>2.2700</td>\n",
       "      <td>0.2046</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>24</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>-0.8598</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>3.0790</td>\n",
       "      <td>1.2460</td>\n",
       "      <td>1.9460</td>\n",
       "      <td>1.4370</td>\n",
       "      <td>2.9780</td>\n",
       "      <td>2.2370</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>trt_cp</td>\n",
       "      <td>72</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows Ã— 752 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          g-0     g-3     g-4     g-7     g-8     g-9    g-10    g-11    g-12  \\\n",
       "0      1.0620 -0.6208 -0.1944 -0.0326  0.5548 -0.0921  1.1830  0.1530  0.5574   \n",
       "1      0.0743  0.0604  1.0190  0.3372 -0.4047  0.8507 -1.1520 -0.4201 -0.0958   \n",
       "2      0.6280 -0.0764 -0.0323  0.2155  0.0065  1.2300 -0.4797 -0.5631 -0.0366   \n",
       "3     -0.5138  0.5288  4.0620  0.1792 -0.1321 -1.0600 -0.8269 -0.3584 -0.8511   \n",
       "4     -0.3254  0.6919  1.4180 -0.1498 -0.8789  0.8630 -0.2219 -0.5121 -0.9577   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "21943  0.1608 -0.2239 -0.2431 -0.1777 -0.7480  0.1368  0.5493 -0.5083  0.4112   \n",
       "21944  0.1394 -0.5080 -0.4713  0.3055 -0.4726  0.1269  0.2531  0.1730 -0.4532   \n",
       "21945 -1.3260  0.9905 -0.7178 -0.5565  0.5112  0.6727 -0.1851  2.8650 -0.2140   \n",
       "21946  0.6660  0.2044  0.8531  0.0463  0.4299 -0.7985  0.5742  0.1421  2.2700   \n",
       "21947 -0.8598  0.7952 -0.3611  0.9146  3.0790  1.2460  1.9460  1.4370  2.9780   \n",
       "\n",
       "         g-13  ...    c-94    c-95    c-96    c-97    c-98    c-99  \\\n",
       "0     -0.4015  ... -0.1912  0.6584 -0.3981  0.2139  0.3801  0.4176   \n",
       "1      0.4590  ...  0.2957  0.4899  0.1522  0.1241  0.6077  0.7371   \n",
       "2     -1.8300  ... -1.3240 -0.3174 -0.6417 -0.2187 -1.4080  0.6931   \n",
       "3     -0.5844  ... -0.8632 -1.2880 -1.6210 -0.8784 -0.3876 -0.8154   \n",
       "4      1.1750  ...  0.5523 -0.3031  0.1094  0.2885 -0.3786  0.7125   \n",
       "...       ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "21943  0.1181  ... -0.4753 -0.2504 -0.7415  0.8413 -0.4259  0.2434   \n",
       "21944 -1.0790  ...  0.5372 -0.3246  0.0631  0.9171  0.5258  0.4680   \n",
       "21945 -0.6153  ... -0.8086 -0.9798 -0.2084 -0.1224 -0.2715  0.3689   \n",
       "21946  0.2046  ...  1.5230  0.7101  0.1732  0.7015 -0.6290  0.0740   \n",
       "21947  2.2370  ... -7.3860 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520   \n",
       "\n",
       "             sig_id  cp_type  cp_time  cp_dose  \n",
       "0      id_000644bb2   trt_cp       24       D1  \n",
       "1      id_000779bfc   trt_cp       72       D1  \n",
       "2      id_000a6266a   trt_cp       48       D1  \n",
       "3      id_0015fd391   trt_cp       48       D1  \n",
       "4      id_001626bd3   trt_cp       72       D2  \n",
       "...             ...      ...      ...      ...  \n",
       "21943  id_fff8c2444   trt_cp       72       D1  \n",
       "21944  id_fffb1ceed   trt_cp       24       D2  \n",
       "21945  id_fffb70c0c   trt_cp       24       D2  \n",
       "21946  id_fffcb9e7c   trt_cp       24       D1  \n",
       "21947  id_ffffdd77b   trt_cp       72       D1  \n",
       "\n",
       "[21948 rows x 752 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084124,
     "end_time": "2020-11-21T04:37:49.010515",
     "exception": false,
     "start_time": "2020-11-21T04:37:48.926391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "25a30a96-a61b-488e-afc3-b673fc00c525",
    "_uuid": "e56a0dcf-057b-496a-88cf-dd405ba00af9",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:49.175541Z",
     "iopub.status.busy": "2020-11-21T04:37:49.173461Z",
     "iopub.status.idle": "2020-11-21T04:37:49.176418Z",
     "shell.execute_reply": "2020-11-21T04:37:49.176995Z"
    },
    "papermill": {
     "duration": 0.091896,
     "end_time": "2020-11-21T04:37:49.177148",
     "exception": false,
     "start_time": "2020-11-21T04:37:49.085252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are actully adding the feature from the pca into the original data in here, it helps the score for some reason\n",
    "def pca(train, test, n_comp = 55, type_ = \"\", SEED = 123):\n",
    "    \n",
    "    kind = \"g\" if type_ == GENES else \"c\"\n",
    "    data = pd.concat([train[type_], test[type_]])\n",
    "    pca = PCA(n_components= n_comp)\n",
    "    data = pd.DataFrame(pca.fit_transform(data), columns = [f'pca_{kind}{i + 1}' for i in range(n_comp)])\n",
    "    train_ = data.iloc[:train.shape[0]]\n",
    "    test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n",
    "    train = pd.concat([train, train_], axis = 1)\n",
    "    test = pd.concat([test, test_], axis = 1)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:49.335790Z",
     "iopub.status.busy": "2020-11-21T04:37:49.334916Z",
     "iopub.status.idle": "2020-11-21T04:37:49.338447Z",
     "shell.execute_reply": "2020-11-21T04:37:49.338949Z"
    },
    "papermill": {
     "duration": 0.085267,
     "end_time": "2020-11-21T04:37:49.339118",
     "exception": false,
     "start_time": "2020-11-21T04:37:49.253851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All the col that has genes and cells\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "167eb790-c44c-488a-a5d8-00a631765427",
    "_uuid": "3396e928-9df2-41af-8c35-b07880c3a179",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:49.551992Z",
     "iopub.status.busy": "2020-11-21T04:37:49.550544Z",
     "iopub.status.idle": "2020-11-21T04:37:51.930011Z",
     "shell.execute_reply": "2020-11-21T04:37:51.928892Z"
    },
    "papermill": {
     "duration": 2.514668,
     "end_time": "2020-11-21T04:37:51.930160",
     "exception": false,
     "start_time": "2020-11-21T04:37:49.415492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features, test_features = pca(train_features, test_features, n_comp = 60, type_ = GENES)\n",
    "train_features, test_features = pca(train_features, test_features, n_comp = 20, type_ = CELLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.076794,
     "end_time": "2020-11-21T04:37:52.082963",
     "exception": false,
     "start_time": "2020-11-21T04:37:52.006169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.5 Gauss Rank Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:52.299714Z",
     "iopub.status.busy": "2020-11-21T04:37:52.297585Z",
     "iopub.status.idle": "2020-11-21T04:37:52.300478Z",
     "shell.execute_reply": "2020-11-21T04:37:52.301017Z"
    },
    "papermill": {
     "duration": 0.138027,
     "end_time": "2020-11-21T04:37:52.301181",
     "exception": false,
     "start_time": "2020-11-21T04:37:52.163154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols_numeric = [feat for feat in list(train_features.columns) if feat not in ['sig_id', 'cp_type', 'cp_time', 'cp_dose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:52.463667Z",
     "iopub.status.busy": "2020-11-21T04:37:52.462275Z",
     "iopub.status.idle": "2020-11-21T04:37:52.688407Z",
     "shell.execute_reply": "2020-11-21T04:37:52.687800Z"
    },
    "papermill": {
     "duration": 0.309019,
     "end_time": "2020-11-21T04:37:52.688536",
     "exception": false,
     "start_time": "2020-11-21T04:37:52.379517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.concat([train_features, test_features], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "0d1b2d54-fc20-4004-adf2-52d1bb8e0caa",
    "_uuid": "7f6bb532-a385-43d9-81de-73c63aac330f",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:37:53.028093Z",
     "iopub.status.busy": "2020-11-21T04:37:53.022558Z",
     "iopub.status.idle": "2020-11-21T04:38:11.377712Z",
     "shell.execute_reply": "2020-11-21T04:38:11.376946Z"
    },
    "papermill": {
     "duration": 18.611747,
     "end_time": "2020-11-21T04:38:11.377847",
     "exception": false,
     "start_time": "2020-11-21T04:37:52.766100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "scaler = GaussRankScaler()\n",
    "data[cols_numeric]  = scaler.fit_transform(data[cols_numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.079897,
     "end_time": "2020-11-21T04:38:11.534419",
     "exception": false,
     "start_time": "2020-11-21T04:38:11.454522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.6 Number of Positive and Negative Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:11.701474Z",
     "iopub.status.busy": "2020-11-21T04:38:11.699283Z",
     "iopub.status.idle": "2020-11-21T04:38:11.704364Z",
     "shell.execute_reply": "2020-11-21T04:38:11.703781Z"
    },
    "papermill": {
     "duration": 0.09071,
     "end_time": "2020-11-21T04:38:11.704491",
     "exception": false,
     "start_time": "2020-11-21T04:38:11.613781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_pos_num_neg(data, type_):\n",
    "    kind = \"g\" if type_ == GENES else \"c\"\n",
    "    train_ = data.iloc[:train_features.shape[0]].reset_index(drop = True)\n",
    "    test_ = data.iloc[train_features.shape[0]:].reset_index(drop = True)\n",
    "    for df in [train_, test_]:\n",
    "        df[f'{kind}_positive'] = df[type_].select_dtypes(include='float64').gt(0).sum(axis=1)\n",
    "        df[f'{kind}_negative'] = df[type_].select_dtypes(include='float64').lt(0).sum(axis=1)\n",
    "    return pd.concat([train_, test_], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:11.868236Z",
     "iopub.status.busy": "2020-11-21T04:38:11.866330Z",
     "iopub.status.idle": "2020-11-21T04:38:11.868966Z",
     "shell.execute_reply": "2020-11-21T04:38:11.869510Z"
    },
    "papermill": {
     "duration": 0.084142,
     "end_time": "2020-11-21T04:38:11.869641",
     "exception": false,
     "start_time": "2020-11-21T04:38:11.785499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = num_pos_num_neg(data, GENES)\n",
    "# data = num_pos_num_neg(data, CELLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:12.033634Z",
     "iopub.status.busy": "2020-11-21T04:38:12.032667Z",
     "iopub.status.idle": "2020-11-21T04:38:12.070861Z",
     "shell.execute_reply": "2020-11-21T04:38:12.071522Z"
    },
    "papermill": {
     "duration": 0.123123,
     "end_time": "2020-11-21T04:38:12.071708",
     "exception": false,
     "start_time": "2020-11-21T04:38:11.948585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>g-10</th>\n",
       "      <th>g-11</th>\n",
       "      <th>g-12</th>\n",
       "      <th>g-13</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_c11</th>\n",
       "      <th>pca_c12</th>\n",
       "      <th>pca_c13</th>\n",
       "      <th>pca_c14</th>\n",
       "      <th>pca_c15</th>\n",
       "      <th>pca_c16</th>\n",
       "      <th>pca_c17</th>\n",
       "      <th>pca_c18</th>\n",
       "      <th>pca_c19</th>\n",
       "      <th>pca_c20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.731433</td>\n",
       "      <td>-0.615315</td>\n",
       "      <td>-0.194236</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.513989</td>\n",
       "      <td>-0.130963</td>\n",
       "      <td>1.094414</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.438805</td>\n",
       "      <td>-0.398784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510728</td>\n",
       "      <td>-0.358790</td>\n",
       "      <td>-0.664748</td>\n",
       "      <td>0.170439</td>\n",
       "      <td>0.417648</td>\n",
       "      <td>-1.248147</td>\n",
       "      <td>1.057254</td>\n",
       "      <td>0.799326</td>\n",
       "      <td>0.338524</td>\n",
       "      <td>-0.231585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017020</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>0.849834</td>\n",
       "      <td>0.363904</td>\n",
       "      <td>-0.298420</td>\n",
       "      <td>0.730029</td>\n",
       "      <td>-0.808914</td>\n",
       "      <td>-0.402677</td>\n",
       "      <td>-0.110287</td>\n",
       "      <td>0.333450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790459</td>\n",
       "      <td>-0.245896</td>\n",
       "      <td>0.775866</td>\n",
       "      <td>0.097851</td>\n",
       "      <td>1.176227</td>\n",
       "      <td>-0.091898</td>\n",
       "      <td>-0.944473</td>\n",
       "      <td>0.301049</td>\n",
       "      <td>-0.174501</td>\n",
       "      <td>-0.354146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.462792</td>\n",
       "      <td>-0.091777</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>0.242395</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.975415</td>\n",
       "      <td>-0.385579</td>\n",
       "      <td>-0.540332</td>\n",
       "      <td>-0.061461</td>\n",
       "      <td>-1.514045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407775</td>\n",
       "      <td>-0.030715</td>\n",
       "      <td>0.490157</td>\n",
       "      <td>0.309419</td>\n",
       "      <td>-0.037031</td>\n",
       "      <td>1.033478</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.987307</td>\n",
       "      <td>0.424269</td>\n",
       "      <td>-0.572252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.531916</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>1.595325</td>\n",
       "      <td>0.206249</td>\n",
       "      <td>-0.066407</td>\n",
       "      <td>-0.889590</td>\n",
       "      <td>-0.656089</td>\n",
       "      <td>-0.347110</td>\n",
       "      <td>-0.771050</td>\n",
       "      <td>-0.580286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219267</td>\n",
       "      <td>-0.232902</td>\n",
       "      <td>0.800115</td>\n",
       "      <td>-1.241293</td>\n",
       "      <td>1.244906</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>-1.185702</td>\n",
       "      <td>-0.673316</td>\n",
       "      <td>-0.668749</td>\n",
       "      <td>0.484881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.341668</td>\n",
       "      <td>0.604615</td>\n",
       "      <td>0.951946</td>\n",
       "      <td>-0.103294</td>\n",
       "      <td>-0.684346</td>\n",
       "      <td>0.741863</td>\n",
       "      <td>-0.175749</td>\n",
       "      <td>-0.491503</td>\n",
       "      <td>-0.865195</td>\n",
       "      <td>0.786547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059066</td>\n",
       "      <td>-0.500958</td>\n",
       "      <td>0.283080</td>\n",
       "      <td>-0.198548</td>\n",
       "      <td>-0.419786</td>\n",
       "      <td>0.804734</td>\n",
       "      <td>0.052181</td>\n",
       "      <td>-0.244644</td>\n",
       "      <td>-0.607698</td>\n",
       "      <td>0.298622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25567</th>\n",
       "      <td>0.332498</td>\n",
       "      <td>-0.614731</td>\n",
       "      <td>0.734282</td>\n",
       "      <td>-0.505731</td>\n",
       "      <td>-0.815152</td>\n",
       "      <td>0.607761</td>\n",
       "      <td>0.248689</td>\n",
       "      <td>0.657995</td>\n",
       "      <td>0.617570</td>\n",
       "      <td>-0.689327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367103</td>\n",
       "      <td>0.358721</td>\n",
       "      <td>-1.140657</td>\n",
       "      <td>-0.952999</td>\n",
       "      <td>-0.371066</td>\n",
       "      <td>-0.264021</td>\n",
       "      <td>0.918896</td>\n",
       "      <td>0.054267</td>\n",
       "      <td>-0.103376</td>\n",
       "      <td>-0.399535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25568</th>\n",
       "      <td>-0.611355</td>\n",
       "      <td>0.285795</td>\n",
       "      <td>0.386483</td>\n",
       "      <td>0.023174</td>\n",
       "      <td>0.260311</td>\n",
       "      <td>1.252813</td>\n",
       "      <td>0.182971</td>\n",
       "      <td>0.283988</td>\n",
       "      <td>0.705622</td>\n",
       "      <td>-1.049138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279104</td>\n",
       "      <td>0.500434</td>\n",
       "      <td>0.544240</td>\n",
       "      <td>0.636289</td>\n",
       "      <td>-0.421027</td>\n",
       "      <td>0.173438</td>\n",
       "      <td>-0.076329</td>\n",
       "      <td>-0.067967</td>\n",
       "      <td>-0.547872</td>\n",
       "      <td>-0.238838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25569</th>\n",
       "      <td>-0.413779</td>\n",
       "      <td>-0.680627</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.823115</td>\n",
       "      <td>0.072491</td>\n",
       "      <td>1.027777</td>\n",
       "      <td>0.419733</td>\n",
       "      <td>-0.410056</td>\n",
       "      <td>-0.129953</td>\n",
       "      <td>-0.281797</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247738</td>\n",
       "      <td>0.860177</td>\n",
       "      <td>0.275511</td>\n",
       "      <td>-0.723795</td>\n",
       "      <td>0.213093</td>\n",
       "      <td>-0.329293</td>\n",
       "      <td>0.027324</td>\n",
       "      <td>0.472416</td>\n",
       "      <td>-0.007552</td>\n",
       "      <td>-0.057875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25570</th>\n",
       "      <td>-1.215509</td>\n",
       "      <td>0.892980</td>\n",
       "      <td>-1.179341</td>\n",
       "      <td>-0.844475</td>\n",
       "      <td>0.937721</td>\n",
       "      <td>0.552455</td>\n",
       "      <td>1.232116</td>\n",
       "      <td>0.507866</td>\n",
       "      <td>0.865186</td>\n",
       "      <td>0.835581</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127191</td>\n",
       "      <td>-0.068036</td>\n",
       "      <td>0.025104</td>\n",
       "      <td>0.149387</td>\n",
       "      <td>0.239948</td>\n",
       "      <td>-0.012613</td>\n",
       "      <td>0.571590</td>\n",
       "      <td>-0.074446</td>\n",
       "      <td>0.209324</td>\n",
       "      <td>0.112712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25571</th>\n",
       "      <td>-0.535701</td>\n",
       "      <td>-0.062814</td>\n",
       "      <td>-0.082960</td>\n",
       "      <td>0.417050</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>0.246355</td>\n",
       "      <td>0.214542</td>\n",
       "      <td>0.163374</td>\n",
       "      <td>-0.151556</td>\n",
       "      <td>0.859550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541262</td>\n",
       "      <td>-0.209967</td>\n",
       "      <td>-0.404342</td>\n",
       "      <td>-0.295206</td>\n",
       "      <td>0.123819</td>\n",
       "      <td>-0.063511</td>\n",
       "      <td>0.826715</td>\n",
       "      <td>-0.376886</td>\n",
       "      <td>0.519041</td>\n",
       "      <td>-0.891355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25572 rows Ã— 832 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            g-0       g-3       g-4       g-7       g-8       g-9      g-10  \\\n",
       "0      0.731433 -0.615315 -0.194236  0.005623  0.513989 -0.130963  1.094414   \n",
       "1      0.017020  0.027107  0.849834  0.363904 -0.298420  0.730029 -0.808914   \n",
       "2      0.462792 -0.091777 -0.039121  0.242395  0.050070  0.975415 -0.385579   \n",
       "3     -0.531916  0.455184  1.595325  0.206249 -0.066407 -0.889590 -0.656089   \n",
       "4     -0.341668  0.604615  0.951946 -0.103294 -0.684346  0.741863 -0.175749   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "25567  0.332498 -0.614731  0.734282 -0.505731 -0.815152  0.607761  0.248689   \n",
       "25568 -0.611355  0.285795  0.386483  0.023174  0.260311  1.252813  0.182971   \n",
       "25569 -0.413779 -0.680627  0.003715  0.823115  0.072491  1.027777  0.419733   \n",
       "25570 -1.215509  0.892980 -1.179341 -0.844475  0.937721  0.552455  1.232116   \n",
       "25571 -0.535701 -0.062814 -0.082960  0.417050  0.005587  0.246355  0.214542   \n",
       "\n",
       "           g-11      g-12      g-13  ...   pca_c11   pca_c12   pca_c13  \\\n",
       "0      0.089109  0.438805 -0.398784  ...  0.510728 -0.358790 -0.664748   \n",
       "1     -0.402677 -0.110287  0.333450  ...  0.790459 -0.245896  0.775866   \n",
       "2     -0.540332 -0.061461 -1.514045  ... -0.407775 -0.030715  0.490157   \n",
       "3     -0.347110 -0.771050 -0.580286  ...  0.219267 -0.232902  0.800115   \n",
       "4     -0.491503 -0.865195  0.786547  ...  0.059066 -0.500958  0.283080   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "25567  0.657995  0.617570 -0.689327  ...  0.367103  0.358721 -1.140657   \n",
       "25568  0.283988  0.705622 -1.049138  ...  0.279104  0.500434  0.544240   \n",
       "25569 -0.410056 -0.129953 -0.281797  ... -0.247738  0.860177  0.275511   \n",
       "25570  0.507866  0.865186  0.835581  ... -0.127191 -0.068036  0.025104   \n",
       "25571  0.163374 -0.151556  0.859550  ...  0.541262 -0.209967 -0.404342   \n",
       "\n",
       "        pca_c14   pca_c15   pca_c16   pca_c17   pca_c18   pca_c19   pca_c20  \n",
       "0      0.170439  0.417648 -1.248147  1.057254  0.799326  0.338524 -0.231585  \n",
       "1      0.097851  1.176227 -0.091898 -0.944473  0.301049 -0.174501 -0.354146  \n",
       "2      0.309419 -0.037031  1.033478  0.205850  0.987307  0.424269 -0.572252  \n",
       "3     -1.241293  1.244906  0.862069 -1.185702 -0.673316 -0.668749  0.484881  \n",
       "4     -0.198548 -0.419786  0.804734  0.052181 -0.244644 -0.607698  0.298622  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "25567 -0.952999 -0.371066 -0.264021  0.918896  0.054267 -0.103376 -0.399535  \n",
       "25568  0.636289 -0.421027  0.173438 -0.076329 -0.067967 -0.547872 -0.238838  \n",
       "25569 -0.723795  0.213093 -0.329293  0.027324  0.472416 -0.007552 -0.057875  \n",
       "25570  0.149387  0.239948 -0.012613  0.571590 -0.074446  0.209324  0.112712  \n",
       "25571 -0.295206  0.123819 -0.063511  0.826715 -0.376886  0.519041 -0.891355  \n",
       "\n",
       "[25572 rows x 832 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090494,
     "end_time": "2020-11-21T04:38:12.247928",
     "exception": false,
     "start_time": "2020-11-21T04:38:12.157434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.7 Stats of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:12.413438Z",
     "iopub.status.busy": "2020-11-21T04:38:12.412532Z",
     "iopub.status.idle": "2020-11-21T04:38:12.416248Z",
     "shell.execute_reply": "2020-11-21T04:38:12.415642Z"
    },
    "papermill": {
     "duration": 0.089596,
     "end_time": "2020-11-21T04:38:12.416370",
     "exception": false,
     "start_time": "2020-11-21T04:38:12.326774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#All the col that has genes and cells\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:12.584827Z",
     "iopub.status.busy": "2020-11-21T04:38:12.583697Z",
     "iopub.status.idle": "2020-11-21T04:38:14.606073Z",
     "shell.execute_reply": "2020-11-21T04:38:14.606599Z"
    },
    "papermill": {
     "duration": 2.110646,
     "end_time": "2020-11-21T04:38:14.606754",
     "exception": false,
     "start_time": "2020-11-21T04:38:12.496108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for stats in tqdm.tqdm([\"mean\", \"std\", \"kurt\", \"skew\"]):\n",
    "    data[\"g_\" + stats] = getattr(data[GENES], stats)(axis = 1)\n",
    "    data[\"c_\" + stats] = getattr(data[CELLS], stats)(axis = 1)    \n",
    "#     data[\"gc_\" + stats] = getattr(data[GENES + CELLS], stats)(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:14.779576Z",
     "iopub.status.busy": "2020-11-21T04:38:14.778695Z",
     "iopub.status.idle": "2020-11-21T04:38:14.783477Z",
     "shell.execute_reply": "2020-11-21T04:38:14.782814Z"
    },
    "papermill": {
     "duration": 0.094923,
     "end_time": "2020-11-21T04:38:14.783607",
     "exception": false,
     "start_time": "2020-11-21T04:38:14.688684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiply_feat(df):\n",
    "    df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "    df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "    df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "    df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "    df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "#     df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "#     df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "#     df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "#     df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "#     df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "#     df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "#     df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "#     df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "#     df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "#     df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "#     df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "#     df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "#     df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "#     df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "#     df['c90_c55'] = df['c-90'] * df['c-55']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:14.955997Z",
     "iopub.status.busy": "2020-11-21T04:38:14.954924Z",
     "iopub.status.idle": "2020-11-21T04:38:14.958150Z",
     "shell.execute_reply": "2020-11-21T04:38:14.957443Z"
    },
    "papermill": {
     "duration": 0.09162,
     "end_time": "2020-11-21T04:38:14.958485",
     "exception": false,
     "start_time": "2020-11-21T04:38:14.866865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = multiply_feat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.083,
     "end_time": "2020-11-21T04:38:15.124168",
     "exception": false,
     "start_time": "2020-11-21T04:38:15.041168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "de9aa3cb-01f0-4e29-b580-13286315aaa3",
    "_uuid": "2e587cf3-1478-4308-bbfc-5c5c1b4acade",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:15.300226Z",
     "iopub.status.busy": "2020-11-21T04:38:15.298739Z",
     "iopub.status.idle": "2020-11-21T04:38:15.357165Z",
     "shell.execute_reply": "2020-11-21T04:38:15.356508Z"
    },
    "papermill": {
     "duration": 0.149113,
     "end_time": "2020-11-21T04:38:15.357292",
     "exception": false,
     "start_time": "2020-11-21T04:38:15.208179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Spliting the data back into train and test\n",
    "train_features = data.iloc[:train_features.shape[0]].reset_index(drop = True)\n",
    "test_features = data.iloc[train_features.shape[0]:].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084382,
     "end_time": "2020-11-21T04:38:15.525090",
     "exception": false,
     "start_time": "2020-11-21T04:38:15.440708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082628,
     "end_time": "2020-11-21T04:38:15.691633",
     "exception": false,
     "start_time": "2020-11-21T04:38:15.609005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Get Dummies & Dropping sig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "e81bae51-ad02-4047-87a5-cc786773b9c6",
    "_uuid": "ab77edfc-b403-46a2-9dcc-9857ab605f82",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:15.868488Z",
     "iopub.status.busy": "2020-11-21T04:38:15.867360Z",
     "iopub.status.idle": "2020-11-21T04:38:15.871712Z",
     "shell.execute_reply": "2020-11-21T04:38:15.873146Z"
    },
    "papermill": {
     "duration": 0.097612,
     "end_time": "2020-11-21T04:38:15.873392",
     "exception": false,
     "start_time": "2020-11-21T04:38:15.775780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(data, target=False):\n",
    "    data.drop(\"sig_id\", axis=1, inplace=True)\n",
    "    if target == False:\n",
    "        data[\"cp_dose_time\"] = data[\"cp_dose\"] + \"_\" + data[\"cp_time\"].astype(\"str\")\n",
    "        data[\"cp_time\"] = data[\"cp_time\"].map({24:0, 48:1, 72:2})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "27aa2178-9d1a-420f-a321-e95ddd73711f",
    "_uuid": "2d4ee019-392b-4a08-8432-c44b36c978a7",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:16.076319Z",
     "iopub.status.busy": "2020-11-21T04:38:16.074704Z",
     "iopub.status.idle": "2020-11-21T04:38:16.439084Z",
     "shell.execute_reply": "2020-11-21T04:38:16.438455Z"
    },
    "papermill": {
     "duration": 0.460824,
     "end_time": "2020-11-21T04:38:16.439208",
     "exception": false,
     "start_time": "2020-11-21T04:38:15.978384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = preprocess(train_features)\n",
    "train_targets = preprocess(train_targets, target=True)\n",
    "train_features = pd.get_dummies(train_features)\n",
    "test_features = preprocess(test_features)\n",
    "test_features = pd.get_dummies(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:16.629566Z",
     "iopub.status.busy": "2020-11-21T04:38:16.628630Z",
     "iopub.status.idle": "2020-11-21T04:38:16.668686Z",
     "shell.execute_reply": "2020-11-21T04:38:16.667515Z"
    },
    "papermill": {
     "duration": 0.144023,
     "end_time": "2020-11-21T04:38:16.668820",
     "exception": false,
     "start_time": "2020-11-21T04:38:16.524797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-7</th>\n",
       "      <th>g-8</th>\n",
       "      <th>g-9</th>\n",
       "      <th>g-10</th>\n",
       "      <th>g-11</th>\n",
       "      <th>g-12</th>\n",
       "      <th>g-13</th>\n",
       "      <th>...</th>\n",
       "      <th>c_skew</th>\n",
       "      <th>cp_type_trt_cp</th>\n",
       "      <th>cp_dose_D1</th>\n",
       "      <th>cp_dose_D2</th>\n",
       "      <th>cp_dose_time_D1_24</th>\n",
       "      <th>cp_dose_time_D1_48</th>\n",
       "      <th>cp_dose_time_D1_72</th>\n",
       "      <th>cp_dose_time_D2_24</th>\n",
       "      <th>cp_dose_time_D2_48</th>\n",
       "      <th>cp_dose_time_D2_72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.731433</td>\n",
       "      <td>-0.615315</td>\n",
       "      <td>-0.194236</td>\n",
       "      <td>0.005623</td>\n",
       "      <td>0.513989</td>\n",
       "      <td>-0.130963</td>\n",
       "      <td>1.094414</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.438805</td>\n",
       "      <td>-0.398784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017020</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>0.849834</td>\n",
       "      <td>0.363904</td>\n",
       "      <td>-0.298420</td>\n",
       "      <td>0.730029</td>\n",
       "      <td>-0.808914</td>\n",
       "      <td>-0.402677</td>\n",
       "      <td>-0.110287</td>\n",
       "      <td>0.333450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043717</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.462792</td>\n",
       "      <td>-0.091777</td>\n",
       "      <td>-0.039121</td>\n",
       "      <td>0.242395</td>\n",
       "      <td>0.050070</td>\n",
       "      <td>0.975415</td>\n",
       "      <td>-0.385579</td>\n",
       "      <td>-0.540332</td>\n",
       "      <td>-0.061461</td>\n",
       "      <td>-1.514045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379548</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.531916</td>\n",
       "      <td>0.455184</td>\n",
       "      <td>1.595325</td>\n",
       "      <td>0.206249</td>\n",
       "      <td>-0.066407</td>\n",
       "      <td>-0.889590</td>\n",
       "      <td>-0.656089</td>\n",
       "      <td>-0.347110</td>\n",
       "      <td>-0.771050</td>\n",
       "      <td>-0.580286</td>\n",
       "      <td>...</td>\n",
       "      <td>2.010409</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.341668</td>\n",
       "      <td>0.604615</td>\n",
       "      <td>0.951946</td>\n",
       "      <td>-0.103294</td>\n",
       "      <td>-0.684346</td>\n",
       "      <td>0.741863</td>\n",
       "      <td>-0.175749</td>\n",
       "      <td>-0.491503</td>\n",
       "      <td>-0.865195</td>\n",
       "      <td>0.786547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21943</th>\n",
       "      <td>0.089366</td>\n",
       "      <td>-0.226762</td>\n",
       "      <td>-0.244678</td>\n",
       "      <td>-0.130161</td>\n",
       "      <td>-0.582338</td>\n",
       "      <td>0.056587</td>\n",
       "      <td>0.510729</td>\n",
       "      <td>-0.487753</td>\n",
       "      <td>0.312606</td>\n",
       "      <td>0.055267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>0.072056</td>\n",
       "      <td>-0.499092</td>\n",
       "      <td>-0.479222</td>\n",
       "      <td>0.333563</td>\n",
       "      <td>-0.356318</td>\n",
       "      <td>0.048109</td>\n",
       "      <td>0.229485</td>\n",
       "      <td>0.104717</td>\n",
       "      <td>-0.417681</td>\n",
       "      <td>-1.131700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367298</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21945</th>\n",
       "      <td>-1.346695</td>\n",
       "      <td>0.878667</td>\n",
       "      <td>-0.735139</td>\n",
       "      <td>-0.446960</td>\n",
       "      <td>0.479298</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>-0.144491</td>\n",
       "      <td>1.105080</td>\n",
       "      <td>-0.209672</td>\n",
       "      <td>-0.614014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484219</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21946</th>\n",
       "      <td>0.490027</td>\n",
       "      <td>0.157679</td>\n",
       "      <td>0.732402</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.405150</td>\n",
       "      <td>-0.742506</td>\n",
       "      <td>0.534247</td>\n",
       "      <td>0.080075</td>\n",
       "      <td>1.093433</td>\n",
       "      <td>0.127083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200423</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21947</th>\n",
       "      <td>-0.939748</td>\n",
       "      <td>0.695101</td>\n",
       "      <td>-0.365699</td>\n",
       "      <td>1.045868</td>\n",
       "      <td>1.472960</td>\n",
       "      <td>0.980444</td>\n",
       "      <td>1.376366</td>\n",
       "      <td>0.826468</td>\n",
       "      <td>1.224178</td>\n",
       "      <td>0.993635</td>\n",
       "      <td>...</td>\n",
       "      <td>1.850527</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21948 rows Ã— 846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            g-0       g-3       g-4       g-7       g-8       g-9      g-10  \\\n",
       "0      0.731433 -0.615315 -0.194236  0.005623  0.513989 -0.130963  1.094414   \n",
       "1      0.017020  0.027107  0.849834  0.363904 -0.298420  0.730029 -0.808914   \n",
       "2      0.462792 -0.091777 -0.039121  0.242395  0.050070  0.975415 -0.385579   \n",
       "3     -0.531916  0.455184  1.595325  0.206249 -0.066407 -0.889590 -0.656089   \n",
       "4     -0.341668  0.604615  0.951946 -0.103294 -0.684346  0.741863 -0.175749   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21943  0.089366 -0.226762 -0.244678 -0.130161 -0.582338  0.056587  0.510729   \n",
       "21944  0.072056 -0.499092 -0.479222  0.333563 -0.356318  0.048109  0.229485   \n",
       "21945 -1.346695  0.878667 -0.735139 -0.446960  0.479298  0.549999 -0.144491   \n",
       "21946  0.490027  0.157679  0.732402  0.079245  0.405150 -0.742506  0.534247   \n",
       "21947 -0.939748  0.695101 -0.365699  1.045868  1.472960  0.980444  1.376366   \n",
       "\n",
       "           g-11      g-12      g-13  ...    c_skew  cp_type_trt_cp  \\\n",
       "0      0.089109  0.438805 -0.398784  ...  0.118667               1   \n",
       "1     -0.402677 -0.110287  0.333450  ... -0.043717               1   \n",
       "2     -0.540332 -0.061461 -1.514045  ...  0.379548               1   \n",
       "3     -0.347110 -0.771050 -0.580286  ...  2.010409               1   \n",
       "4     -0.491503 -0.865195  0.786547  ...  0.191181               1   \n",
       "...         ...       ...       ...  ...       ...             ...   \n",
       "21943 -0.487753  0.312606  0.055267  ...  0.473883               1   \n",
       "21944  0.104717 -0.417681 -1.131700  ...  0.367298               1   \n",
       "21945  1.105080 -0.209672 -0.614014  ...  0.484219               1   \n",
       "21946  0.080075  1.093433  0.127083  ...  0.200423               1   \n",
       "21947  0.826468  1.224178  0.993635  ...  1.850527               1   \n",
       "\n",
       "       cp_dose_D1  cp_dose_D2  cp_dose_time_D1_24  cp_dose_time_D1_48  \\\n",
       "0               1           0                   1                   0   \n",
       "1               1           0                   0                   0   \n",
       "2               1           0                   0                   1   \n",
       "3               1           0                   0                   1   \n",
       "4               0           1                   0                   0   \n",
       "...           ...         ...                 ...                 ...   \n",
       "21943           1           0                   0                   0   \n",
       "21944           0           1                   0                   0   \n",
       "21945           0           1                   0                   0   \n",
       "21946           1           0                   1                   0   \n",
       "21947           1           0                   0                   0   \n",
       "\n",
       "       cp_dose_time_D1_72  cp_dose_time_D2_24  cp_dose_time_D2_48  \\\n",
       "0                       0                   0                   0   \n",
       "1                       1                   0                   0   \n",
       "2                       0                   0                   0   \n",
       "3                       0                   0                   0   \n",
       "4                       0                   0                   0   \n",
       "...                   ...                 ...                 ...   \n",
       "21943                   1                   0                   0   \n",
       "21944                   0                   1                   0   \n",
       "21945                   0                   1                   0   \n",
       "21946                   0                   0                   0   \n",
       "21947                   1                   0                   0   \n",
       "\n",
       "       cp_dose_time_D2_72  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "...                   ...  \n",
       "21943                   0  \n",
       "21944                   0  \n",
       "21945                   0  \n",
       "21946                   0  \n",
       "21947                   0  \n",
       "\n",
       "[21948 rows x 846 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b88ba9a1-8c8f-4017-8f66-11502b1a66c0",
    "_uuid": "ec5a0a57-9fe2-4010-b301-4eebe652da87",
    "papermill": {
     "duration": 0.08508,
     "end_time": "2020-11-21T04:38:16.839493",
     "exception": false,
     "start_time": "2020-11-21T04:38:16.754413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Define Error/Loss Metric and Prediction clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "efaf4180-50af-4dcc-b3b0-1c4038301ce5",
    "_uuid": "16856a4e-8060-40d7-ac95-632d81b42b95",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:17.017907Z",
     "iopub.status.busy": "2020-11-21T04:38:17.015998Z",
     "iopub.status.idle": "2020-11-21T04:38:17.018687Z",
     "shell.execute_reply": "2020-11-21T04:38:17.019238Z"
    },
    "papermill": {
     "duration": 0.094651,
     "end_time": "2020-11-21T04:38:17.019376",
     "exception": false,
     "start_time": "2020-11-21T04:38:16.924725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_log_loss(y_true, y_pred):\n",
    "    losses = []\n",
    "    for col in y_true.columns:\n",
    "        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "516b17c4-4697-4194-a9a1-e07020699394",
    "_uuid": "ad337b21-38a2-4984-b182-05c23e91307b",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:17.196182Z",
     "iopub.status.busy": "2020-11-21T04:38:17.195366Z",
     "iopub.status.idle": "2020-11-21T04:38:17.199634Z",
     "shell.execute_reply": "2020-11-21T04:38:17.199057Z"
    },
    "papermill": {
     "duration": 0.095263,
     "end_time": "2020-11-21T04:38:17.199763",
     "exception": false,
     "start_time": "2020-11-21T04:38:17.104500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip(y_pred, c_min, c_max):\n",
    "    return np.clip(y_pred, c_min, c_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "c1529cef-55df-4c59-a23d-2b21e6dd49ba",
    "_uuid": "6af491c3-aebc-4fd9-b2f8-1a78fe1c71a4",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:17.374796Z",
     "iopub.status.busy": "2020-11-21T04:38:17.373847Z",
     "iopub.status.idle": "2020-11-21T04:38:17.378416Z",
     "shell.execute_reply": "2020-11-21T04:38:17.377691Z"
    },
    "papermill": {
     "duration": 0.09409,
     "end_time": "2020-11-21T04:38:17.378534",
     "exception": false,
     "start_time": "2020-11-21T04:38:17.284444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_min = 0.001\n",
    "p_max = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1d569890-835d-4698-b4d3-52ae6ee6adea",
    "_uuid": "af8f3cde-4a98-4edb-a255-6d845e38a274",
    "papermill": {
     "duration": 0.086957,
     "end_time": "2020-11-21T04:38:17.551908",
     "exception": false,
     "start_time": "2020-11-21T04:38:17.464951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Create Model and Perform KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.083736,
     "end_time": "2020-11-21T04:38:17.721253",
     "exception": false,
     "start_time": "2020-11-21T04:38:17.637517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1.1 Two Layer DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:17.907935Z",
     "iopub.status.busy": "2020-11-21T04:38:17.906897Z",
     "iopub.status.idle": "2020-11-21T04:38:17.909219Z",
     "shell.execute_reply": "2020-11-21T04:38:17.909766Z"
    },
    "papermill": {
     "duration": 0.103459,
     "end_time": "2020-11-21T04:38:17.909916",
     "exception": false,
     "start_time": "2020-11-21T04:38:17.806457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The model param is tuned with optuna \n",
    "#The leaky ReLu performs the best\n",
    "def create_model_v1(shape):\n",
    "    model = M.Sequential()\n",
    "    model.add(L.Input(shape = (shape)))\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.397816152960733))\n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(521)))\n",
    "    model.add(L.LeakyReLU(alpha = 0.10059420295821832))\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.4365714774136811))\n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(1634)))\n",
    "    model.add(L.LeakyReLU(alpha = 0.10059420295821832))\n",
    "    model.add(L.BatchNormalization())\n",
    "    model.add(L.Dropout(0.4365714774136811))\n",
    "    model.add(tfa.layers.WeightNormalization(L.Dense(206, activation = \"sigmoid\")))\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr = 2e-3, weight_decay = 1e-5, clipvalue = 756), \n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001),\n",
    "                  metrics=tf.keras.metrics.BinaryCrossentropy())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.132436,
     "end_time": "2020-11-21T04:38:18.130170",
     "exception": false,
     "start_time": "2020-11-21T04:38:17.997734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1.2 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:18.381252Z",
     "iopub.status.busy": "2020-11-21T04:38:18.380162Z",
     "iopub.status.idle": "2020-11-21T04:38:18.392917Z",
     "shell.execute_reply": "2020-11-21T04:38:18.393885Z"
    },
    "papermill": {
     "duration": 0.142337,
     "end_time": "2020-11-21T04:38:18.394129",
     "exception": false,
     "start_time": "2020-11-21T04:38:18.251792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ann(seed, fold, train_features, train_targets, train_ind, val_ind, shape):\n",
    "\n",
    "    model = create_model_v1(shape)\n",
    "    \n",
    "            \n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold + 1}')\n",
    "\n",
    "    checkpoint_path = f'repeat:{seed}_Fold:{fold}.hdf5'\n",
    "    cb_checkpt = C.ModelCheckpoint(checkpoint_path, monitor = 'val_binary_crossentropy', verbose = 2, save_best_only = True, save_weights_only = True, mode = 'auto')\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_binary_crossentropy',\n",
    "                                                    mode = 'min',\n",
    "                                                    factor = 0.2,\n",
    "                                                    patience = 3,\n",
    "                                                     \n",
    "                                                     \n",
    "                                                    verbose = 3)\n",
    "    early = tf.keras.callbacks.EarlyStopping(monitor = 'val_binary_crossentropy',\n",
    "                                            mode = 'min',\n",
    "                                            patience = 10,\n",
    "                                            restore_best_weights = True,\n",
    "                                            verbose = 3)\n",
    "    model.fit(train_features.values[train_ind],\n",
    "            train_targets.values[train_ind], validation_data=(train_features.values[val_ind], train_targets.values[val_ind]),\n",
    "            epochs=200, batch_size=96, verbose=2, callbacks = [reduce_lr, early, cb_checkpt])\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.127036,
     "end_time": "2020-11-21T04:38:18.642322",
     "exception": false,
     "start_time": "2020-11-21T04:38:18.515286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.2 TabNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.132387,
     "end_time": "2020-11-21T04:38:18.897219",
     "exception": false,
     "start_time": "2020-11-21T04:38:18.764832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2.1 AdaBelief Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:19.159642Z",
     "iopub.status.busy": "2020-11-21T04:38:19.158259Z",
     "iopub.status.idle": "2020-11-21T04:38:19.223915Z",
     "shell.execute_reply": "2020-11-21T04:38:19.225147Z"
    },
    "papermill": {
     "duration": 0.205199,
     "end_time": "2020-11-21T04:38:19.225340",
     "exception": false,
     "start_time": "2020-11-21T04:38:19.020141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
    "\n",
    "class AdaBelief(Optimizer):\n",
    "    r\"\"\"Implements AdaBelief algorithm. Modified from Adam in PyTorch\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "        weight_decouple (boolean, optional): ( default: False) If set as True, then\n",
    "            the optimizer uses decoupled weight decay as in AdamW\n",
    "        fixed_decay (boolean, optional): (default: False) This is used when weight_decouple\n",
    "            is set as True.\n",
    "            When fixed_decay == True, the weight decay is performed as\n",
    "            $W_{new} = W_{old} - W_{old} \\times decay$.\n",
    "            When fixed_decay == False, the weight decay is performed as\n",
    "            $W_{new} = W_{old} - W_{old} \\times decay \\times lr$. Note that in this case, the\n",
    "            weight decay ratio decreases with learning rate (lr).\n",
    "        rectify (boolean, optional): (default: False) If set as True, then perform the rectified\n",
    "            update similar to RAdam\n",
    "    reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients\n",
    "               NeurIPS 2020 Spotlight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, weight_decouple = False, fixed_decay=False, rectify = False ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AdaBelief, self).__init__(params, defaults)\n",
    "\n",
    "        self.weight_decouple = weight_decouple\n",
    "        self.rectify = rectify\n",
    "        self.fixed_decay = fixed_decay\n",
    "        if self.weight_decouple:\n",
    "            print('Weight decoupling enabled in AdaBelief')\n",
    "            if self.fixed_decay:\n",
    "                print('Weight decay fixed')\n",
    "        if self.rectify:\n",
    "            print('Rectification enabled in AdaBelief')\n",
    "        if amsgrad:\n",
    "            print('AMS enabled in AdaBelief')\n",
    "    def __setstate__(self, state):\n",
    "        super(AdaBelief, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def reset(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                # State initialization\n",
    "                state['step'] = 0\n",
    "                # Exponential moving average of gradient values\n",
    "                state['exp_avg'] = torch.zeros_like(p.data,\n",
    "                                   memory_format=torch.preserve_format) if version_higher else torch.zeros_like(p.data)\n",
    "\n",
    "                # Exponential moving average of squared gradient values\n",
    "                state['exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if version_higher else torch.zeros_like(p.data)\n",
    "                if amsgrad:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    state['max_exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if version_higher else torch.zeros_like(p.data)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdaBelief does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "               \n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['rho_inf'] = 2.0 / (1.0 - beta2) - 1.0\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if version_higher else torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if version_higher else torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if version_higher else torch.zeros_like(p.data)\n",
    "\n",
    "                # get current state variable\n",
    "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                # perform weight decay, check if decoupled weight decay\n",
    "                if self.weight_decouple:\n",
    "                    if not self.fixed_decay:\n",
    "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
    "                    else:\n",
    "                        p.data.mul_(1.0 - group['weight_decay'])\n",
    "                else:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Update first and second moment running average\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                grad_residual = grad - exp_avg\n",
    "                exp_avg_var.mul_(beta2).addcmul_(1 - beta2, grad_residual, grad_residual)\n",
    "\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_var = state['max_exp_avg_var']\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
    "\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                else:\n",
    "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "\n",
    "                if not self.rectify:\n",
    "                    # Default update\n",
    "                    step_size = group['lr'] / bias_correction1\n",
    "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                else:# Rectified update\n",
    "                    # calculate rho_t\n",
    "                    state['rho_t'] = state['rho_inf'] - 2 * state['step'] * beta2 ** state['step'] / (\n",
    "                            1.0 - beta2 ** state['step'])\n",
    "\n",
    "                    if state['rho_t'] > 4: # perform Adam style update if variance is small\n",
    "                        rho_inf, rho_t = state['rho_inf'], state['rho_t']\n",
    "                        rt = (rho_t - 4.0) * (rho_t - 2.0) * rho_inf / (rho_inf - 4.0) / (rho_inf - 2.0) / rho_t\n",
    "                        rt = math.sqrt(rt)\n",
    "\n",
    "                        step_size = rt * group['lr'] / bias_correction1\n",
    "\n",
    "                        p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                    else: # perform SGD style update\n",
    "                        p.data.add_( -group['lr'], exp_avg)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.085353,
     "end_time": "2020-11-21T04:38:19.425240",
     "exception": false,
     "start_time": "2020-11-21T04:38:19.339887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2.2 Custom Loss for TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:19.649460Z",
     "iopub.status.busy": "2020-11-21T04:38:19.648419Z",
     "iopub.status.idle": "2020-11-21T04:38:19.652334Z",
     "shell.execute_reply": "2020-11-21T04:38:19.653608Z"
    },
    "papermill": {
     "duration": 0.141215,
     "end_time": "2020-11-21T04:38:19.653812",
     "exception": false,
     "start_time": "2020-11-21T04:38:19.512597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogitsLogLoss(Metric):\n",
    "    \"\"\"\n",
    "    LogLoss with sigmoid applied\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._name = \"logits_ll\"\n",
    "        self._maximize = False\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute LogLoss of predictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true: np.ndarray\n",
    "            Target matrix or vector\n",
    "        y_score: np.ndarray\n",
    "            Score matrix or vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            float\n",
    "            LogLoss of predictions vs targets.\n",
    "        \"\"\"\n",
    "        logits = 1 / (1 + np.exp(-y_pred))\n",
    "        aux = (1 - y_true) * np.log(1 - logits + 1e-15) + y_true * np.log(logits + 1e-15)\n",
    "        return np.mean(-aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.150067,
     "end_time": "2020-11-21T04:38:19.933659",
     "exception": false,
     "start_time": "2020-11-21T04:38:19.783592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2.3 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:20.193281Z",
     "iopub.status.busy": "2020-11-21T04:38:20.192250Z",
     "iopub.status.idle": "2020-11-21T04:38:20.199040Z",
     "shell.execute_reply": "2020-11-21T04:38:20.200296Z"
    },
    "papermill": {
     "duration": 0.143076,
     "end_time": "2020-11-21T04:38:20.200482",
     "exception": false,
     "start_time": "2020-11-21T04:38:20.057406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_tabnet(seed, fold, train_features, train_targets, train_ind, val_ind):\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold + 1}')\n",
    "                \n",
    "                \n",
    "    model.fit(X_train = train_features.values[train_ind], y_train = train_targets.values[train_ind], eval_set = [(train_features.values[val_ind], train_targets.values[val_ind])], \n",
    "            eval_name = [\"val\"], eval_metric = [\"logits_ll\"], max_epochs = MAX_EPOCH,\n",
    "            patience = 20, batch_size = 256, virtual_batch_size = 32, num_workers = 4, drop_last = False,\n",
    "#               from_unsupervised=pretrainer,\n",
    "            # To use binary cross entropy because this is not a regression problem\n",
    "            loss_fn = F.binary_cross_entropy_with_logits\n",
    "            )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.114907,
     "end_time": "2020-11-21T04:38:20.459295",
     "exception": false,
     "start_time": "2020-11-21T04:38:20.344388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2.5 Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:20.688347Z",
     "iopub.status.busy": "2020-11-21T04:38:20.687408Z",
     "iopub.status.idle": "2020-11-21T04:38:20.692214Z",
     "shell.execute_reply": "2020-11-21T04:38:20.693846Z"
    },
    "papermill": {
     "duration": 0.12552,
     "end_time": "2020-11-21T04:38:20.694068",
     "exception": false,
     "start_time": "2020-11-21T04:38:20.568548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_EPOCH = 200\n",
    "tabnet_params = dict(\n",
    "    n_d = 64,\n",
    "    n_a = 32,\n",
    "    n_steps = 1,\n",
    "    gamma = 1.3,\n",
    "    momentum = 0.05,\n",
    "    n_independent = 1,\n",
    "    n_shared = 1,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = AdaBelief,\n",
    "    optimizer_params = dict(lr = 6e-3, weight_decay = 1e-5, eps=1e-15),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 4, min_lr = 7e-6, factor = 0.25),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = 1,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:20.945376Z",
     "iopub.status.busy": "2020-11-21T04:38:20.944326Z",
     "iopub.status.idle": "2020-11-21T04:38:20.946490Z",
     "shell.execute_reply": "2020-11-21T04:38:20.947061Z"
    },
    "papermill": {
     "duration": 0.138078,
     "end_time": "2020-11-21T04:38:20.947209",
     "exception": false,
     "start_time": "2020-11-21T04:38:20.809131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "\n",
    "# BS=512\n",
    "# MAX_EPOCH=75\n",
    "\n",
    "# pretrainer = TabNetPretrainer(**tabnet_params)\n",
    "\n",
    "# pretrainer.fit(X_train=train_features.values, #  np.vstack([train.values[:,1:], test.values[:,1:]])\n",
    "#           max_epochs=MAX_EPOCH,\n",
    "#           patience=20, batch_size=BS, virtual_batch_size=128, #128,\n",
    "#           num_workers=0, drop_last=True,\n",
    "#           pretraining_ratio=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.088688,
     "end_time": "2020-11-21T04:38:21.128725",
     "exception": false,
     "start_time": "2020-11-21T04:38:21.040037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.3 KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "244e8f03-2225-496c-8215-c4487bfd8e61",
    "_uuid": "65dcb7f8-edf5-4544-a311-fa8202b8c793",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:21.327328Z",
     "iopub.status.busy": "2020-11-21T04:38:21.318900Z",
     "iopub.status.idle": "2020-11-21T04:38:21.330311Z",
     "shell.execute_reply": "2020-11-21T04:38:21.329707Z"
    },
    "papermill": {
     "duration": 0.111114,
     "end_time": "2020-11-21T04:38:21.330437",
     "exception": false,
     "start_time": "2020-11-21T04:38:21.219323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_folds(folds):\n",
    "    models = []\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        \n",
    "        seed = i\n",
    "        seed_everything(seed)\n",
    "        \n",
    "        model_list = list()\n",
    "        kfold = MultilabelStratifiedKFold(folds, shuffle = True, random_state = seed)\n",
    "        oof_preds = train_targets.copy()\n",
    "        \n",
    "        print(f\"REAPEAT NUMBER {i + 1} SEED {seed}\")\n",
    "        \n",
    "        for fold, (train_ind, val_ind) in enumerate(kfold.split(train_features, train_targets)):\n",
    "            \n",
    "            if i > 0:\n",
    "                print(\"TabNet\")\n",
    "                model = train_tabnet(seed, fold, train_features, train_targets, train_ind, val_ind)\n",
    "                model_list.append(model)\n",
    "                preds = model.predict(train_features.values[val_ind])\n",
    "                preds = 1 / (1 + np.exp(-preds))\n",
    "                oof_preds.iloc[val_ind, :] = clip(preds, p_min, p_max)\n",
    "            \n",
    "            if i == 0:\n",
    "                print(\"ANN\")\n",
    "                model = train_ann(seed, fold, train_features, train_targets, train_ind, val_ind, 846)\n",
    "                model_list.append(model)\n",
    "                oof_preds.iloc[val_ind, :] = clip(model.predict(train_features.values[val_ind]), p_min, p_max)\n",
    "\n",
    "        m, oof = model_list, oof_preds\n",
    "        models = models + m\n",
    "        \n",
    "        predictions.append(oof)\n",
    "    return models, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_cell_guid": "29fa0f27-87b8-4fce-bc79-f669825381fa",
    "_uuid": "237db3f7-035b-4c21-87a1-1dbc90f6176a",
    "execution": {
     "iopub.execute_input": "2020-11-21T04:38:21.511535Z",
     "iopub.status.busy": "2020-11-21T04:38:21.510592Z",
     "iopub.status.idle": "2020-11-21T05:49:02.306886Z",
     "shell.execute_reply": "2020-11-21T05:49:02.307668Z"
    },
    "papermill": {
     "duration": 4240.889798,
     "end_time": "2020-11-21T05:49:02.307856",
     "exception": false,
     "start_time": "2020-11-21T04:38:21.418058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=0 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAPEAT NUMBER 1 SEED 0\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02137, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.1652 - binary_crossentropy: 0.1636 - val_loss: 0.0240 - val_binary_crossentropy: 0.0214\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02137 to 0.01876, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0238 - binary_crossentropy: 0.0210 - val_loss: 0.0218 - val_binary_crossentropy: 0.0188\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01876 to 0.01779, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0223 - binary_crossentropy: 0.0192 - val_loss: 0.0209 - val_binary_crossentropy: 0.0178\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01779 to 0.01745, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0216 - binary_crossentropy: 0.0184 - val_loss: 0.0206 - val_binary_crossentropy: 0.0174\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01745 to 0.01707, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0177 - val_loss: 0.0202 - val_binary_crossentropy: 0.0171\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01707 to 0.01673, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 1s - loss: 0.0205 - binary_crossentropy: 0.0172 - val_loss: 0.0200 - val_binary_crossentropy: 0.0167\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01673 to 0.01664, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 1s - loss: 0.0202 - binary_crossentropy: 0.0169 - val_loss: 0.0199 - val_binary_crossentropy: 0.0166\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01664 to 0.01644, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 1s - loss: 0.0199 - binary_crossentropy: 0.0166 - val_loss: 0.0197 - val_binary_crossentropy: 0.0164\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01644 to 0.01626, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 1s - loss: 0.0196 - binary_crossentropy: 0.0162 - val_loss: 0.0196 - val_binary_crossentropy: 0.0163\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01626 to 0.01610, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0159 - val_loss: 0.0194 - val_binary_crossentropy: 0.0161\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01610 to 0.01606, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0157 - val_loss: 0.0194 - val_binary_crossentropy: 0.0161\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01606 to 0.01604, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 3s - loss: 0.0188 - binary_crossentropy: 0.0154 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01604 to 0.01599, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0187 - binary_crossentropy: 0.0152 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01599 to 0.01594, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0185 - binary_crossentropy: 0.0150 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01594 to 0.01584, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01584\n",
      "206/206 - 2s - loss: 0.0181 - binary_crossentropy: 0.0146 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy did not improve from 0.01584\n",
      "206/206 - 1s - loss: 0.0180 - binary_crossentropy: 0.0145 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy did not improve from 0.01584\n",
      "206/206 - 2s - loss: 0.0179 - binary_crossentropy: 0.0143 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01584 to 0.01566, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0171 - binary_crossentropy: 0.0135 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01566 to 0.01562, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 1s - loss: 0.0169 - binary_crossentropy: 0.0133 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy did not improve from 0.01562\n",
      "206/206 - 1s - loss: 0.0167 - binary_crossentropy: 0.0132 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy did not improve from 0.01562\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01562 to 0.01562, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.01562 to 0.01561, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy improved from 0.01561 to 0.01561, saving model to repeat:0_Fold:0.hdf5\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0190 - val_binary_crossentropy: 0.0157\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0157\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0190 - val_binary_crossentropy: 0.0157\n",
      "Epoch 35/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: val_binary_crossentropy did not improve from 0.01561\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0157\n",
      "Epoch 00035: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02096, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.1646 - binary_crossentropy: 0.1629 - val_loss: 0.0236 - val_binary_crossentropy: 0.0210\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02096 to 0.01906, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0238 - binary_crossentropy: 0.0209 - val_loss: 0.0221 - val_binary_crossentropy: 0.0191\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01906 to 0.01789, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0223 - binary_crossentropy: 0.0192 - val_loss: 0.0210 - val_binary_crossentropy: 0.0179\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01789 to 0.01730, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 1s - loss: 0.0215 - binary_crossentropy: 0.0183 - val_loss: 0.0205 - val_binary_crossentropy: 0.0173\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01730 to 0.01696, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0178 - val_loss: 0.0202 - val_binary_crossentropy: 0.0170\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01696 to 0.01674, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0207 - binary_crossentropy: 0.0174 - val_loss: 0.0200 - val_binary_crossentropy: 0.0167\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01674 to 0.01663, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0203 - binary_crossentropy: 0.0169 - val_loss: 0.0199 - val_binary_crossentropy: 0.0166\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01663 to 0.01645, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 1s - loss: 0.0199 - binary_crossentropy: 0.0165 - val_loss: 0.0198 - val_binary_crossentropy: 0.0165\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01645 to 0.01636, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 1s - loss: 0.0196 - binary_crossentropy: 0.0162 - val_loss: 0.0197 - val_binary_crossentropy: 0.0164\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01636 to 0.01619, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0194 - binary_crossentropy: 0.0160 - val_loss: 0.0195 - val_binary_crossentropy: 0.0162\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy did not improve from 0.01619\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0157 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01619 to 0.01608, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0189 - binary_crossentropy: 0.0155 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01608 to 0.01600, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0187 - binary_crossentropy: 0.0152 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01600 to 0.01591, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0185 - binary_crossentropy: 0.0151 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01591 to 0.01586, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0192 - val_binary_crossentropy: 0.0159\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01586\n",
      "206/206 - 2s - loss: 0.0181 - binary_crossentropy: 0.0146 - val_loss: 0.0195 - val_binary_crossentropy: 0.0160\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy did not improve from 0.01586\n",
      "206/206 - 1s - loss: 0.0180 - binary_crossentropy: 0.0145 - val_loss: 0.0194 - val_binary_crossentropy: 0.0159\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01586 to 0.01571, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0173 - binary_crossentropy: 0.0138 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01571 to 0.01569, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 1s - loss: 0.0170 - binary_crossentropy: 0.0135 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy did not improve from 0.01569\n",
      "206/206 - 2s - loss: 0.0170 - binary_crossentropy: 0.0134 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy did not improve from 0.01569\n",
      "206/206 - 1s - loss: 0.0169 - binary_crossentropy: 0.0133 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy improved from 0.01569 to 0.01567, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01567 to 0.01566, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.01566 to 0.01566, saving model to repeat:0_Fold:1.hdf5\n",
      "206/206 - 2s - loss: 0.0166 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 34/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0158\n",
      "Epoch 00034: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02131, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.1651 - binary_crossentropy: 0.1635 - val_loss: 0.0240 - val_binary_crossentropy: 0.0213\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02131 to 0.01889, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0236 - binary_crossentropy: 0.0208 - val_loss: 0.0219 - val_binary_crossentropy: 0.0189\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01889 to 0.01807, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0222 - binary_crossentropy: 0.0191 - val_loss: 0.0212 - val_binary_crossentropy: 0.0181\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01807 to 0.01769, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0214 - binary_crossentropy: 0.0183 - val_loss: 0.0209 - val_binary_crossentropy: 0.0177\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01769 to 0.01753, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0210 - binary_crossentropy: 0.0177 - val_loss: 0.0208 - val_binary_crossentropy: 0.0175\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01753 to 0.01698, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0205 - binary_crossentropy: 0.0172 - val_loss: 0.0203 - val_binary_crossentropy: 0.0170\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01698 to 0.01679, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0202 - binary_crossentropy: 0.0169 - val_loss: 0.0201 - val_binary_crossentropy: 0.0168\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01679 to 0.01665, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0198 - binary_crossentropy: 0.0165 - val_loss: 0.0200 - val_binary_crossentropy: 0.0166\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01665 to 0.01664, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0196 - binary_crossentropy: 0.0162 - val_loss: 0.0200 - val_binary_crossentropy: 0.0166\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01664 to 0.01645, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0159 - val_loss: 0.0198 - val_binary_crossentropy: 0.0165\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01645 to 0.01633, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0190 - binary_crossentropy: 0.0156 - val_loss: 0.0197 - val_binary_crossentropy: 0.0163\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01633 to 0.01627, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0188 - binary_crossentropy: 0.0154 - val_loss: 0.0197 - val_binary_crossentropy: 0.0163\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01627 to 0.01620, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0186 - binary_crossentropy: 0.0151 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01620 to 0.01617, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0184 - binary_crossentropy: 0.0150 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01617 to 0.01617, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy improved from 0.01617 to 0.01608, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0182 - binary_crossentropy: 0.0146 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy did not improve from 0.01608\n",
      "206/206 - 2s - loss: 0.0180 - binary_crossentropy: 0.0145 - val_loss: 0.0197 - val_binary_crossentropy: 0.0163\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01608 to 0.01603, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0178 - binary_crossentropy: 0.0142 - val_loss: 0.0195 - val_binary_crossentropy: 0.0160\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01603 to 0.01603, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0176 - binary_crossentropy: 0.0141 - val_loss: 0.0195 - val_binary_crossentropy: 0.0160\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01603 to 0.01586, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0169 - binary_crossentropy: 0.0134 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy improved from 0.01586 to 0.01582, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0167 - binary_crossentropy: 0.0131 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy did not improve from 0.01582\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0130 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy did not improve from 0.01582\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.01582 to 0.01582, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0162 - binary_crossentropy: 0.0126 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy improved from 0.01582 to 0.01582, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 2s - loss: 0.0162 - binary_crossentropy: 0.0126 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy improved from 0.01582 to 0.01580, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0162 - binary_crossentropy: 0.0126 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 1s - loss: 0.0161 - binary_crossentropy: 0.0125 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy improved from 0.01580 to 0.01580, saving model to repeat:0_Fold:2.hdf5\n",
      "206/206 - 1s - loss: 0.0161 - binary_crossentropy: 0.0125 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 1s - loss: 0.0161 - binary_crossentropy: 0.0125 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 1s - loss: 0.0161 - binary_crossentropy: 0.0125 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 2s - loss: 0.0161 - binary_crossentropy: 0.0126 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 2s - loss: 0.0161 - binary_crossentropy: 0.0126 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 1s - loss: 0.0161 - binary_crossentropy: 0.0126 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 2s - loss: 0.0161 - binary_crossentropy: 0.0126 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00035: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 2s - loss: 0.0161 - binary_crossentropy: 0.0126 - val_loss: 0.0192 - val_binary_crossentropy: 0.0159\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 1s - loss: 0.0161 - binary_crossentropy: 0.0127 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 1s - loss: 0.0162 - binary_crossentropy: 0.0127 - val_loss: 0.0192 - val_binary_crossentropy: 0.0159\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-08.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00038: val_binary_crossentropy did not improve from 0.01580\n",
      "206/206 - 2s - loss: 0.0162 - binary_crossentropy: 0.0127 - val_loss: 0.0192 - val_binary_crossentropy: 0.0159\n",
      "Epoch 00038: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02112, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.1648 - binary_crossentropy: 0.1631 - val_loss: 0.0238 - val_binary_crossentropy: 0.0211\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02112 to 0.01889, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 1s - loss: 0.0237 - binary_crossentropy: 0.0208 - val_loss: 0.0219 - val_binary_crossentropy: 0.0189\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01889 to 0.01793, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 1s - loss: 0.0221 - binary_crossentropy: 0.0191 - val_loss: 0.0210 - val_binary_crossentropy: 0.0179\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01793 to 0.01746, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0215 - binary_crossentropy: 0.0183 - val_loss: 0.0206 - val_binary_crossentropy: 0.0175\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01746 to 0.01709, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 1s - loss: 0.0210 - binary_crossentropy: 0.0178 - val_loss: 0.0203 - val_binary_crossentropy: 0.0171\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01709 to 0.01685, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 1s - loss: 0.0205 - binary_crossentropy: 0.0172 - val_loss: 0.0201 - val_binary_crossentropy: 0.0168\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01685 to 0.01668, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0202 - binary_crossentropy: 0.0169 - val_loss: 0.0200 - val_binary_crossentropy: 0.0167\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01668 to 0.01648, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0199 - binary_crossentropy: 0.0165 - val_loss: 0.0198 - val_binary_crossentropy: 0.0165\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01648 to 0.01642, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0196 - binary_crossentropy: 0.0163 - val_loss: 0.0198 - val_binary_crossentropy: 0.0164\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01642 to 0.01625, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0194 - binary_crossentropy: 0.0160 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01625 to 0.01614, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0157 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01614 to 0.01608, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0189 - binary_crossentropy: 0.0154 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01608 to 0.01599, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0188 - binary_crossentropy: 0.0153 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01599 to 0.01587, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0185 - binary_crossentropy: 0.0150 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy did not improve from 0.01587\n",
      "206/206 - 1s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01587\n",
      "206/206 - 1s - loss: 0.0181 - binary_crossentropy: 0.0146 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy did not improve from 0.01587\n",
      "206/206 - 2s - loss: 0.0180 - binary_crossentropy: 0.0145 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01587 to 0.01566, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0173 - binary_crossentropy: 0.0137 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01566 to 0.01562, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0171 - binary_crossentropy: 0.0135 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01562 to 0.01560, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0169 - binary_crossentropy: 0.0134 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy improved from 0.01560 to 0.01559, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy improved from 0.01559 to 0.01558, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01558 to 0.01557, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.01557 to 0.01557, saving model to repeat:0_Fold:3.hdf5\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0190 - val_binary_crossentropy: 0.0157\n",
      "Epoch 34/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0131 - val_loss: 0.0190 - val_binary_crossentropy: 0.0157\n",
      "Epoch 00034: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02125, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.1640 - binary_crossentropy: 0.1623 - val_loss: 0.0239 - val_binary_crossentropy: 0.0212\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02125 to 0.01885, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0236 - binary_crossentropy: 0.0207 - val_loss: 0.0219 - val_binary_crossentropy: 0.0188\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01885 to 0.01818, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0222 - binary_crossentropy: 0.0191 - val_loss: 0.0213 - val_binary_crossentropy: 0.0182\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01818 to 0.01770, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0215 - binary_crossentropy: 0.0183 - val_loss: 0.0209 - val_binary_crossentropy: 0.0177\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01770 to 0.01758, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0177 - val_loss: 0.0207 - val_binary_crossentropy: 0.0176\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01758 to 0.01719, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0207 - binary_crossentropy: 0.0174 - val_loss: 0.0205 - val_binary_crossentropy: 0.0172\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01719 to 0.01702, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0202 - binary_crossentropy: 0.0169 - val_loss: 0.0203 - val_binary_crossentropy: 0.0170\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01702 to 0.01674, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0200 - binary_crossentropy: 0.0166 - val_loss: 0.0201 - val_binary_crossentropy: 0.0167\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01674 to 0.01663, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0196 - binary_crossentropy: 0.0163 - val_loss: 0.0200 - val_binary_crossentropy: 0.0166\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy did not improve from 0.01663\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0159 - val_loss: 0.0200 - val_binary_crossentropy: 0.0166\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01663 to 0.01633, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0158 - val_loss: 0.0197 - val_binary_crossentropy: 0.0163\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01633 to 0.01628, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0189 - binary_crossentropy: 0.0154 - val_loss: 0.0197 - val_binary_crossentropy: 0.0163\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01628 to 0.01625, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0187 - binary_crossentropy: 0.0153 - val_loss: 0.0196 - val_binary_crossentropy: 0.0163\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01625 to 0.01621, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0186 - binary_crossentropy: 0.0151 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01621 to 0.01604, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0184 - binary_crossentropy: 0.0149 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01604\n",
      "206/206 - 2s - loss: 0.0182 - binary_crossentropy: 0.0147 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy improved from 0.01604 to 0.01596, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0181 - binary_crossentropy: 0.0145 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy did not improve from 0.01596\n",
      "206/206 - 1s - loss: 0.0180 - binary_crossentropy: 0.0144 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01596 to 0.01575, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0172 - binary_crossentropy: 0.0137 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01575 to 0.01571, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0170 - binary_crossentropy: 0.0134 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy did not improve from 0.01571\n",
      "206/206 - 1s - loss: 0.0169 - binary_crossentropy: 0.0133 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy improved from 0.01571 to 0.01570, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0167 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01570 to 0.01567, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.01567 to 0.01566, saving model to repeat:0_Fold:4.hdf5\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01566\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 00034: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 6\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02117, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.1662 - binary_crossentropy: 0.1645 - val_loss: 0.0239 - val_binary_crossentropy: 0.0212\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02117 to 0.01880, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0237 - binary_crossentropy: 0.0208 - val_loss: 0.0218 - val_binary_crossentropy: 0.0188\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01880 to 0.01783, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0223 - binary_crossentropy: 0.0192 - val_loss: 0.0210 - val_binary_crossentropy: 0.0178\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01783 to 0.01738, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0216 - binary_crossentropy: 0.0184 - val_loss: 0.0205 - val_binary_crossentropy: 0.0174\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01738 to 0.01705, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0178 - val_loss: 0.0202 - val_binary_crossentropy: 0.0171\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01705 to 0.01663, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0205 - binary_crossentropy: 0.0173 - val_loss: 0.0199 - val_binary_crossentropy: 0.0166\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01663 to 0.01649, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0203 - binary_crossentropy: 0.0170 - val_loss: 0.0198 - val_binary_crossentropy: 0.0165\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01649 to 0.01633, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0198 - binary_crossentropy: 0.0165 - val_loss: 0.0196 - val_binary_crossentropy: 0.0163\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01633 to 0.01623, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0196 - binary_crossentropy: 0.0162 - val_loss: 0.0195 - val_binary_crossentropy: 0.0162\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01623 to 0.01608, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0194 - binary_crossentropy: 0.0160 - val_loss: 0.0194 - val_binary_crossentropy: 0.0161\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01608 to 0.01601, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0156 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01601 to 0.01591, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0189 - binary_crossentropy: 0.0154 - val_loss: 0.0192 - val_binary_crossentropy: 0.0159\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy did not improve from 0.01591\n",
      "206/206 - 2s - loss: 0.0187 - binary_crossentropy: 0.0152 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01591 to 0.01590, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0185 - binary_crossentropy: 0.0150 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01590 to 0.01582, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy improved from 0.01582 to 0.01562, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0177 - binary_crossentropy: 0.0141 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy improved from 0.01562 to 0.01562, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0174 - binary_crossentropy: 0.0139 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01562 to 0.01554, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0173 - binary_crossentropy: 0.0137 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01554 to 0.01553, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0171 - binary_crossentropy: 0.0136 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01553 to 0.01552, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0169 - binary_crossentropy: 0.0134 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy improved from 0.01552 to 0.01552, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0169 - binary_crossentropy: 0.0134 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy improved from 0.01552 to 0.01552, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01552 to 0.01551, saving model to repeat:0_Fold:5.hdf5\n",
      "206/206 - 2s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0132 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0132 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0132 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 2s - loss: 0.0167 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 2s - loss: 0.0167 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0134 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 33/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01551\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 00033: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 7\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02142, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.1657 - binary_crossentropy: 0.1640 - val_loss: 0.0241 - val_binary_crossentropy: 0.0214\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02142 to 0.01870, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0238 - binary_crossentropy: 0.0209 - val_loss: 0.0217 - val_binary_crossentropy: 0.0187\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01870 to 0.01777, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0222 - binary_crossentropy: 0.0192 - val_loss: 0.0209 - val_binary_crossentropy: 0.0178\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01777 to 0.01735, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0215 - binary_crossentropy: 0.0183 - val_loss: 0.0206 - val_binary_crossentropy: 0.0174\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01735 to 0.01699, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0210 - binary_crossentropy: 0.0177 - val_loss: 0.0203 - val_binary_crossentropy: 0.0170\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01699 to 0.01677, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0206 - binary_crossentropy: 0.0173 - val_loss: 0.0200 - val_binary_crossentropy: 0.0168\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01677 to 0.01659, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0202 - binary_crossentropy: 0.0169 - val_loss: 0.0199 - val_binary_crossentropy: 0.0166\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01659 to 0.01641, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0199 - binary_crossentropy: 0.0166 - val_loss: 0.0197 - val_binary_crossentropy: 0.0164\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01641 to 0.01631, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0196 - binary_crossentropy: 0.0162 - val_loss: 0.0196 - val_binary_crossentropy: 0.0163\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01631 to 0.01607, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0159 - val_loss: 0.0194 - val_binary_crossentropy: 0.0161\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01607 to 0.01605, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0157 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01605 to 0.01590, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0189 - binary_crossentropy: 0.0155 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy did not improve from 0.01590\n",
      "206/206 - 1s - loss: 0.0186 - binary_crossentropy: 0.0152 - val_loss: 0.0193 - val_binary_crossentropy: 0.0160\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy did not improve from 0.01590\n",
      "206/206 - 2s - loss: 0.0185 - binary_crossentropy: 0.0151 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01590 to 0.01579, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01579\n",
      "206/206 - 1s - loss: 0.0182 - binary_crossentropy: 0.0147 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy improved from 0.01579 to 0.01575, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0180 - binary_crossentropy: 0.0145 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01575 to 0.01570, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0178 - binary_crossentropy: 0.0142 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01570 to 0.01552, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0171 - binary_crossentropy: 0.0136 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01552 to 0.01551, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0169 - binary_crossentropy: 0.0133 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy improved from 0.01551 to 0.01548, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 2s - loss: 0.0168 - binary_crossentropy: 0.0132 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy did not improve from 0.01548\n",
      "206/206 - 1s - loss: 0.0167 - binary_crossentropy: 0.0131 - val_loss: 0.0190 - val_binary_crossentropy: 0.0155\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01548 to 0.01547, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy improved from 0.01547 to 0.01547, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy improved from 0.01547 to 0.01546, saving model to repeat:0_Fold:6.hdf5\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0189 - val_binary_crossentropy: 0.0155\n",
      "Epoch 35/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00035: val_binary_crossentropy did not improve from 0.01546\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0129 - val_loss: 0.0189 - val_binary_crossentropy: 0.0156\n",
      "Epoch 00035: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 8\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02162, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.1651 - binary_crossentropy: 0.1634 - val_loss: 0.0242 - val_binary_crossentropy: 0.0216\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02162 to 0.01915, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0238 - binary_crossentropy: 0.0209 - val_loss: 0.0221 - val_binary_crossentropy: 0.0191\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01915 to 0.01806, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0222 - binary_crossentropy: 0.0191 - val_loss: 0.0211 - val_binary_crossentropy: 0.0181\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01806 to 0.01762, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0215 - binary_crossentropy: 0.0184 - val_loss: 0.0208 - val_binary_crossentropy: 0.0176\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01762 to 0.01707, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0177 - val_loss: 0.0203 - val_binary_crossentropy: 0.0171\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy did not improve from 0.01707\n",
      "206/206 - 2s - loss: 0.0205 - binary_crossentropy: 0.0173 - val_loss: 0.0207 - val_binary_crossentropy: 0.0175\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01707 to 0.01675, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0205 - binary_crossentropy: 0.0172 - val_loss: 0.0201 - val_binary_crossentropy: 0.0168\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01675 to 0.01648, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0198 - binary_crossentropy: 0.0165 - val_loss: 0.0198 - val_binary_crossentropy: 0.0165\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01648 to 0.01636, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0196 - binary_crossentropy: 0.0162 - val_loss: 0.0197 - val_binary_crossentropy: 0.0164\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01636 to 0.01630, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0159 - val_loss: 0.0196 - val_binary_crossentropy: 0.0163\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01630 to 0.01614, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0190 - binary_crossentropy: 0.0156 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy improved from 0.01614 to 0.01607, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0188 - binary_crossentropy: 0.0154 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01607 to 0.01596, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0187 - binary_crossentropy: 0.0152 - val_loss: 0.0193 - val_binary_crossentropy: 0.0160\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy did not improve from 0.01596\n",
      "206/206 - 2s - loss: 0.0184 - binary_crossentropy: 0.0150 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy did not improve from 0.01596\n",
      "206/206 - 1s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01596\n",
      "206/206 - 1s - loss: 0.0181 - binary_crossentropy: 0.0145 - val_loss: 0.0194 - val_binary_crossentropy: 0.0160\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy improved from 0.01596 to 0.01568, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0174 - binary_crossentropy: 0.0139 - val_loss: 0.0191 - val_binary_crossentropy: 0.0157\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01568 to 0.01563, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 2s - loss: 0.0172 - binary_crossentropy: 0.0136 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy did not improve from 0.01563\n",
      "206/206 - 1s - loss: 0.0170 - binary_crossentropy: 0.0135 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01563 to 0.01557, saving model to repeat:0_Fold:7.hdf5\n",
      "206/206 - 1s - loss: 0.0169 - binary_crossentropy: 0.0134 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0167 - binary_crossentropy: 0.0131 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 2s - loss: 0.0166 - binary_crossentropy: 0.0130 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 2s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0191 - val_binary_crossentropy: 0.0156\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0162 - binary_crossentropy: 0.0127 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0162 - binary_crossentropy: 0.0126 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0162 - binary_crossentropy: 0.0127 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 30/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01557\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0127 - val_loss: 0.0190 - val_binary_crossentropy: 0.0156\n",
      "Epoch 00030: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 9\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02123, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.1649 - binary_crossentropy: 0.1632 - val_loss: 0.0239 - val_binary_crossentropy: 0.0212\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02123 to 0.01862, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0235 - binary_crossentropy: 0.0207 - val_loss: 0.0216 - val_binary_crossentropy: 0.0186\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01862 to 0.01800, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0226 - binary_crossentropy: 0.0195 - val_loss: 0.0211 - val_binary_crossentropy: 0.0180\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01800 to 0.01735, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0215 - binary_crossentropy: 0.0183 - val_loss: 0.0205 - val_binary_crossentropy: 0.0173\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01735 to 0.01709, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0177 - val_loss: 0.0203 - val_binary_crossentropy: 0.0171\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy did not improve from 0.01709\n",
      "206/206 - 1s - loss: 0.0207 - binary_crossentropy: 0.0174 - val_loss: 0.0205 - val_binary_crossentropy: 0.0171\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01709 to 0.01666, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0202 - binary_crossentropy: 0.0169 - val_loss: 0.0200 - val_binary_crossentropy: 0.0167\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01666 to 0.01651, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0199 - binary_crossentropy: 0.0165 - val_loss: 0.0198 - val_binary_crossentropy: 0.0165\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01651 to 0.01641, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0195 - binary_crossentropy: 0.0162 - val_loss: 0.0198 - val_binary_crossentropy: 0.0164\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01641 to 0.01637, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0192 - binary_crossentropy: 0.0158 - val_loss: 0.0197 - val_binary_crossentropy: 0.0164\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy improved from 0.01637 to 0.01619, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0157 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy did not improve from 0.01619\n",
      "206/206 - 2s - loss: 0.0188 - binary_crossentropy: 0.0154 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy did not improve from 0.01619\n",
      "206/206 - 1s - loss: 0.0186 - binary_crossentropy: 0.0152 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy improved from 0.01619 to 0.01609, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0185 - binary_crossentropy: 0.0150 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01609 to 0.01608, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0183 - binary_crossentropy: 0.0148 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy did not improve from 0.01608\n",
      "206/206 - 1s - loss: 0.0182 - binary_crossentropy: 0.0147 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy did not improve from 0.01608\n",
      "206/206 - 1s - loss: 0.0179 - binary_crossentropy: 0.0144 - val_loss: 0.0196 - val_binary_crossentropy: 0.0161\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01608 to 0.01590, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0173 - binary_crossentropy: 0.0137 - val_loss: 0.0193 - val_binary_crossentropy: 0.0159\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01590 to 0.01585, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0170 - binary_crossentropy: 0.0134 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy improved from 0.01585 to 0.01583, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0169 - binary_crossentropy: 0.0134 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy improved from 0.01583 to 0.01581, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0168 - binary_crossentropy: 0.0132 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy improved from 0.01581 to 0.01580, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0130 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy improved from 0.01580 to 0.01579, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0130 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy did not improve from 0.01579\n",
      "206/206 - 1s - loss: 0.0165 - binary_crossentropy: 0.0129 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy improved from 0.01579 to 0.01579, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy improved from 0.01579 to 0.01578, saving model to repeat:0_Fold:8.hdf5\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0163 - binary_crossentropy: 0.0128 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0128 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "\n",
      "Epoch 00032: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 2s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.280000105907675e-07.\n",
      "\n",
      "Epoch 00035: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0129 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 36/200\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00036: val_binary_crossentropy did not improve from 0.01578\n",
      "206/206 - 1s - loss: 0.0164 - binary_crossentropy: 0.0130 - val_loss: 0.0192 - val_binary_crossentropy: 0.0159\n",
      "Epoch 00036: early stopping\n",
      "ANN\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 10\n",
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_binary_crossentropy improved from inf to 0.02124, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.1645 - binary_crossentropy: 0.1628 - val_loss: 0.0239 - val_binary_crossentropy: 0.0212\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_binary_crossentropy improved from 0.02124 to 0.01880, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0235 - binary_crossentropy: 0.0206 - val_loss: 0.0218 - val_binary_crossentropy: 0.0188\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_binary_crossentropy improved from 0.01880 to 0.01797, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0222 - binary_crossentropy: 0.0191 - val_loss: 0.0211 - val_binary_crossentropy: 0.0180\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_binary_crossentropy improved from 0.01797 to 0.01762, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0214 - binary_crossentropy: 0.0183 - val_loss: 0.0208 - val_binary_crossentropy: 0.0176\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_binary_crossentropy improved from 0.01762 to 0.01717, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0210 - binary_crossentropy: 0.0178 - val_loss: 0.0204 - val_binary_crossentropy: 0.0172\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_binary_crossentropy improved from 0.01717 to 0.01693, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 1s - loss: 0.0206 - binary_crossentropy: 0.0173 - val_loss: 0.0202 - val_binary_crossentropy: 0.0169\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_binary_crossentropy improved from 0.01693 to 0.01683, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 1s - loss: 0.0204 - binary_crossentropy: 0.0171 - val_loss: 0.0201 - val_binary_crossentropy: 0.0168\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_binary_crossentropy improved from 0.01683 to 0.01656, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0199 - binary_crossentropy: 0.0166 - val_loss: 0.0199 - val_binary_crossentropy: 0.0166\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_binary_crossentropy improved from 0.01656 to 0.01648, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0197 - binary_crossentropy: 0.0164 - val_loss: 0.0199 - val_binary_crossentropy: 0.0165\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_binary_crossentropy improved from 0.01648 to 0.01623, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0193 - binary_crossentropy: 0.0159 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_binary_crossentropy did not improve from 0.01623\n",
      "206/206 - 2s - loss: 0.0191 - binary_crossentropy: 0.0157 - val_loss: 0.0196 - val_binary_crossentropy: 0.0162\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_binary_crossentropy did not improve from 0.01623\n",
      "206/206 - 2s - loss: 0.0189 - binary_crossentropy: 0.0154 - val_loss: 0.0197 - val_binary_crossentropy: 0.0164\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_binary_crossentropy improved from 0.01623 to 0.01610, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0187 - binary_crossentropy: 0.0153 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_binary_crossentropy did not improve from 0.01610\n",
      "206/206 - 2s - loss: 0.0184 - binary_crossentropy: 0.0150 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_binary_crossentropy improved from 0.01610 to 0.01609, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0184 - binary_crossentropy: 0.0149 - val_loss: 0.0195 - val_binary_crossentropy: 0.0161\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0004000000189989805.\n",
      "\n",
      "Epoch 00016: val_binary_crossentropy improved from 0.01609 to 0.01604, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0181 - binary_crossentropy: 0.0146 - val_loss: 0.0195 - val_binary_crossentropy: 0.0160\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_binary_crossentropy improved from 0.01604 to 0.01582, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0174 - binary_crossentropy: 0.0139 - val_loss: 0.0193 - val_binary_crossentropy: 0.0158\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_binary_crossentropy improved from 0.01582 to 0.01577, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0173 - binary_crossentropy: 0.0137 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_binary_crossentropy improved from 0.01577 to 0.01574, saving model to repeat:0_Fold:9.hdf5\n",
      "206/206 - 2s - loss: 0.0171 - binary_crossentropy: 0.0135 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 8.000000379979611e-05.\n",
      "\n",
      "Epoch 00020: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 1s - loss: 0.0170 - binary_crossentropy: 0.0135 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 2s - loss: 0.0168 - binary_crossentropy: 0.0133 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 2s - loss: 0.0167 - binary_crossentropy: 0.0132 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.6000001050997525e-05.\n",
      "\n",
      "Epoch 00023: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 1s - loss: 0.0167 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 1s - loss: 0.0167 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0157\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 3.2000003557186575e-06.\n",
      "\n",
      "Epoch 00026: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 1s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 2s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-07.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00029: val_binary_crossentropy did not improve from 0.01574\n",
      "206/206 - 2s - loss: 0.0166 - binary_crossentropy: 0.0131 - val_loss: 0.0192 - val_binary_crossentropy: 0.0158\n",
      "Epoch 00029: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAPEAT NUMBER 2 SEED 1\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:135: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.15344 | val_logits_ll: 0.02226 |  0:00:03s\n",
      "epoch 10 | loss: 0.01743 | val_logits_ll: 0.0176  |  0:00:35s\n",
      "epoch 20 | loss: 0.01667 | val_logits_ll: 0.01706 |  0:01:06s\n",
      "epoch 30 | loss: 0.01621 | val_logits_ll: 0.01687 |  0:01:38s\n",
      "epoch 40 | loss: 0.01469 | val_logits_ll: 0.0161  |  0:02:10s\n",
      "epoch 50 | loss: 0.01296 | val_logits_ll: 0.01656 |  0:02:41s\n",
      "epoch 60 | loss: 0.01232 | val_logits_ll: 0.01676 |  0:03:11s\n",
      "\n",
      "Early stopping occured at epoch 60 with best_epoch = 40 and best_val_logits_ll = 0.0161\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\n",
      "epoch 0  | loss: 0.15379 | val_logits_ll: 0.02231 |  0:00:03s\n",
      "epoch 10 | loss: 0.0176  | val_logits_ll: 0.01753 |  0:00:34s\n",
      "epoch 20 | loss: 0.01659 | val_logits_ll: 0.01666 |  0:01:05s\n",
      "epoch 30 | loss: 0.01492 | val_logits_ll: 0.01622 |  0:01:36s\n",
      "epoch 40 | loss: 0.01316 | val_logits_ll: 0.01644 |  0:02:07s\n",
      "epoch 50 | loss: 0.0129  | val_logits_ll: 0.01649 |  0:02:39s\n",
      "\n",
      "Early stopping occured at epoch 52 with best_epoch = 32 and best_val_logits_ll = 0.01612\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\n",
      "epoch 0  | loss: 0.15357 | val_logits_ll: 0.02234 |  0:00:02s\n",
      "epoch 10 | loss: 0.01744 | val_logits_ll: 0.01779 |  0:00:35s\n",
      "epoch 20 | loss: 0.01667 | val_logits_ll: 0.01736 |  0:01:06s\n",
      "epoch 30 | loss: 0.01613 | val_logits_ll: 0.01678 |  0:01:37s\n",
      "epoch 40 | loss: 0.01519 | val_logits_ll: 0.01617 |  0:02:08s\n",
      "epoch 50 | loss: 0.01321 | val_logits_ll: 0.0165  |  0:02:40s\n",
      "epoch 60 | loss: 0.01258 | val_logits_ll: 0.01671 |  0:03:10s\n",
      "\n",
      "Early stopping occured at epoch 60 with best_epoch = 40 and best_val_logits_ll = 0.01617\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\n",
      "epoch 0  | loss: 0.15362 | val_logits_ll: 0.02248 |  0:00:03s\n",
      "epoch 10 | loss: 0.01734 | val_logits_ll: 0.01777 |  0:00:35s\n",
      "epoch 20 | loss: 0.01665 | val_logits_ll: 0.01744 |  0:01:05s\n",
      "epoch 30 | loss: 0.01624 | val_logits_ll: 0.01686 |  0:01:37s\n",
      "epoch 40 | loss: 0.01592 | val_logits_ll: 0.01666 |  0:02:09s\n",
      "epoch 50 | loss: 0.01355 | val_logits_ll: 0.0164  |  0:02:40s\n",
      "epoch 60 | loss: 0.01261 | val_logits_ll: 0.01669 |  0:03:12s\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 45 and best_val_logits_ll = 0.0162\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\n",
      "epoch 0  | loss: 0.15362 | val_logits_ll: 0.02236 |  0:00:03s\n",
      "epoch 10 | loss: 0.0175  | val_logits_ll: 0.01742 |  0:00:34s\n",
      "epoch 20 | loss: 0.01697 | val_logits_ll: 0.01735 |  0:01:07s\n",
      "epoch 30 | loss: 0.01628 | val_logits_ll: 0.01675 |  0:01:38s\n",
      "epoch 40 | loss: 0.01387 | val_logits_ll: 0.01643 |  0:02:09s\n",
      "epoch 50 | loss: 0.01307 | val_logits_ll: 0.01667 |  0:02:41s\n",
      "\n",
      "Early stopping occured at epoch 52 with best_epoch = 32 and best_val_logits_ll = 0.01611\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 6\n",
      "epoch 0  | loss: 0.15428 | val_logits_ll: 0.02257 |  0:00:03s\n",
      "epoch 10 | loss: 0.01747 | val_logits_ll: 0.01748 |  0:00:33s\n",
      "epoch 20 | loss: 0.01664 | val_logits_ll: 0.01682 |  0:01:05s\n",
      "epoch 30 | loss: 0.01624 | val_logits_ll: 0.0167  |  0:01:36s\n",
      "epoch 40 | loss: 0.01353 | val_logits_ll: 0.0164  |  0:02:07s\n",
      "epoch 50 | loss: 0.01267 | val_logits_ll: 0.01659 |  0:02:39s\n",
      "\n",
      "Early stopping occured at epoch 52 with best_epoch = 32 and best_val_logits_ll = 0.01611\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 7\n",
      "epoch 0  | loss: 0.15323 | val_logits_ll: 0.02243 |  0:00:03s\n",
      "epoch 10 | loss: 0.01746 | val_logits_ll: 0.01746 |  0:00:35s\n",
      "epoch 20 | loss: 0.01678 | val_logits_ll: 0.01709 |  0:01:07s\n",
      "epoch 30 | loss: 0.0143  | val_logits_ll: 0.01655 |  0:01:38s\n",
      "epoch 40 | loss: 0.01343 | val_logits_ll: 0.01685 |  0:02:11s\n",
      "\n",
      "Early stopping occured at epoch 42 with best_epoch = 22 and best_val_logits_ll = 0.0163\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 8\n",
      "epoch 0  | loss: 0.15388 | val_logits_ll: 0.02265 |  0:00:02s\n",
      "epoch 10 | loss: 0.01748 | val_logits_ll: 0.01766 |  0:00:34s\n",
      "epoch 20 | loss: 0.01661 | val_logits_ll: 0.01705 |  0:01:07s\n",
      "epoch 30 | loss: 0.01625 | val_logits_ll: 0.01694 |  0:01:37s\n",
      "epoch 40 | loss: 0.014   | val_logits_ll: 0.01653 |  0:02:09s\n",
      "epoch 50 | loss: 0.01285 | val_logits_ll: 0.01676 |  0:02:40s\n",
      "\n",
      "Early stopping occured at epoch 54 with best_epoch = 34 and best_val_logits_ll = 0.01641\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 9\n",
      "epoch 0  | loss: 0.15312 | val_logits_ll: 0.02206 |  0:00:02s\n",
      "epoch 10 | loss: 0.01738 | val_logits_ll: 0.01779 |  0:00:33s\n",
      "epoch 20 | loss: 0.01653 | val_logits_ll: 0.01724 |  0:01:04s\n",
      "epoch 30 | loss: 0.01405 | val_logits_ll: 0.01664 |  0:01:35s\n",
      "epoch 40 | loss: 0.01306 | val_logits_ll: 0.01687 |  0:02:07s\n",
      "\n",
      "Early stopping occured at epoch 43 with best_epoch = 23 and best_val_logits_ll = 0.01636\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 10\n",
      "epoch 0  | loss: 0.15401 | val_logits_ll: 0.0225  |  0:00:02s\n",
      "epoch 10 | loss: 0.01759 | val_logits_ll: 0.01797 |  0:00:34s\n",
      "epoch 20 | loss: 0.01698 | val_logits_ll: 0.01718 |  0:01:04s\n",
      "epoch 30 | loss: 0.01641 | val_logits_ll: 0.01674 |  0:01:35s\n",
      "epoch 40 | loss: 0.01607 | val_logits_ll: 0.01673 |  0:02:05s\n",
      "epoch 50 | loss: 0.01439 | val_logits_ll: 0.01652 |  0:02:38s\n",
      "epoch 60 | loss: 0.0129  | val_logits_ll: 0.01661 |  0:03:09s\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 45 and best_val_logits_ll = 0.01614\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass shuffle=True, random_state=2 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAPEAT NUMBER 3 SEED 2\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 1\n",
      "epoch 0  | loss: 0.15392 | val_logits_ll: 0.0225  |  0:00:03s\n",
      "epoch 10 | loss: 0.01752 | val_logits_ll: 0.01772 |  0:00:34s\n",
      "epoch 20 | loss: 0.01667 | val_logits_ll: 0.01677 |  0:01:06s\n",
      "epoch 30 | loss: 0.01519 | val_logits_ll: 0.01606 |  0:01:36s\n",
      "epoch 40 | loss: 0.01335 | val_logits_ll: 0.01632 |  0:02:08s\n",
      "epoch 50 | loss: 0.01254 | val_logits_ll: 0.01655 |  0:02:40s\n",
      "\n",
      "Early stopping occured at epoch 51 with best_epoch = 31 and best_val_logits_ll = 0.01599\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 2\n",
      "epoch 0  | loss: 0.15323 | val_logits_ll: 0.02216 |  0:00:02s\n",
      "epoch 10 | loss: 0.01745 | val_logits_ll: 0.01766 |  0:00:34s\n",
      "epoch 20 | loss: 0.0166  | val_logits_ll: 0.01733 |  0:01:05s\n",
      "epoch 30 | loss: 0.01621 | val_logits_ll: 0.01688 |  0:01:36s\n",
      "epoch 40 | loss: 0.01594 | val_logits_ll: 0.0164  |  0:02:06s\n",
      "epoch 50 | loss: 0.01454 | val_logits_ll: 0.01607 |  0:02:38s\n",
      "epoch 60 | loss: 0.01296 | val_logits_ll: 0.01626 |  0:03:08s\n",
      "\n",
      "Early stopping occured at epoch 68 with best_epoch = 48 and best_val_logits_ll = 0.01597\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 3\n",
      "epoch 0  | loss: 0.15338 | val_logits_ll: 0.02269 |  0:00:02s\n",
      "epoch 10 | loss: 0.01749 | val_logits_ll: 0.01859 |  0:00:34s\n",
      "epoch 20 | loss: 0.01659 | val_logits_ll: 0.01714 |  0:01:05s\n",
      "epoch 30 | loss: 0.01491 | val_logits_ll: 0.01684 |  0:01:36s\n",
      "epoch 40 | loss: 0.01302 | val_logits_ll: 0.017   |  0:02:07s\n",
      "\n",
      "Early stopping occured at epoch 47 with best_epoch = 27 and best_val_logits_ll = 0.01656\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 4\n",
      "epoch 0  | loss: 0.15277 | val_logits_ll: 0.02223 |  0:00:03s\n",
      "epoch 10 | loss: 0.01745 | val_logits_ll: 0.01769 |  0:00:34s\n",
      "epoch 20 | loss: 0.01663 | val_logits_ll: 0.01677 |  0:01:06s\n",
      "epoch 30 | loss: 0.01637 | val_logits_ll: 0.01666 |  0:01:38s\n",
      "epoch 40 | loss: 0.01598 | val_logits_ll: 0.01662 |  0:02:11s\n",
      "epoch 50 | loss: 0.01449 | val_logits_ll: 0.0163  |  0:02:41s\n",
      "epoch 60 | loss: 0.01298 | val_logits_ll: 0.01654 |  0:03:13s\n",
      "\n",
      "Early stopping occured at epoch 67 with best_epoch = 47 and best_val_logits_ll = 0.01613\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 5\n",
      "epoch 0  | loss: 0.15453 | val_logits_ll: 0.02239 |  0:00:02s\n",
      "epoch 10 | loss: 0.01738 | val_logits_ll: 0.01748 |  0:00:34s\n",
      "epoch 20 | loss: 0.01668 | val_logits_ll: 0.01684 |  0:01:05s\n",
      "epoch 30 | loss: 0.01629 | val_logits_ll: 0.01665 |  0:01:38s\n",
      "epoch 40 | loss: 0.01598 | val_logits_ll: 0.01643 |  0:02:09s\n",
      "epoch 50 | loss: 0.01415 | val_logits_ll: 0.01643 |  0:02:41s\n",
      "epoch 60 | loss: 0.01259 | val_logits_ll: 0.01661 |  0:03:12s\n",
      "\n",
      "Early stopping occured at epoch 65 with best_epoch = 45 and best_val_logits_ll = 0.01604\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 6\n",
      "epoch 0  | loss: 0.15327 | val_logits_ll: 0.02196 |  0:00:03s\n",
      "epoch 10 | loss: 0.01739 | val_logits_ll: 0.01778 |  0:00:33s\n",
      "epoch 20 | loss: 0.01657 | val_logits_ll: 0.01715 |  0:01:06s\n",
      "epoch 30 | loss: 0.01637 | val_logits_ll: 0.01685 |  0:01:37s\n",
      "epoch 40 | loss: 0.01483 | val_logits_ll: 0.01642 |  0:02:09s\n",
      "epoch 50 | loss: 0.0133  | val_logits_ll: 0.01665 |  0:02:40s\n",
      "\n",
      "Early stopping occured at epoch 57 with best_epoch = 37 and best_val_logits_ll = 0.01627\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 7\n",
      "epoch 0  | loss: 0.1544  | val_logits_ll: 0.02304 |  0:00:02s\n",
      "epoch 10 | loss: 0.0175  | val_logits_ll: 0.01843 |  0:00:34s\n",
      "epoch 20 | loss: 0.01656 | val_logits_ll: 0.01718 |  0:01:06s\n",
      "epoch 30 | loss: 0.01493 | val_logits_ll: 0.01623 |  0:01:36s\n",
      "epoch 40 | loss: 0.01307 | val_logits_ll: 0.01664 |  0:02:08s\n",
      "\n",
      "Early stopping occured at epoch 49 with best_epoch = 29 and best_val_logits_ll = 0.0162\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 8\n",
      "epoch 0  | loss: 0.15391 | val_logits_ll: 0.02252 |  0:00:02s\n",
      "epoch 10 | loss: 0.01744 | val_logits_ll: 0.01739 |  0:00:34s\n",
      "epoch 20 | loss: 0.01657 | val_logits_ll: 0.01716 |  0:01:04s\n",
      "epoch 30 | loss: 0.01629 | val_logits_ll: 0.01676 |  0:01:36s\n",
      "epoch 40 | loss: 0.01453 | val_logits_ll: 0.01635 |  0:02:07s\n",
      "epoch 50 | loss: 0.01298 | val_logits_ll: 0.01658 |  0:02:39s\n",
      "\n",
      "Early stopping occured at epoch 55 with best_epoch = 35 and best_val_logits_ll = 0.01619\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 9\n",
      "epoch 0  | loss: 0.15354 | val_logits_ll: 0.02222 |  0:00:02s\n",
      "epoch 10 | loss: 0.01734 | val_logits_ll: 0.0177  |  0:00:35s\n",
      "epoch 20 | loss: 0.01657 | val_logits_ll: 0.01798 |  0:01:05s\n",
      "epoch 30 | loss: 0.01425 | val_logits_ll: 0.01664 |  0:01:36s\n",
      "epoch 40 | loss: 0.01324 | val_logits_ll: 0.01688 |  0:02:06s\n",
      "\n",
      "Early stopping occured at epoch 43 with best_epoch = 23 and best_val_logits_ll = 0.01647\n",
      "Best weights from best epoch are automatically used!\n",
      "TabNet\n",
      "Device used : cuda\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Training fold 10\n",
      "epoch 0  | loss: 0.15406 | val_logits_ll: 0.02241 |  0:00:03s\n",
      "epoch 10 | loss: 0.01755 | val_logits_ll: 0.01746 |  0:00:33s\n",
      "epoch 20 | loss: 0.01674 | val_logits_ll: 0.01684 |  0:01:06s\n",
      "epoch 30 | loss: 0.01633 | val_logits_ll: 0.01697 |  0:01:37s\n",
      "epoch 40 | loss: 0.01616 | val_logits_ll: 0.01651 |  0:02:10s\n",
      "epoch 50 | loss: 0.01584 | val_logits_ll: 0.01632 |  0:02:41s\n",
      "epoch 60 | loss: 0.01387 | val_logits_ll: 0.01589 |  0:03:14s\n",
      "epoch 70 | loss: 0.01301 | val_logits_ll: 0.01617 |  0:03:45s\n",
      "\n",
      "Early stopping occured at epoch 74 with best_epoch = 54 and best_val_logits_ll = 0.01575\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "models, oof_preds = make_folds(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "b7d35a42-c682-49e2-a0c6-b33a528442df",
    "_uuid": "9a8b91ee-8644-4072-b810-1c4d51913302",
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:03.255129Z",
     "iopub.status.busy": "2020-11-21T05:49:03.253765Z",
     "iopub.status.idle": "2020-11-21T05:49:08.841517Z",
     "shell.execute_reply": "2020-11-21T05:49:08.842231Z"
    },
    "papermill": {
     "duration": 6.064814,
     "end_time": "2020-11-21T05:49:08.842440",
     "exception": false,
     "start_time": "2020-11-21T05:49:02.777626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1 OOF Log Loss: 0.01585272294425286\n",
      "Repeat 2 OOF Log Loss: 0.016394556938144025\n",
      "Repeat 3 OOF Log Loss: 0.01635482369218773\n",
      "Mean OOF Log Loss: 0.015753509530521195\n"
     ]
    }
   ],
   "source": [
    "mean_oof_preds = train_targets.copy()\n",
    "mean_oof_preds.loc[:, target_cols] = 0\n",
    "scores = []\n",
    "for i, p in enumerate(oof_preds):\n",
    "    loss = multi_log_loss(train_targets, p)\n",
    "    print(f\"Repeat {i + 1} OOF Log Loss: {loss}\")\n",
    "    scores.append(loss)\n",
    "    mean_oof_preds.loc[:,target_cols] += p\n",
    "mean_oof_preds.loc[:, target_cols] /= len(oof_preds)\n",
    "print(f\"Mean OOF Log Loss: {multi_log_loss(train_targets, mean_oof_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.419785,
     "end_time": "2020-11-21T05:49:09.730055",
     "exception": false,
     "start_time": "2020-11-21T05:49:09.310270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.4 Export Predictions For Optimizing Blending Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:10.580825Z",
     "iopub.status.busy": "2020-11-21T05:49:10.580122Z",
     "iopub.status.idle": "2020-11-21T05:49:47.744212Z",
     "shell.execute_reply": "2020-11-21T05:49:47.743164Z"
    },
    "papermill": {
     "duration": 37.588346,
     "end_time": "2020-11-21T05:49:47.744354",
     "exception": false,
     "start_time": "2020-11-21T05:49:10.156008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, p in enumerate(oof_preds):\n",
    "    p.to_csv(f\"pred_{i}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.431376,
     "end_time": "2020-11-21T05:49:48.606663",
     "exception": false,
     "start_time": "2020-11-21T05:49:48.175287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4.5 Bleding Using Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:49.459181Z",
     "iopub.status.busy": "2020-11-21T05:49:49.458337Z",
     "iopub.status.idle": "2020-11-21T05:49:49.463421Z",
     "shell.execute_reply": "2020-11-21T05:49:49.462816Z"
    },
    "papermill": {
     "duration": 0.429653,
     "end_time": "2020-11-21T05:49:49.463535",
     "exception": false,
     "start_time": "2020-11-21T05:49:49.033882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = np.zeros((len(oof_preds), train_targets.shape[0], train_targets.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:50.619175Z",
     "iopub.status.busy": "2020-11-21T05:49:50.617933Z",
     "iopub.status.idle": "2020-11-21T05:49:50.708417Z",
     "shell.execute_reply": "2020-11-21T05:49:50.707757Z"
    },
    "papermill": {
     "duration": 0.578449,
     "end_time": "2020-11-21T05:49:50.708551",
     "exception": false,
     "start_time": "2020-11-21T05:49:50.130102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(oof_preds)):\n",
    "    oof[i] = oof_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:51.590134Z",
     "iopub.status.busy": "2020-11-21T05:49:51.589278Z",
     "iopub.status.idle": "2020-11-21T05:49:51.594779Z",
     "shell.execute_reply": "2020-11-21T05:49:51.594145Z"
    },
    "papermill": {
     "duration": 0.465771,
     "end_time": "2020-11-21T05:49:51.594914",
     "exception": false,
     "start_time": "2020-11-21T05:49:51.129143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blend_preds(weights):\n",
    "    oof_blend = np.tensordot(weights, oof, axes = ((0), (0)))\n",
    "    return oof_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:52.486465Z",
     "iopub.status.busy": "2020-11-21T05:49:52.484416Z",
     "iopub.status.idle": "2020-11-21T05:49:52.487182Z",
     "shell.execute_reply": "2020-11-21T05:49:52.487688Z"
    },
    "papermill": {
     "duration": 0.434134,
     "end_time": "2020-11-21T05:49:52.487826",
     "exception": false,
     "start_time": "2020-11-21T05:49:52.053692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_loss_numpy(y_pred):\n",
    "    y_true_ravel = np.asarray(train_targets).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    loss = np.where(y_true_ravel == 1, - np.log(y_pred), - np.log(1 - y_pred))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:53.743290Z",
     "iopub.status.busy": "2020-11-21T05:49:53.741371Z",
     "iopub.status.idle": "2020-11-21T05:49:54.050615Z",
     "shell.execute_reply": "2020-11-21T05:49:54.050012Z"
    },
    "papermill": {
     "duration": 0.928725,
     "end_time": "2020-11-21T05:49:54.050739",
     "exception": false,
     "start_time": "2020-11-21T05:49:53.122014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Blend Loss: 0.01579669752677265\n"
     ]
    }
   ],
   "source": [
    "oof_blend = blend_preds([0.26, 0.39, 0.35])\n",
    "print(f\"Optimized Blend Loss: {log_loss_numpy(oof_blend)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.475142,
     "end_time": "2020-11-21T05:49:55.009931",
     "exception": false,
     "start_time": "2020-11-21T05:49:54.534789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:55.889901Z",
     "iopub.status.busy": "2020-11-21T05:49:55.888737Z",
     "iopub.status.idle": "2020-11-21T05:49:56.812251Z",
     "shell.execute_reply": "2020-11-21T05:49:56.811621Z"
    },
    "papermill": {
     "duration": 1.3602,
     "end_time": "2020-11-21T05:49:56.812384",
     "exception": false,
     "start_time": "2020-11-21T05:49:55.452184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_ = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sig_id = test_[test_[\"cp_type\"] != \"ctl_vehicle\"].sig_id.reset_index(drop = True)\n",
    "test_preds = sub.copy()\n",
    "test_preds[target_cols] = 0\n",
    "test_preds = test_preds.drop(\"sig_id\", axis = 1)\n",
    "test_preds[\"sig_id\"] = sig_id\n",
    "test_preds.dropna(inplace=True)\n",
    "test_preds.drop(\"sig_id\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:58.064685Z",
     "iopub.status.busy": "2020-11-21T05:49:58.063311Z",
     "iopub.status.idle": "2020-11-21T05:49:58.095079Z",
     "shell.execute_reply": "2020-11-21T05:49:58.096084Z"
    },
    "papermill": {
     "duration": 0.735477,
     "end_time": "2020-11-21T05:49:58.096296",
     "exception": false,
     "start_time": "2020-11-21T05:49:57.360819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof = np.zeros((len(oof_preds), test_preds.shape[0], test_preds.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:49:59.417721Z",
     "iopub.status.busy": "2020-11-21T05:49:59.416267Z",
     "iopub.status.idle": "2020-11-21T05:49:59.421960Z",
     "shell.execute_reply": "2020-11-21T05:49:59.422474Z"
    },
    "papermill": {
     "duration": 0.499117,
     "end_time": "2020-11-21T05:49:59.422644",
     "exception": false,
     "start_time": "2020-11-21T05:49:58.923527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred1 = test_preds.copy()\n",
    "test_pred2 = test_preds.copy()\n",
    "test_pred3 = test_preds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "_cell_guid": "85c6e9de-4e94-47d5-a573-0d76d45adbb8",
    "_uuid": "d9b8369d-3130-4cfd-a750-41f043108434",
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:00.345406Z",
     "iopub.status.busy": "2020-11-21T05:50:00.343442Z",
     "iopub.status.idle": "2020-11-21T05:50:09.500262Z",
     "shell.execute_reply": "2020-11-21T05:50:09.499565Z"
    },
    "papermill": {
     "duration": 9.65469,
     "end_time": "2020-11-21T05:50:09.500426",
     "exception": false,
     "start_time": "2020-11-21T05:49:59.845736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    if (i + 1) <= 10:\n",
    "        preds = model.predict(test_features.values)\n",
    "        test_pred1.loc[:,target_cols] += clip(preds, p_min, p_max)\n",
    "    if (i + 1) <= 20 and (i + 1) > 10:\n",
    "        preds = model.predict(test_features.values)\n",
    "        preds = 1 / (1 + np.exp(-preds))\n",
    "        test_pred2.loc[:,target_cols] += clip(preds, p_min, p_max)\n",
    "    if (i + 1) > 20:\n",
    "        preds = model.predict(test_features.values)\n",
    "        preds = 1 / (1 + np.exp(-preds))\n",
    "        test_pred3.loc[:,target_cols] += clip(preds, p_min, p_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:10.453242Z",
     "iopub.status.busy": "2020-11-21T05:50:10.451956Z",
     "iopub.status.idle": "2020-11-21T05:50:10.467806Z",
     "shell.execute_reply": "2020-11-21T05:50:10.467179Z"
    },
    "papermill": {
     "duration": 0.446177,
     "end_time": "2020-11-21T05:50:10.467928",
     "exception": false,
     "start_time": "2020-11-21T05:50:10.021751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred1.loc[:, target_cols] /= 10\n",
    "test_pred2.loc[:, target_cols] /= 10\n",
    "test_pred3.loc[:, target_cols] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:11.330999Z",
     "iopub.status.busy": "2020-11-21T05:50:11.329609Z",
     "iopub.status.idle": "2020-11-21T05:50:11.342360Z",
     "shell.execute_reply": "2020-11-21T05:50:11.341744Z"
    },
    "papermill": {
     "duration": 0.448839,
     "end_time": "2020-11-21T05:50:11.342494",
     "exception": false,
     "start_time": "2020-11-21T05:50:10.893655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof_preds = [test_pred1, test_pred2, test_pred3]\n",
    "\n",
    "for i in range(len(oof_preds)):\n",
    "    oof[i] = oof_preds[i]\n",
    "    \n",
    "test_blend_preds = blend_preds([0.26, 0.39, 0.35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:12.459712Z",
     "iopub.status.busy": "2020-11-21T05:50:12.458743Z",
     "iopub.status.idle": "2020-11-21T05:50:12.893896Z",
     "shell.execute_reply": "2020-11-21T05:50:12.892718Z"
    },
    "papermill": {
     "duration": 0.999489,
     "end_time": "2020-11-21T05:50:12.894065",
     "exception": false,
     "start_time": "2020-11-21T05:50:11.894576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds[target_cols] = test_blend_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:13.754307Z",
     "iopub.status.busy": "2020-11-21T05:50:13.752922Z",
     "iopub.status.idle": "2020-11-21T05:50:16.620218Z",
     "shell.execute_reply": "2020-11-21T05:50:16.619125Z"
    },
    "papermill": {
     "duration": 3.303263,
     "end_time": "2020-11-21T05:50:16.620401",
     "exception": false,
     "start_time": "2020-11-21T05:50:13.317138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds[\"sig_id\"] = sig_id\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "submission = pd.merge(test_features[[\"sig_id\"]], test_preds, on = \"sig_id\", how = \"left\")\n",
    "submission.fillna(0, inplace = True)\n",
    "sub = submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:17.486520Z",
     "iopub.status.busy": "2020-11-21T05:50:17.485458Z",
     "iopub.status.idle": "2020-11-21T05:50:17.524743Z",
     "shell.execute_reply": "2020-11-21T05:50:17.525370Z"
    },
    "papermill": {
     "duration": 0.472909,
     "end_time": "2020-11-21T05:50:17.525531",
     "exception": false,
     "start_time": "2020-11-21T05:50:17.052622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.003061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.024072</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.001951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>0.024744</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.001579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>id_ff7004b87</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.298259</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3978</th>\n",
       "      <td>id_ff925dd0d</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3979</th>\n",
       "      <td>id_ffb710450</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3980</th>\n",
       "      <td>id_ffbb869f2</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.006315</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.003379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3981</th>\n",
       "      <td>id_ffd5800b6</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.017771</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3982 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0     id_0004d9e33                     0.001155                0.001297   \n",
       "1     id_001897cda                     0.001000                0.001098   \n",
       "2     id_002429b5b                     0.000000                0.000000   \n",
       "3     id_00276f245                     0.001026                0.001035   \n",
       "4     id_0027f1083                     0.001433                0.001858   \n",
       "...            ...                          ...                     ...   \n",
       "3977  id_ff7004b87                     0.001027                0.001302   \n",
       "3978  id_ff925dd0d                     0.002918                0.002433   \n",
       "3979  id_ffb710450                     0.001668                0.001298   \n",
       "3980  id_ffbb869f2                     0.001329                0.001304   \n",
       "3981  id_ffd5800b6                     0.001118                0.001197   \n",
       "\n",
       "      acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0           0.002330                        0.015206   \n",
       "1           0.001935                        0.002532   \n",
       "2           0.000000                        0.000000   \n",
       "3           0.001715                        0.013131   \n",
       "4           0.001794                        0.018297   \n",
       "...              ...                             ...   \n",
       "3977        0.001287                        0.003127   \n",
       "3978        0.001484                        0.008940   \n",
       "3979        0.001160                        0.013833   \n",
       "3980        0.001201                        0.025012   \n",
       "3981        0.001675                        0.017771   \n",
       "\n",
       "      acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                              0.017180                        0.004809   \n",
       "1                              0.002347                        0.002031   \n",
       "2                              0.000000                        0.000000   \n",
       "3                              0.012946                        0.005428   \n",
       "4                              0.024744                        0.004441   \n",
       "...                                 ...                             ...   \n",
       "3977                           0.007432                        0.002429   \n",
       "3978                           0.018561                        0.004863   \n",
       "3979                           0.047847                        0.006083   \n",
       "3980                           0.023658                        0.006315   \n",
       "3981                           0.021451                        0.006345   \n",
       "\n",
       "      adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                       0.002277                       0.006463   \n",
       "1                       0.002773                       0.008872   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       0.002843                       0.003284   \n",
       "4                       0.004645                       0.002538   \n",
       "...                          ...                            ...   \n",
       "3977                    0.001647                       0.002708   \n",
       "3978                    0.003738                       0.003348   \n",
       "3979                    0.003079                       0.004218   \n",
       "3980                    0.004800                       0.003115   \n",
       "3981                    0.003564                       0.005365   \n",
       "\n",
       "      adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                       0.001000  ...                               0.001152   \n",
       "1                       0.002745  ...                               0.001048   \n",
       "2                       0.000000  ...                               0.000000   \n",
       "3                       0.001000  ...                               0.001000   \n",
       "4                       0.001000  ...                               0.001180   \n",
       "...                          ...  ...                                    ...   \n",
       "3977                    0.001036  ...                               0.001007   \n",
       "3978                    0.001012  ...                               0.001000   \n",
       "3979                    0.001000  ...                               0.001000   \n",
       "3980                    0.001122  ...                               0.001000   \n",
       "3981                    0.001000  ...                               0.001039   \n",
       "\n",
       "      trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0         0.001675         0.003923           0.001639   \n",
       "1         0.001226         0.004098           0.001157   \n",
       "2         0.000000         0.000000           0.000000   \n",
       "3         0.001843         0.002613           0.024072   \n",
       "4         0.001084         0.003385           0.001777   \n",
       "...            ...              ...                ...   \n",
       "3977      0.004393         0.002225           0.298259   \n",
       "3978      0.001368         0.002843           0.001908   \n",
       "3979      0.001014         0.002422           0.001425   \n",
       "3980      0.001000         0.002844           0.001556   \n",
       "3981      0.001580         0.002978           0.005258   \n",
       "\n",
       "      tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                      0.001116                               0.001015   \n",
       "1                      0.005570                               0.001012   \n",
       "2                      0.000000                               0.000000   \n",
       "3                      0.004450                               0.001000   \n",
       "4                      0.001332                               0.001323   \n",
       "...                         ...                                    ...   \n",
       "3977                   0.012208                               0.001074   \n",
       "3978                   0.002225                               0.001300   \n",
       "3979                   0.001376                               0.001000   \n",
       "3980                   0.001486                               0.001000   \n",
       "3981                   0.001727                               0.001065   \n",
       "\n",
       "      vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0            0.001246   0.002270                    0.006989       0.001972  \n",
       "1            0.005074   0.001067                    0.006349       0.003061  \n",
       "2            0.000000   0.000000                    0.000000       0.000000  \n",
       "3            0.001368   0.001864                    0.001170       0.001951  \n",
       "4            0.001071   0.001861                    0.001026       0.001579  \n",
       "...               ...        ...                         ...            ...  \n",
       "3977         0.004907   0.001394                    0.001022       0.001198  \n",
       "3978         0.001411   0.001543                    0.001005       0.001584  \n",
       "3979         0.001044   0.001487                    0.001009       0.001395  \n",
       "3980         0.001420   0.001753                    0.001034       0.003379  \n",
       "3981         0.001404   0.002425                    0.001033       0.002029  \n",
       "\n",
       "[3982 rows x 207 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "_cell_guid": "18a1cc50-0be4-4f77-8342-162b5ca2ec0f",
    "_uuid": "b91ecf02-c13e-4005-a8db-9f2b0cd86e5e",
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:18.438594Z",
     "iopub.status.busy": "2020-11-21T05:50:18.437618Z",
     "iopub.status.idle": "2020-11-21T05:50:18.477612Z",
     "shell.execute_reply": "2020-11-21T05:50:18.476989Z"
    },
    "papermill": {
     "duration": 0.474392,
     "end_time": "2020-11-21T05:50:18.477746",
     "exception": false,
     "start_time": "2020-11-21T05:50:18.003354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>...</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "      <th>sig_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.017180</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.009842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.006989</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>id_0004d9e33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.008872</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.008051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>id_001897cda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.024072</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>id_00276f245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.018297</td>\n",
       "      <td>0.024744</td>\n",
       "      <td>0.004441</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.010131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>id_0027f1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.025927</td>\n",
       "      <td>0.022872</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.013770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>id_006fc47b8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.003127</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.002429</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.298259</td>\n",
       "      <td>0.012208</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>id_ff7004b87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.008940</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>id_ff925dd0d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.013833</td>\n",
       "      <td>0.047847</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.013463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>id_ffb710450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.023658</td>\n",
       "      <td>0.006315</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.001556</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.003379</td>\n",
       "      <td>id_ffbb869f2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.017771</td>\n",
       "      <td>0.021451</td>\n",
       "      <td>0.006345</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.008688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.005258</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>id_ffd5800b6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3624 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  acat_inhibitor  \\\n",
       "0                        0.001155                0.001297        0.002330   \n",
       "1                        0.001000                0.001098        0.001935   \n",
       "2                        0.001026                0.001035        0.001715   \n",
       "3                        0.001433                0.001858        0.001794   \n",
       "4                        0.001040                0.001056        0.001959   \n",
       "...                           ...                     ...             ...   \n",
       "3619                     0.001027                0.001302        0.001287   \n",
       "3620                     0.002918                0.002433        0.001484   \n",
       "3621                     0.001668                0.001298        0.001160   \n",
       "3622                     0.001329                0.001304        0.001201   \n",
       "3623                     0.001118                0.001197        0.001675   \n",
       "\n",
       "      acetylcholine_receptor_agonist  acetylcholine_receptor_antagonist  \\\n",
       "0                           0.015206                           0.017180   \n",
       "1                           0.002532                           0.002347   \n",
       "2                           0.013131                           0.012946   \n",
       "3                           0.018297                           0.024744   \n",
       "4                           0.025927                           0.022872   \n",
       "...                              ...                                ...   \n",
       "3619                        0.003127                           0.007432   \n",
       "3620                        0.008940                           0.018561   \n",
       "3621                        0.013833                           0.047847   \n",
       "3622                        0.025012                           0.023658   \n",
       "3623                        0.017771                           0.021451   \n",
       "\n",
       "      acetylcholinesterase_inhibitor  adenosine_receptor_agonist  \\\n",
       "0                           0.004809                    0.002277   \n",
       "1                           0.002031                    0.002773   \n",
       "2                           0.005428                    0.002843   \n",
       "3                           0.004441                    0.004645   \n",
       "4                           0.004958                    0.005107   \n",
       "...                              ...                         ...   \n",
       "3619                        0.002429                    0.001647   \n",
       "3620                        0.004863                    0.003738   \n",
       "3621                        0.006083                    0.003079   \n",
       "3622                        0.006315                    0.004800   \n",
       "3623                        0.006345                    0.003564   \n",
       "\n",
       "      adenosine_receptor_antagonist  adenylyl_cyclase_activator  \\\n",
       "0                          0.006463                    0.001000   \n",
       "1                          0.008872                    0.002745   \n",
       "2                          0.003284                    0.001000   \n",
       "3                          0.002538                    0.001000   \n",
       "4                          0.002404                    0.001015   \n",
       "...                             ...                         ...   \n",
       "3619                       0.002708                    0.001036   \n",
       "3620                       0.003348                    0.001012   \n",
       "3621                       0.004218                    0.001000   \n",
       "3622                       0.003115                    0.001122   \n",
       "3623                       0.005365                    0.001000   \n",
       "\n",
       "      adrenergic_receptor_agonist  ...  trpv_agonist  trpv_antagonist  \\\n",
       "0                        0.009842  ...      0.001675         0.003923   \n",
       "1                        0.008051  ...      0.001226         0.004098   \n",
       "2                        0.010716  ...      0.001843         0.002613   \n",
       "3                        0.010131  ...      0.001084         0.003385   \n",
       "4                        0.013770  ...      0.001119         0.003209   \n",
       "...                           ...  ...           ...              ...   \n",
       "3619                     0.003464  ...      0.004393         0.002225   \n",
       "3620                     0.007547  ...      0.001368         0.002843   \n",
       "3621                     0.013463  ...      0.001014         0.002422   \n",
       "3622                     0.027397  ...      0.001000         0.002844   \n",
       "3623                     0.008688  ...      0.001580         0.002978   \n",
       "\n",
       "      tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0              0.001639                   0.001116   \n",
       "1              0.001157                   0.005570   \n",
       "2              0.024072                   0.004450   \n",
       "3              0.001777                   0.001332   \n",
       "4              0.002514                   0.001411   \n",
       "...                 ...                        ...   \n",
       "3619           0.298259                   0.012208   \n",
       "3620           0.001908                   0.002225   \n",
       "3621           0.001425                   0.001376   \n",
       "3622           0.001556                   0.001486   \n",
       "3623           0.005258                   0.001727   \n",
       "\n",
       "      ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                                  0.001015         0.001246   0.002270   \n",
       "1                                  0.001012         0.005074   0.001067   \n",
       "2                                  0.001000         0.001368   0.001864   \n",
       "3                                  0.001323         0.001071   0.001861   \n",
       "4                                  0.001000         0.001080   0.002109   \n",
       "...                                     ...              ...        ...   \n",
       "3619                               0.001074         0.004907   0.001394   \n",
       "3620                               0.001300         0.001411   0.001543   \n",
       "3621                               0.001000         0.001044   0.001487   \n",
       "3622                               0.001000         0.001420   0.001753   \n",
       "3623                               0.001065         0.001404   0.002425   \n",
       "\n",
       "      vitamin_d_receptor_agonist  wnt_inhibitor        sig_id  \n",
       "0                       0.006989       0.001972  id_0004d9e33  \n",
       "1                       0.006349       0.003061  id_001897cda  \n",
       "2                       0.001170       0.001951  id_00276f245  \n",
       "3                       0.001026       0.001579  id_0027f1083  \n",
       "4                       0.001014       0.001812  id_006fc47b8  \n",
       "...                          ...            ...           ...  \n",
       "3619                    0.001022       0.001198  id_ff7004b87  \n",
       "3620                    0.001005       0.001584  id_ff925dd0d  \n",
       "3621                    0.001009       0.001395  id_ffb710450  \n",
       "3622                    0.001034       0.003379  id_ffbb869f2  \n",
       "3623                    0.001033       0.002029  id_ffd5800b6  \n",
       "\n",
       "[3624 rows x 207 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "_cell_guid": "c62c652f-4c90-4a01-a1d3-931c8b6edf9d",
    "_uuid": "93d6474d-7ee0-482b-9094-c47068c4e216",
    "execution": {
     "iopub.execute_input": "2020-11-21T05:50:19.387708Z",
     "iopub.status.busy": "2020-11-21T05:50:19.386708Z",
     "iopub.status.idle": "2020-11-21T05:50:19.512280Z",
     "shell.execute_reply": "2020-11-21T05:50:19.512867Z"
    },
    "papermill": {
     "duration": 0.560746,
     "end_time": "2020-11-21T05:50:19.513034",
     "exception": false,
     "start_time": "2020-11-21T05:50:18.952288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Diff(list1, list2): \n",
    "    return (list(list(set(list1)-set(list2)) + list(set(list2)-set(list1)))) \n",
    "\n",
    "Diff (submission.columns, pd.read_csv('../input/lish-moa/sample_submission.csv').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "de9c971a-7604-4a7e-bec7-90e39a82fdc6",
    "_uuid": "e603f1e3-aa40-49dd-9863-19511b31a206",
    "papermill": {
     "duration": 0.435685,
     "end_time": "2020-11-21T05:50:20.382263",
     "exception": false,
     "start_time": "2020-11-21T05:50:19.946578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "683dfabd-24ea-4fcf-9569-9f0593e24aa7",
    "_uuid": "0bef53c9-1ea1-46c1-8a67-f29fa4179cf4",
    "papermill": {
     "duration": 0.433744,
     "end_time": "2020-11-21T05:50:21.253751",
     "exception": false,
     "start_time": "2020-11-21T05:50:20.820007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 4454.907279,
   "end_time": "2020-11-21T05:50:23.819494",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-21T04:36:08.912215",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
